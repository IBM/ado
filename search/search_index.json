{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is the webpage for the accelerated discovery orchestrator (<code>ado</code>).</p> <p><code>ado</code> is a unified platform for executing computational experiments at scale and analysing their results. It can be extended with new experiments or new analysis tools. It allows distributed teams of researchers and engineers to collaborate on projects, execute experiments, and share data.</p> <p>You can run the experiments and analysis tools already available in <code>ado</code> in a distributed, shared, environment with your team. You can also use <code>ado</code> to get features like data-tracking, data-sharing, tool integration and a CLI, for your analysis method or experiment for free.</p> <p>\ud83e\uddd1\u200d\ud83d\udcbb Using <code>ado</code> assumes familiarity with command line tools.</p> <p>\ud83d\udee0\ufe0f Developing <code>ado</code> requires knowledge of python.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> CLI: Our human-centric CLI follows   best practices</li> <li> Projects: Allow distributed groups of users to   collaborate and share data</li> <li> Extendable: Easily   add new experiments,   optimizers or other tools.</li> <li> Scalable: We use ray as our execution engine   allowing experiments and tools to easily scale</li> <li> Automatic data-reuse: Avoid repeating work with   transparent reuse of experiment results.   <code>ado</code> internal protocols ensure this happens only when it makes sense</li> <li> Provenance: As you work, the relationship between the data you create   and operations you perform are   automatically tracked</li> <li> Optimization and sampling: Out-of-the-box, leverage powerful   optimization methods via <code>raytune</code>   or use our flexible in built sampler</li> </ul>"},{"location":"#foundation-model-experimentation","title":"Foundation Model Experimentation","text":"<p>We have developed <code>ado</code> plugins providing advanced experiments for testing foundation-models:</p> <ul> <li> fine-tuning performance benchmarking</li> <li> inference performance benchmarking (using the   vLLM performance benchmark)</li> <li>COMING SOON  inference and fine-tuning prediction</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>A basic installation of <code>ado</code> only requires a recent Python version (3.10+). This will allow you to run many of our examples and explore ado features.</p>"},{"location":"#additional-requirements","title":"Additional Requirements","text":"<p>Some advanced features have additional requirements:</p> <ul> <li>Distributed Projects (Optional): To support projects with multiple   users you will need a remote, accessible, MySQL database. See   here   for more</li> <li>Multi-Node Execution (Optional): To support multi-node or scaling   execution you may need a multi-node RayCluster. See   here   for more details</li> </ul> <p>In addition <code>ado</code> plugins may have additional requirements for executing realistic experiments. For example,</p> <ul> <li>Fine-Tuning Benchmarking: Requires a   RayCluster with GPUs</li> <li>vLLM Performance Benchmarking: Requires an OpenShift cluster with GPUs</li> </ul>"},{"location":"#try-it-out","title":"Try it out","text":"<ul> <li> <p> Set up in 1 minute</p> <p>You can install ado by:</p> <pre><code>pip install ado-core\n</code></pre> <p>Now try:</p> <pre><code>ado get contexts\n</code></pre> <p>You will see a context, <code>local</code>, is listed.</p> <p>A context is like a project. The <code>local</code> context links to a local database you can use as a sandbox for testing.</p> <p>Try:</p> <pre><code>ado get operators\n</code></pre> <p>to see a list of the in-built operators.  </p> <p>Next, we recommend you try our short tutorial which will give an idea of how <code>ado</code> works.</p> </li> </ul>"},{"location":"#example","title":"Example","text":"<p>This video shows listing actuators and getting the details of an experiment. Check demo for more videos.</p>"},{"location":"#whats-next","title":"What's next","text":"<ul> <li> <p> Let's get started!</p> <p>Jump into our tutorial</p> <p>Taking a random walk </p> </li> <li> <p> Check out the ADO cli</p> <p>Get familiar with the capabilities of the <code>ado</code> command-line interface.</p> <p>Dive into the CLI reference docs </p> </li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p> <p>[TAGS]</p>"},{"location":"actuators/creating-actuator-classes/","title":"Extending `ado` with new actuators","text":"<p>Info</p> <p>A complete template actuator can be found  here. This example actuator is functional out-of-the-box  and can be used as the basis to create new actuators.</p> <p>Developers can write their own actuator plugins to add new experiments (a.k.a. tests, experiment protocols) in new domains to <code>ado</code>. Actuator plugins are written in python and can live in their own repository.</p> <p>The main part of writing an actuator plugin is writing a python class that implements a specific interface. <code>ado</code> then interacts with your plugin, and the experiments it provides, via an instance of this class.</p> <p>This page gives an overview of how to get started creating your own actuator. It's not intended to be comprehensive. After reading this page the best resource is to check our example actuator or to check an existing actuator plugin.</p>"},{"location":"actuators/creating-actuator-classes/#knowledge-required","title":"Knowledge required","text":"<ul> <li>Knowledge of python</li> <li>Knowledge of pydantic is useful, but not   necessary</li> </ul>"},{"location":"actuators/creating-actuator-classes/#actuator-plugin-package-structure","title":"Actuator plugin package structure","text":"<p>To create an actuator plugin you must use the following package structure</p> <pre><code>$YOUR_REPO_NAME\n\u251c\u2500\u2500 ado_actuators # This is ado's namespaced package for actuator plugins\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 $YOUR_PLUGIN_PACKAGE        # Your plugin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 actuator_definitions.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The above is structure creates a python <code>namespace</code> package. In this case the namespace package is called \"ado_actuators\", which is the namespace for <code>ado</code> plugins. Namespace packages allow developers to independently create and distribute python modules that will be installed under a common package name.</p> <p>When you <code>pip install</code> the above package <code>ado</code> will detect it when its next run. If you want to import the installed package in e.g. the python console you use</p> <pre><code>import ado_actuators.$YOUR_PLUGIN_NAME\n</code></pre> <p>Warning</p> <p>NOTE: Never put an <code>__init__.py</code> under <code>ado_actuators/</code> -  this will overwrite all installed plugins.</p> <p>Info</p> <p>You can have multiple plugins under <code>ado_actuators</code> in $YOUR_REPO_NAME above. When you install your package all the plugins will be installed.</p>"},{"location":"actuators/creating-actuator-classes/#pyprojecttoml","title":"pyproject.toml","text":"<p>The pyproject.toml for an actuator plugin should contain the following fields</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"setuptools_scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\ninclude-package-data = true # This is on by default, including it for clarity\n\n[tool.setuptools_scm]\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\n\n[tool.setuptools.package-data]\n# Note: This is optional.\n# If you don't specify every non python file that's in SCM will be added\nrobotic_lab_actuator = [\n    \"actuator_definitions.yaml\", # Required: The file that describes the actuator classes the plugin provides\n    \"experiments.yaml\" # Optional file that contains definitions for experiment catalog\n]\n\n[project]\nname=\"robotic_lab\" # Change to your preferred name, along with the actual package\ndescription=\"A template for creating an actuator\" # Change to describing your actuator\ndependencies=[\n    \"black\"\n]\ndynamic = [\"version\"]\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#the-actuator-class","title":"The Actuator Class","text":"<p>Your actuator plugin package must contain at least one class that is a subclass of <code>orchestrator.modules.actuators.base.ActuatorBase</code>. Each of these subclasses is an interface to a set of experiments (or tests)</p> <p>The subclass has to implement two methods:</p> <ul> <li><code>catalog</code>: This returns an   <code>orchestrator.modules.actuators.catalog.ExperimentCatalog</code> instance detailing   the experiments you actuator provides.</li> <li><code>submit</code>: This is an <code>async</code> method that <code>ado</code> will call to run an Experiment   on an Entity.</li> </ul> <p>In addition, the case must be decorated with <code>@ray.remote</code></p> <p>A sketch example:</p> <pre><code>import orchestrator.modules.actuators.base\nfrom orchestrator.schema.entity import Entity\nfrom orchestrator.schema.experiment import Experiment\nfrom orchestrator.modules.actuators.catalog import ExperimentCatalog\n\nclass MyActuator(orchestrator.modules.actuators.base):\n\n  async def submit(self, entities: [Entity], experiment: Experiment) -&gt; list[str]: #Returns a list of identifiers for the created experiments\n    ...\n\n  def catalog(self, **kwargs) -&gt; ExperimentCatalog:\n    pass\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#telling-ado-about-your-actuator-classes","title":"Telling ado about your actuator class(es)","text":"<p>Actuator plugins must include a file called <code>actuator_definitions.yaml</code> that is installed with the plugin. This file lists all the actuator classes that are available in the plugin.</p> <p>An example:</p> <pre><code>- module:\n    moduleClass: MyActuator\n    moduleName: ado_actuators.myplugin.actuators\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#what-an-actuator-is-expected-to-do-on-submit","title":"What an actuator is expected to do on <code>submit</code>","text":"<p>The key method of an <code>actuator</code> is the <code>submit</code> method as this is what runs an experiment. On a call to this method three things are expected to happen in the Actuator:</p> <ul> <li>One or more <code>MeasurementRequest</code> instances are created representing an   execution of the experiment that was requested</li> <li><code>One or more</code> as the actuator can launch a separate experiment for each     entity or one for them all. Which method is used depends on developer choice</li> <li>Launch the experiment asynchronously and return the <code>MeasurementRequest</code>   identifier(s)</li> <li>i.e. Its expected the <code>submit</code> method will return almost immediately and the     requested experiments will be executed asynchronously</li> <li>When an experiment has finished</li> <li>Add the results to the relevant <code>MeasurementRequest</code> instance</li> <li>Put the <code>MeasurementRequest</code> on the <code>MeasurementQueue</code> that was provided to     the actuator on <code>__init__</code></li> </ul> <p>From the <code>submit</code> callers point of view this means:</p> <ol> <li>It expects to immediately get back a set of strings that are    MeasurementRequest ids</li> <li>At some later time it will find MeasurementRequests with these ids on the    <code>MeasurementQueue</code> containing the experiment results.</li> </ol> <p>For everything else the actuator developer is free to implement as they want.</p>"},{"location":"actuators/creating-actuator-classes/#enabling-custom-configuration-of-an-actuator","title":"Enabling custom configuration of an actuator","text":"<p>Actuators may require a custom configuration (i.e., parameters) to be provided. For example, an actuator calling an inference server can require an endpoint to connect and its related authorisation token.</p> <p><code>ado</code> provides this capability through the <code>GenericActuatorParameters</code> class, which allows developers to define a Pydantic model of the parameters expected by the actuator. This model will be validated at runtime.</p> <p>To write your own actuator parameters class, simply create a class that inherits from <code>GenericActuatorParameters</code> and add a reference to it in the <code>parameters_class</code> class variable of your Actuator, as such:</p> <pre><code>from orchestrator.core.actuatorconfiguration.config import GenericActuatorParameters\nfrom orchestrator.modules.actuators.base import ActuatorBase\nimport pydantic\n\nclass InferenceActuatorParameters(GenericActuatorParameters):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n\n    endpoint: str = pydantic.Field(\n        description=\"Endpoint to an inference service\",\n        default=None,\n        validate_default=True,\n    )\n    authToken: str = pydantic.Field(\n        description=\"The token to access the inference service\",\n        default=None,\n        validate_default=True,\n    )\n\n@ray.remote\nclass Actuator(ActuatorBase):\n    identifier = \"my_actuator\"\n    parameters_class = InferenceActuatorParameters\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#example-custom-configurations","title":"Example custom configurations","text":"<p>Users can obtain an example configuration for your actuator using:</p> <pre><code>ado template actuatorconfiguration --actuator-identifier $YOUR_ACTUATOR_ID`\n</code></pre> <p>This example will be obtained by calling <code>model_construct()</code> on your actuator parameter class. This means</p> <ul> <li>default values you specify for fields are output</li> <li>you need default values for all fields</li> <li>the defaults are not validated</li> </ul> <p>This is useful when your configuration has required fields i.e. you need the user to supply them and can't set a default value for them. This way, the generated example template will include those fields, but <code>ado</code> will catch any missing or incorrect values when the user is creating the <code>actuatorconfiguration</code> resource.</p> <p>For example, you can declare a required field like this</p> <pre><code>authToken: str = pydantic.Field(\n    description=\"The token to access the inference service\",\n    default=None,              # &lt;--- value that will be written for examples. It is actually invalid\n    validate_default=True,     # &lt;--- This will check if the value is None and raise an error if it is i.e. if the example value was not changed\n)\n</code></pre> <p>If you have no required fields, you may want <code>ado</code> to validate your default values before outputting them. This is useful for e.g. tests, to ensure there isn't an error with the defaults. To do this you can override the <code>default_parameters</code> method in your Actuator to turn validation on e.g.</p> <pre><code>@override\ndef default_parameters(self) -&gt; GenericActuatorParameters:\n    return MyActuatorParams()\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#using-custom-actuatorconfiguration-parameters","title":"Using custom ActuatorConfiguration parameters","text":"<p>Once users have set the relevant values for your actuator in a YAML file they can create an <code>actuatorconfiguration</code> resource from them</p> <pre><code>ado create actuatorconfiguration -f $FILLED_IN_TEMPLATE\n</code></pre> <p>The actuatorconfiguration resource documentation contains for more information on how users will create and supply actuator parameters to your actuator.</p>"},{"location":"actuators/creating-actuator-classes/#how-the-custom-configuration-is-stored-and-output","title":"How the custom configuration is stored and output","text":"<p>When storing an instance of your custom configuration model in the metastore, the serialized representation is obtained using <code>model_dump_json()</code> with no options.</p> <p>When outputting for <code>ado get actuatorconfiguration</code>, the serialized representation is also obtained with <code>model_dump_json()</code>, and the schema with <code>model_json_schema()</code>. In this case various options to <code>model_dump_json</code> or <code>model_json_schema</code> may be used, e.g. <code>exclude_unset=True</code>.</p> <p>When outputting for <code>ado template actuatorconfiguration</code>, <code>model_construct()</code> is used by default as described in the previous section.</p>"},{"location":"actuators/creating-actuator-classes/#how-to-update-your-actuators-custom-configuration","title":"How to update your actuator's custom configuration","text":"<p>During development, there will be times when you might need to update the input parameter model for your actuator, adding, removing or modifying fields. In these cases, it's important not to break backwards compatibility (where possible) while making sure that users are aware of the changes to the model and do not rely indefinitely on the model being auto upgraded.</p> <p>In ado, we recommend using Pydantic before validators coupled with the <code>ado upgrade</code> command. At a high level, you should:</p> <ol> <li>Use a before validator to create a temporary upgrade path for your model.</li> <li>Enable a warning in this validator using the provided support functions    (described below). This warning will inform users that an upgrade is needed.    The support function will automatically print the command to upgrade stored    model versions and remove the warning. It will also display a message    indicating that auto-upgrade functionality will be removed in a future    release.</li> <li>Remove the upgrade path in the specified future version.</li> </ol> <p>Let's see a practical example using <code>MyActuatorParams</code>. We will consider two cases:</p> <ul> <li>We want to deprecate a field.</li> <li>We want to apply changes to a field without deprecating it.</li> </ul>"},{"location":"actuators/creating-actuator-classes/#deprecating-a-field-in-your-actuators-custom-configuration","title":"Deprecating a field in your actuator's custom configuration","text":"<p>Let's imagine we want to change the name of the <code>authToken</code> field to be <code>authorization_token</code>. The model for our actuator v2 would then be:</p> <pre><code>from orchestrator.core.actuatorconfiguration.config import GenericActuatorParameters\nimport pydantic\n\nclass InferenceActuatorParameters(GenericActuatorParameters):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n\n    endpoint: str = pydantic.Field(\n        description=\"Endpoint to an inference service\",\n        default=None,\n        validate_default=True,\n    )\n    authorization_token: str = pydantic.Field(\n        description=\"The token to access the inference service\",\n        default=None,\n        validate_default=True,\n    )\n</code></pre> <p>To enable upgrading of the previous model versions when fields are being deprecated, we recommended using a Pydantic Before Model Validator. This allows the dictionary content of the model to be changed as appropriate before validation is applied. To ensure the users are aware of the change, we will also use the <code>warn_deprecated_actuator_parameters_model_in_use</code> method in the validator:</p> <pre><code>from orchestrator.core.actuatorconfiguration.config import GenericActuatorParameters\nimport pydantic\n\nclass InferenceActuatorParameters(GenericActuatorParameters):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n\n    endpoint: str = pydantic.Field(\n        description=\"Endpoint to an inference service\",\n        default=None,\n        validate_default=True,\n    )\n    authorization_token: str = pydantic.Field(\n        description=\"The token to access the inference service\",\n        default=None,\n        validate_default=True,\n    )\n\n    @pydantic.model_validator(mode=\"before\")\n    @classmethod\n    def rename_authToken(cls, values: dict):\n        from orchestrator.core.actuatorconfiguration.config import (\n            warn_deprecated_actuator_parameters_model_in_use,\n        )\n\n        old_key = \"authToken\"\n        new_key = \"authorization_token\"\n        if old_key in values:\n\n            # Notify the user that the authToken\n            # field is deprecated\n            warn_deprecated_actuator_parameters_model_in_use(\n                affected_actuator=\"my_actuator\",\n                deprecated_from_actuator_version=\"v2\",\n                removed_from_actuator_version=\"v3\",\n                deprecated_fields=old_key,\n                latest_format_documentation_url=\"https://example.com\",\n            )\n\n            # The user has set both the old\n            # and the new key - the new key\n            # takes precedence.\n            if new_key in values:\n                values.pop(old_key)\n            # Set the old value in the\n            # new field\n            else:\n                values[new_key] = values.pop(old_key)\n\n        return values\n</code></pre> <p>When a model with the old field will be loaded, the user will see the following warning:</p> <pre><code>WARN:   The parameters for the my_actuator actuator have been updated as of my_actuator v2.\n        They are being temporarily auto-upgraded to the latest version.\n        This behavior will be removed with my_actuator v3.\nHINT:   Run ado upgrade actuatorconfigurations to upgrade the stored actuatorconfigurations.\n        Update your actuatorconfiguration YAML files to use the latest format: https://example.com\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#updating-a-field-in-your-actuators-configuration-without-deprecating-it","title":"Updating a field in your actuator's configuration without deprecating it","text":"<p>Let's imagine we want to change the type of the <code>endpoint</code> field to be <code>pydantic.HttpUrl</code>. The model for our actuator v2 would then be:</p> <pre><code>from orchestrator.core.actuatorconfiguration.config import GenericActuatorParameters\nimport pydantic\n\nclass InferenceActuatorParameters(GenericActuatorParameters):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n\n    endpoint: pydantic.HttpUrl = pydantic.Field(\n        description=\"Endpoint to an inference service\",\n        default=None,\n        validate_default=True,\n    )\n    authToken: str = pydantic.Field(\n        description=\"The token to access the inference service\",\n        default=None,\n        validate_default=True,\n    )\n</code></pre> <p>To enable upgrading of the previous model versions when fields are not being deprecated, we recommended using a Pydantic Before Field Validator. This allows the specific field to be changed as appropriate before validation is applied. To ensure the users are aware of the change, we will also use the <code>warn_deprecated_actuator_parameters_model_in_use</code> method in the validator:</p> <p>Note</p> <p>warning about deprecated fields, but we omit the <code>deprecated_fields</code> parameter.</p> <pre><code>from orchestrator.core.actuatorconfiguration.config import GenericActuatorParameters\nimport pydantic\n\nclass InferenceActuatorParameters(GenericActuatorParameters):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n\n    endpoint: pydantic.HttpUrl = pydantic.Field(\n        description=\"Endpoint to an inference service\",\n        default=None,\n        validate_default=True,\n    )\n    authToken: str = pydantic.Field(\n        description=\"The token to access the inference service\",\n        default=None,\n        validate_default=True,\n    )\n\n    @pydantic.field_validator(\"endpoint\", mode=\"before\")\n    @classmethod\n    def convert_endpoint_to_url(cls, value: str | pydantic.HttpUrl):\n        from orchestrator.core.actuatorconfiguration.config import (\n            warn_deprecated_actuator_parameters_model_in_use,\n        )\n\n        if isinstance(value, str):\n            # Notify the user that the parameters of my_actuator\n            # have been updated\n            warn_deprecated_actuator_parameters_model_in_use(\n                affected_actuator=\"my_actuator\",\n                deprecated_from_actuator_version=\"v2\",\n                removed_from_actuator_version=\"v3\",\n                latest_format_documentation_url=\"https://example.com\",\n            )\n            value = pydantic.HttpUrl(value)\n\n        return value\n</code></pre> <p>When a model using <code>str</code>s will be loaded, the user will see the following warning:</p> <pre><code>WARN:   The parameters for the my_actuator actuator have been updated as of my_actuator v1.\n        They are being temporarily auto-upgraded to the latest version.\n        This behavior will be removed with my_actuator v2.\nHINT:   Run ado upgrade actuatorconfigurations to upgrade the stored actuatorconfigurations.\n        Update your actuatorconfiguration YAML files to use the latest format: https://example.com\n</code></pre>"},{"location":"actuators/creating-actuator-classes/#ensure-actuator-cleanup","title":"Ensure actuator cleanup","text":"<p>An actuator implementation can create resources that need to be cleaned up at execution completion. Two options are provided for doing this:</p>"},{"location":"actuators/creating-actuator-classes/#python-atexit-based-cleanup","title":"Python atexit based cleanup","text":"<p>The <code>atexit</code> module defines functions to register and unregister cleanup functions. Functions thus registered are automatically executed upon normal interpreter termination. atexit runs these functions in the reverse order in which they were registered; if you register A, B, and C, at interpreter termination time they will be run in the order C, B, A. This method works well for clean up resources used by the actuator implementation itself, but not for cleaning up resources created by custom Ray actors created by the actuators.</p>"},{"location":"actuators/creating-actuator-classes/#custom-ray-actors-cleanup","title":"Custom Ray actors cleanup","text":"<p>This option uses a named detached actor. This actor is started in the Ray namespace of the <code>operation</code> using the actuator with the name of <code>resource_cleaner</code> and can be used by any custom actor implementing <code>cleanup</code> method.</p> <p>To ensure the cleanup actor has been created when you retrieve it, the safest approach is to only access it within your actuator class implementation or actors that were directly created by it.</p> <p>Below is an example of registering a custom class for cleanup:</p> <pre><code>from orchestrator.modules.operators.orchestrate import CLEANER_ACTOR, ResourceCleaner\nimport ray\n...\ntry:\n    cleaner_handle = ray.get_actor(name=CLEANER_ACTOR)\n    cleaner_handle.add_to_cleanup.remote(handle='your actor handle')\nexcept Exception as e:\n    print(f\"Failed to register custom actors for clean up {e}. Make sure you clean it up\")\n</code></pre> <p>Once the registration is in place, the <code>cleanup</code> method of this actor is invoked at the end of execution</p>"},{"location":"actuators/creating-actuator-classes/#experiment-executor","title":"Experiment executor","text":"<p>The actuator submit method invokes a Ray remote function <code>run_experiment</code> implemented by an experiment_executor. The actual name of this function and its parameters can be defined by the actuator implementer. Typically the set of parameters includes:</p> <pre><code>request: MeasurementRequest,  # measurement request\nexperiment: Union[Experiment, ParameterizedExperiment],  # experiment definition\nstate_update_queue: orchestrator.modules.actuators.measurement_queue.MeasurementQueue,  # state update queue\n</code></pre> <p>Any additional parameters can be added to these, as required for actuator implementation</p> <p>Implementation of <code>run_experiment</code> does the following:</p> <ol> <li>For each Entity in the request it retrieves the values required to run the    experiment</li> <li>Run experiment with the retrieved entities</li> <li>Create a MeasurementResult to hold the results</li> <li>Compute the overall request status</li> <li>Put completed request to the <code>state_update_queue</code></li> </ol>"},{"location":"actuators/creating-actuator-classes/#helper-functions-for-experiment-executor","title":"Helper functions for Experiment executor","text":"<p>To simplify Experiment executor implementation, we provide several helper functions and methods:</p> <ul> <li><code>Experiment.propertyValuesFromEntity</code> - Get the input values for the   experiment based on the entity and the experiment definition</li> <li><code>orchestrator.utilities.support.dict_to_measurements</code> - Extract the values   related to an experiment from a dictionary of measurements and convert to   PropertyValues</li> <li><code>orchestrator.utilities.support.create_measurement_result</code> - Create   measurement result</li> <li><code>orchestrator.utilities.support.compute_measurement_status</code> - Compute   execution status</li> <li><code>orchestrator.utilities.async_task_runner.AsyncTaskRunner</code> - wait for the   completion of an async function and get execution result</li> </ul>"},{"location":"actuators/creating-custom-experiments/","title":"Adding custom experiments","text":"<p>Note</p> <p>The search a space with an optimizer example contains the code described here</p> <p>Often you might want to use an experiment that is a simple python function. A typical example is a cost-function to be used with an optimization. <code>ado</code> provides a way to add such functions as experiments without having to create an Actuator class.</p> <p>The process involves creating a python package with two files</p> <ul> <li>A python module with your functions</li> <li>A yaml file describing the experiments they provide</li> </ul> <p>And then installing this package.</p>"},{"location":"actuators/creating-custom-experiments/#custom-experiment-package-structure","title":"Custom experiment package structure","text":"<p>To create a package with one or more custom experiments you must use the following package structure</p> <pre><code>$YOUR_REPO_NAME\n   - setup.py\n   - ado_actuators/    # This is `ado`'s namespaced package for actuator plugins and custom experiments\n       - $YOUR_CUSTOM_EXPERIMENT_PLUGIN_PACKAGE/  # Your package with custom experiments\n         - __init__.py\n         - $EXPERIMENTS.py # Python file with your function in it\n         - custom_experiments.yaml # A yaml file describing the custom experiments your package provides\n</code></pre>"},{"location":"actuators/creating-custom-experiments/#writing-the-experiment-catalog","title":"Writing the experiment catalog","text":"<p>The experiment catalog contains the following critical pieces of information:</p> <ol> <li>What your experiment is called</li> <li>Who is going to execute your experiment - for custom python functions this    will always be the special actuator \"custom_experiments\"</li> <li>What the python function that executes your experiment is called</li> <li>What properties your experiment measures</li> <li>The properties from other experiments this experiment requires as input - if    your function does not require properties from another experiment you don't    need this field</li> </ol> <p>A catalog can define multiple experiments. Each one is a new element in the top-level list.</p> <p>An example experiment description file is:</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\n\nexperiments:\n  - identifier: 'nevergrad_opt_3d_test_func'\n    actuatorIdentifier: \"custom_experiments\"\n    metadata:\n        module:\n          moduleName: ado_actuators.optimization_test_functions.optimization_test_functions\n          moduleFunction: artificial_function\n    targetProperties:\n      - identifier: \"function_value\"\n    requiredProperties:\n      - identifier: \"x0\"\n        propertyDomain:\n          variableType:  \"CONTINUOUS_VARIABLE_TYPE\"\n      - identifier: \"x1\"\n        propertyDomain:\n          variableType:  \"CONTINUOUS_VARIABLE_TYPE\"\n      - identifier: \"x2\"\n        propertyDomain:\n          variableType: \"CONTINUOUS_VARIABLE_TYPE\"\n    optionalProperties:\n      - identifier: \"name\"\n        propertyDomain:\n          values: ['discus', 'sphere', 'cigar', 'griewank', 'rosenbrock', 'st1']\n      - identifier: \"num_blocks\"\n        propertyDomain:\n          domainRange: [1,10]\n          variableType: \"DISCRETE_VARIABLE_TYPE\"\n          interval: 1\n    defaultParameterization:\n      - value: \"rosenbrock\"\n        property:\n          identifier: \"name\"\n      - value: 1\n        property:\n          identifier: \"num_blocks\"\n</code></pre> <p>This YAML describes:</p> <ul> <li>a single experiment called <code>nevergrad_opt_3d_test_func</code></li> <li>the measurement will be executed using a python function called   <code>artificial_function</code></li> <li>the function is in the module     <code>ado_actuators.optimization_test_functions.optimization_test_functions</code> i.e.     <code>ado_actuators.$YOUR_CUSTOM_EXPERIMENT_PLUGIN_PACKAGE.$EXPERIMENTS</code>     following above package layout)</li> <li>this name will always start with <code>ado_actuators</code></li> <li>The experiment has a set of required input properties - <code>x0</code>, <code>x1</code> and <code>x2</code> -   and set of optional input properties - <code>name</code> and <code>num_blocks</code></li> <li>these input properties are what the python function is expected to use</li> <li>The experiment measures a single target property <code>function_value</code></li> <li>so applying this experiment will return a value of an observed property     called <code>nevergrad_opt_3d_test_func.total_cost</code></li> </ul>"},{"location":"actuators/creating-custom-experiments/#writing-your-custom-experiment-functions","title":"Writing your custom experiment functions","text":"<p>The python function that implements the experiment described in the catalog must</p> <ol> <li>Be called the name you gave in the catalog (<code>metadata.function</code> field)</li> <li>Have a specific signature and return value</li> </ol> <p>A snippet of the above function, <code>artifical_function</code>, showing the signature and return value is:</p> <pre><code>import typing\nfrom orchestrator.schema.experiment import Experiment\nfrom orchestrator.schema.entity import Entity\nfrom orchestrator.schema.property_value import PropertyValue\n\n\ndef artificial_function(\n        entity: Entity,\n        experiment: Experiment,\n        parameters=None,  # deprecated field\n) -&gt; typing.List[PropertyValue]:\n    \"\"\"\n\n    :param entity: The entity to be measured\n    :param experiment: The Experiment object representing the exact Experiment to perform\n        Required as multiple experiments can measure this property\n    :param parameters: A dictionary.\n    :return: A list of PropertyValue objects\n    \"\"\"\n    # parameters is a dictionary of key:value pairs of the experiment required/optional inputs\n    # defined in custom_experiments.yaml\n    parameters = experiment.propertyValuesFromEntity(entity)\n\n    #Experiment logic elided\n    ...\n\n    # At end return the results\n    pv = PropertyValue(\n        value=value,\n        property=experiment.observedPropertyForTargetIdentifier(\"function_value\"),\n        valueType=ValueTypeEnum.NUMERIC_VALUE_TYPE,\n    )\n    return [pv]\n</code></pre> <p>In the above function <code>entity</code> and <code>experiment</code> are <code>ado</code> objects describing what to measure and what to measure it with. Since the custom experiment package only defined one experiment (see Writing the experiment catalog) the <code>experiment</code> object will represent the <code>nevergrad_opt_3d_test_func</code> experiment. The <code>entity</code> and <code>experiment</code> objects can be converted into a dictionary of required and optional input properties names and values using <code>experiment.propertyValuesFromEntity</code></p> <p>Once the values to return have been calculated the function has to create <code>PropertyValue</code> objects as shown above.</p> <p>To find out more about the class instances passed to this function check the <code>ado</code> source code.</p> <p>Warning</p> <p> If your function returns properties with different names than those you specified in the catalog for the experiment entry, they will be ignored.</p>"},{"location":"actuators/creating-custom-experiments/#using-your-custom-experiments-the-custom_experiments-actuator","title":"Using your custom experiments: the custom_experiments actuator","text":"<p>All your custom experiments in <code>ado</code> are accessed via a special actuator called custom_experiments.</p>"},{"location":"actuators/creating-custom-experiments/#add-your-experiments-to-ado","title":"Add your experiments to <code>ado</code>","text":"<p>First to add your experiments to <code>ado</code> run <code>pip install</code> in the same directory as your custom experiment packages <code>setup.py</code></p> <p>Confirm the experiment has been added:</p> <pre><code>ado describe actuators --details\n</code></pre> <p>If the custom experiment was the one defined in above you would see a new experiment entry for the objective_functions actuator called <code>ml-multicloud-cost-v1.0</code>.</p>"},{"location":"actuators/creating-custom-experiments/#add-a-custom-experiment-to-a-discoveryspace","title":"Add a custom experiment to a <code>discoveryspace</code>","text":"<p>To use a custom experiment you declare it the <code>measurementspace</code> of a <code>discoveryspaces</code> - exactly like other experiments. The only difference is you used the <code>custom_experiments</code> actuator.</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\n\nsampleStoreIdentifier: dfe035\nentitySpace:\n- identifier: x2\n  propertyDomain:\n    domainRange: [-10,10]\n    variableType: CONTINUOUS_VARIABLE_TYPE\n- identifier: x1\n  propertyDomain:\n    domainRange: [-10,10]\n    variableType: CONTINUOUS_VARIABLE_TYPE\n- identifier: x0\n  propertyDomain:\n    domainRange: [-10,10]\n    variableType: CONTINUOUS_VARIABLE_TYPE\nexperiments:\n- actuatorIdentifier: custom_experiments\n  experimentIdentifier: nevergrad_opt_3d_test_func\nmetadata:\n  name: 'rosenbrock_3d'\n</code></pre> <p>Note <code>ado</code> will validate the measurement space as normal. So in this case if the custom experiment <code>benchmark_performance</code> from the <code>replay</code> actuator is not included the space creation will fail.</p>"},{"location":"actuators/creating-custom-experiments/#next-steps","title":"Next Steps","text":"<p>Follow the search a space with an optimizer example to see how the custom experiment described here works in practice.</p>"},{"location":"actuators/replay/","title":"Using externally obtained data","text":""},{"location":"actuators/replay/#using-externally-obtained-data-the-replay-actuator","title":"Using externally obtained data: the replay actuator","text":"<p>The replay actuator allows you to leverage results that were obtained via experiments outside of <code>ado</code>, and are contained in external sources like CSV files. We can't repeat these experiments, or add new data using them, in <code>ado</code> as no actuator exits to do so. However, you still might want to define measurement spaces with them so entities that have the relevant data can be sampled and the data used, perhaps in a custom objective function.</p> <p>The talking a random walk tutorial uses external data and the replay actuator.</p>"},{"location":"actuators/replay/#importing-data-from-a-csv","title":"Importing data from a CSV","text":"<p>Often external data is stored in a CSV or table where each row contains measurement results for some entity. One set of columns defines the entity (the thing being measured) and another set of columns the results of one or more experiments on the entity.</p> <p>To use this data with <code>ado</code> the first step is to copy it into a <code>samplestore</code> at creation time. When copying this data into the <code>samplestore</code> the columns containing measured values (observed properties) and which columns containing constitutive properties are defined. With this information <code>ado</code> can create entities for each row. The following example is from talking a random walk:</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\nmetadata:\n  name: \"ml_multi_cloud\"\n  description: \"samplestore initialised with ml_multi_cloud data\"\nspecification:\n  module:\n    moduleName: orchestrator.core.samplestore.sql\n    moduleClass: SQLSampleStore\ncopyFrom:\n  - module:\n      moduleClass: CSVSampleStore\n      moduleName: orchestrator.core.samplestore.csv\n    storageLocation:\n      path: 'examples/ml-multi-cloud/ml_export.csv'\n    parameters:\n      generatorIdentifier: 'multi-cloud-ml'\n      identifierColumn: 'config'\n      constitutivePropertyColumns:\n        - cpu_family\n        - vcpu_size\n        - nodes\n        - provider\n      experiments:\n          - experimentIdentifier: 'benchmark_performance'\n            propertyMap:\n              wallClockRuntime: 'wallClockRuntime'\n              status:   'status' \n</code></pre> <p>The <code>copyFrom</code> section is where the external sources data should be copied into the <code>samplestore</code> are defined. There can be multiple but here there is just one.</p> <p>The relevant fields are:</p> <ul> <li><code>module</code>: These are the values you set to indicate the data is in a CSV file</li> <li><code>storageLocation</code>: This is the path the CSV file</li> <li><code>parameters.identifierColumn</code>: This is the column in the CSV, if any, to use   as the identifier of the created entities.</li> <li><code>parameters.constitutivePropertyColumns</code>: This is a list of the columns in the   CSV file that define the constitutive properties of the entities</li> <li><code>experiments</code>: This section defines the experiments that were used to generate   the data in the CSV file</li> <li><code>experiments.experimentIdentifier</code>: This is the name for the experiment in     ado</li> <li><code>experiments.propertyMap</code>: This is a dictionary mapping the names of the     properties experiment as they will appear in <code>ado</code> to column names in the     CSV</li> </ul> <p>The above YAML says to associate the data in the columns <code>wallClockRuntime</code> and <code>status</code> with an experiment 'benchmark_performance' that measures properties with the same name.</p> <p>The <code>propertyMap</code> field allows you to handle column headers had names that are not suitable for names of properties. For example if there was a column with measurements on a molecule called <code>Real_pKa (-0.83, 10.58)</code>, you might want to associate this with a property called <code>pka</code> instead:</p> <pre><code>propertyMap:\n  pka: \"Real_pKa (-0.83, 10.58)\"\n</code></pre>"},{"location":"actuators/replay/#using-the-external-data-in-a-discoveryspace","title":"Using the external data in a <code>discoveryspace</code>","text":"<p>If you copied entities from an external source to $SAMPLE_STORE_IDENTIFIER and in the process defined an external experiment called <code>my_experiment</code> then you can use it in a <code>discoveryspace</code> with:</p> <pre><code>sampleStoreIdentifier: $SAMPLE_STORE_IDENTIFIER\nexperiments:\n  - actuatorIdentifier: replay\n    experimentIdentifier: my_experiment\n</code></pre> <p>The ml multi cloud example uses this approach.</p>"},{"location":"actuators/replay/#how-the-replay-actuator-works","title":"How the <code>replay</code> actuator works","text":"<p>Looking at the example in Importing data from a CSV, you might wonder how <code>ado</code> can use it, if it does not have an actuator that provides the experiment <code>benchmark_performance</code>!</p> <p>What happens is that when a measurement of an experiment associated with the <code>replay</code> actuator is requested to be performed on an entity, if the data is present (because it was copied in) it is reused as normal by <code>ado</code>'s memoization mechanism. If there is no data, it can't be measured as no real experiment exists, and the <code>replay</code> actuator handles this case correctly - it creates the <code>No value to replay</code> messages seen here.</p> <p>Important</p> <p>To use external data via the replay actuator the relevant operator must be configured to use memoization. With the randomwalk and ray_tune operators this means singleMeasurement parameter is set to True (the default).</p>"},{"location":"actuators/sft-trainer/","title":"SFTTrainer - measure fine-tuning performance","text":""},{"location":"actuators/sft-trainer/#overview","title":"Overview","text":"<p>The <code>SFTTrainer</code> actuator provides a flexible and scalable interface for running supervised fine-tuning (SFT) experiments on large language and vision-language models. It supports a variety of fine-tuning strategies including full fine-tuning, LoRA, QPTQ-LoRA, and prompt-tuning across both text-to-text and image-to-text datasets.</p> <p>Designed for high-performance and distributed environments, <code>SFTTrainer</code> supports:</p> <ul> <li>Single-GPU, multi-GPU, and multi-node training</li> <li>Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP)   strategies</li> <li>RDMA over Converged Ethernet (RoCE) for optimized multi-node communication</li> <li>Ray-based task scheduling, enabling execution on both Kubernetes clusters   and bare-metal infrastructure</li> </ul> <p>Under the hood, this actuator wraps the fms-hf-tuning library, which itself builds on the <code>SFTTrainer</code> API from Hugging Face Transformers. This layered design allows users to leverage the robustness of the Hugging Face ecosystem while benefiting from ado\u2019s orchestration and reproducibility features.</p>"},{"location":"actuators/sft-trainer/#available-experiments","title":"Available experiments","text":"<p>The <code>SFTTrainer</code> actuator includes a set of experiments that evaluate different fine-tuning strategies under controlled conditions. These experiments use artificial datasets to ensure reproducibility and comparability across runs. A full list of available experiments and their configurations is available in the README.MD file of the Actuator.</p> <p>The most frequently used experiments are:</p>"},{"location":"actuators/sft-trainer/#finetune_full_benchmark-v100","title":"finetune_full_benchmark-v1.0.0","text":"<p>Performs full fine-tuning of all model parameters. This experiment is ideal for evaluating end-to-end training performance and resource utilization on large models.</p> Experiment documentation <p>An experiment instance:</p> <ul> <li>performs full fine tuning</li> <li>You may notice that even large-memory GPUs like the 80GB variant of the     NVIDIA A100 chip need at least 2 GPUs to train models as big as 13B     parameters.</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default,    can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via   <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <p> <pre><code>compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: ${fsdp_sharding_strategy}\n  fsdp_state_dict_type: ${fsdp_state_dict_type}\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: ${accelerate_config_fsdp_transformer_layer_cls_to_wrap}\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: ${SOME_PORT}\nnum_processes: ${NUM_GPUS}\n</code></pre> </p> <p>For DDP runs we use this instead:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndebug: False\ndowncast_bf16: no\ndistributed_type: MULTI_GPU\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>Commandline:</p> <p> <pre><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \\\n  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \\\n  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \\\n  --response_template \"\\n### Response:\" --dataset_text_field output --log_level debug \\\n  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \\\n  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \\\n  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \\\n  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \\\n  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \\\n  --packing False --peft_method none --optim ${OPTIM} --bf16 ${BF16} \\\n  --gradient_checkpointing_kwargs='{\"use_reentrant\": ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}' \\\n  --fast_moe ${FAST_MOE}\n</code></pre> </p> <p>Note: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:</li> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul>"},{"location":"actuators/sft-trainer/#full-finetuning-requirements","title":"Full Finetuning Requirements","text":"<ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the   models:</li> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic   datasets of the SFTTrainer actuator</li> </ul>"},{"location":"actuators/sft-trainer/#full-finetuning-entity-space","title":"Full Finetuning Entity space","text":"<p>Required:</p> <ul> <li>model_name: Supported models:   <code>[\"granite-3b-1.5\", \"hf-tiny-model-private/tiny-random-BloomForCausalLM\", \"llama-7b\", \"granite-13b-v2\", \"llama-13b\", \"granite-20b-v2\", \"granite-7b-base\", \"granite-8b-japanese\", \"granite-8b-code-base\", \"granite-34b-code-base\", \"mistral-7b-v0.1\", \"llama3-8b\", \"llama3-70b\", \"mixtral-8x7b-instruct-v0.1\", \"llama2-70b\", \"llama3.1-8b\", \"llama3.1-70b\", \"llama3.1-405b\", \"granite-3b-code-base-128k\", \"granite-8b-code-base-128k\", \"allam-1-13b\", \"granite-3-8b\", \"granite-3.1-2b\", \"granite-3.1-8b-instruct\", \"mistral-123b-v2\", \"granite-3.1-3b-a800m-instruct\", \"granite-vision-3.2-2b\", \"smollm2-135m\", \"llava-v1.6-mistral-7b\"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded   (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to   <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1,   number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for   example</li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options   are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127     (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127     (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127     (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least     16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2,     llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096     entries. Each entry includes at least 16384 tokens when tokenized with     <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image     with dimensions 384\u00d7384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset     also contains 4096 entries with a minimum of 16384 tokens per entry     (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies     of a single image sized 384\u00d7768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient   checkpointing to save memory (i.e. higher batchsizes) at the expense of slower   backward pass</li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to   accumulate before performing a backward/update pass. Only takes effect when   gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform.   Set to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if   max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked   to stop after the specified time elapses. The check is performed after   the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code>   (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend   to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple   nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use   1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning   to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>,   <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node   runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched   on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding.   number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is   a list with strings of boolean values for   <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use.   Available options are   <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>,   <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>,   <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>,   <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>,   <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>,   <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>,   <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>,   <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>,   <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>,   <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of   32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support   for NPU architecture or using CPU (use_cpu) or Ascend NPU.   This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether   to use the activation checkpoint variant that requires reentrant autograd.   This parameter should be passed explicitly. Torch version 2.5 will raise   an exception if use_reentrant is not passed. If use_reentrant=False,   checkpoint will use an implementation that does not require reentrant autograd.   This allows checkpoint to support additional functionality, such as working   as expected with torch.autograd.grad and support for keyword arguments input   into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards   optimizer states, gradients and parameters), \" [2] SHARD_GRAD_OP (shards   optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards   optimizer states, gradients and parameters within each node while each node   has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients   within each node while each node has full copy). For more information, please   refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>.   [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform   <code>requires_grad</code> during init, which means support for interspersed frozen   and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed   precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code>   requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None.   List of transformer layer class names (case-sensitive) to wrap, e.g,   <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>,   <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing   single sequence. Either the dataset_text_field or data_formatter_template   need to be supplied.   For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning   pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the   model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures     trl to skip preparing the dataset</li> </ul> <p>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p>"},{"location":"actuators/sft-trainer/#finetune_full_stability-v100","title":"finetune_full_stability-v1.0.0","text":"<p>Runs full fine-tuning five times and reports the proportion of tasks that fail due to GPU memory limits, unknown errors, or complete successfully. This experiment is useful for testing the model stability under different configurations.</p> Experiment documentation <p>An experiment instance:</p> <ul> <li>performs full fine-tuning 5 times and reports the fraction of tasks that ran   out of GPU memory, exhibited some unknown error, or completed successfully</li> <li>You may notice that even large-memory GPUs like the 80GB variant of the     NVIDIA A100 chip need at least 2 GPUs to train models as big as 13B     parameters.</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code></li> <li>uses the <code>FSDP</code> distributed backend</li> <li>runs 5 optimization steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>We use the following <code>accelerate_config.yml</code> YAML file for all models:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndebug: False\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_sync_module_states: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>Commandline:</p> <p> <pre><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \\\n  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \\\n  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \\\n  --response_template \"\\n### Response:\" --dataset_text_field output --log_level debug \\\n  --max_steps -1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \\\n  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \\\n  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \\\n  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \\\n  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \\\n  --packing False --peft_method none --optim ${OPTIM} --bf16 ${BF16} \\\n  --gradient_checkpointing_kwargs='{\"use_reentrant\": ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}' \\\n  --fast_moe ${FAST_MOE}\n</code></pre> </p> <p>Note: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:</li> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul>"},{"location":"actuators/sft-trainer/#full-finetuning-stability-requirements","title":"Full Finetuning (Stability) Requirements","text":"<ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the   models:</li> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic   datasets of the SFTTrainer actuator</li> </ul>"},{"location":"actuators/sft-trainer/#full-finetuning-stability-entity-space","title":"Full Finetuning (Stability) Entity space","text":"<pre><code>  Required:\n</code></pre> <ul> <li>model_name: Supported models:   <code>[\"granite-3b-1.5\", \"hf-tiny-model-private/tiny-random-BloomForCausalLM\", \"llama-7b\", \"granite-13b-v2\", \"llama-13b\", \"granite-20b-v2\", \"granite-7b-base\", \"granite-8b-japanese\", \"granite-8b-code-base\", \"granite-34b-code-base\", \"mistral-7b-v0.1\", \"llama3-8b\", \"llama3-70b\", \"mixtral-8x7b-instruct-v0.1\", \"llama2-70b\", \"llama3.1-8b\", \"llama3.1-70b\", \"llama3.1-405b\", \"granite-3b-code-base-128k\", \"granite-8b-code-base-128k\", \"allam-1-13b\", \"granite-3-8b\", \"granite-3.1-2b\", \"granite-3.1-8b-instruct\", \"mistral-123b-v2\", \"granite-3.1-3b-a800m-instruct\", \"granite-vision-3.2-2b\", \"smollm2-135m\", \"llava-v1.6-mistral-7b\"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and   possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to   <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1,   number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for   example</li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options   are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127     (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127     (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127     (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least     16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2,     llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096     entries. Each entry includes at least 16384 tokens when tokenized with     <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image     with dimensions 384\u00d7384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset     also contains 4096 entries with a minimum of 16384 tokens per entry     (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies     of a single image sized 384\u00d7768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient   checkpointing to save memory (i.e. higher batchsizes) at the expense of slower   backward pass</li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to   accumulate before performing a backward/update pass. Only takes effect when   gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>.   One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked   to stop after the specified time elapses. The check is performed after   the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code>   (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend   to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple   nodes. Each Node will use number_gpus/number_nodes GPUs.   Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to   use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>,   <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node   runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on   or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding.   number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list   with strings of boolean values for   <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are   <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>,   <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>,   <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>,   <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>,   <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>,   <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>,   <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>,   <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>,   <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>,   <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of   32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for   NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental   API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to   use the activation checkpoint variant that requires reentrant autograd. This   parameter should be passed explicitly. Torch version 2.5 will raise an   exception if use_reentrant is not passed. If use_reentrant=False, checkpoint   will use an implementation that does not require reentrant autograd. This   allows checkpoint to support additional functionality, such as working as   expected with torch.autograd.grad and support for keyword arguments input into   the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards   optimizer states, gradients and parameters), \" [2] SHARD_GRAD_OP (shards   optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards   optimizer states, gradients and parameters within each node while each node   has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients   within each node while each node has full copy). For more information, please   refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2]   LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform   <code>requires_grad</code> during init, which means support for interspersed frozen and   trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed   precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code>   requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None.   List of transformer layer class names (case-sensitive) to wrap, e.g,   <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>,   <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing   single sequence. Either the dataset_text_field or data_formatter_template need   to be supplied. For running vision language model tuning pass the column name   for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning   pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the   model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures   trl to skip preparing the dataset</li> </ul>"},{"location":"actuators/sft-trainer/#full-finetuning-stability-measured-properties","title":"Full Finetuning (Stability) Measured properties","text":"<ul> <li>f_gpu_oom: fraction of tasks that ran out of GPU memory</li> <li>f_other_error: fraction of tasks that ran into an unknown error</li> <li>f_no_error: fraction of tasks that completed successfully</li> <li>is_valid: whether this collection of tasks is a valid point to investigate</li> </ul> <p>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p>"},{"location":"actuators/sft-trainer/#finetune_lora_benchmark-v100","title":"finetune_lora_benchmark-v1.0.0","text":"<p>Executes LoRA-based fine-tuning, a parameter-efficient method that adapts only a small subset of model weights. This benchmark is useful for scenarios where compute or memory resources are limited, while still enabling meaningful adaptation.</p> Experiment documentation <p>An experiment instance:</p> <ul> <li>performs LORA fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default,   can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via   <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: ${fsdp_sharding_strategy}\n  fsdp_state_dict_type: ${fsdp_state_dict_type}\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: ${accelerate_config_fsdp_transformer_layer_cls_to_wrap}\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>For DDP runs we use this instead:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndebug: False\ndowncast_bf16: no\ndistributed_type: MULTI_GPU\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>Commandline:</p> <p> <pre><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \\\n  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \\\n  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \\\n  --response_template \"\\n### Response:\" --dataset_text_field output --log_level debug \\\n  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \\\n  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \\\n  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \\\n  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \\\n  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \\\n  --packing False --peft_method lora --target_modules ${SPACE SEPARATED LAYER NAMES} \\\n  --optim ${OPTIM} --bf16 ${BF16} \\\n  --gradient_checkpointing_kwargs='{\"use_reentrant\": ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}' \\\n  --fast_moe ${FAST_MOE}\n</code></pre> </p> <p>Note: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:</li> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul>"},{"location":"actuators/sft-trainer/#lora-requirements","title":"LoRA Requirements","text":"<ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the   models:</li> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic   datasets of the SFTTrainer actuator</li> </ul>"},{"location":"actuators/sft-trainer/#lora-entity-space","title":"LoRA Entity space","text":"<p>Required:</p> <ul> <li>model_name: Supported models:   <code>[\"granite-3b-1.5\", \"hf-tiny-model-private/tiny-random-BloomForCausalLM\", \"llama-7b\", \"granite-13b-v2\", \"llama-13b\", \"granite-20b-v2\", \"granite-7b-base\", \"granite-8b-japanese\", \"granite-8b-code-base\", \"granite-34b-code-base\", \"mistral-7b-v0.1\", \"llama3-8b\", \"llama3-70b\", \"mixtral-8x7b-instruct-v0.1\", \"llama2-70b\", \"llama3.1-8b\", \"llama3.1-70b\", \"llama3.1-405b\", \"granite-3b-code-base-128k\", \"granite-8b-code-base-128k\", \"allam-1-13b\", \"granite-3-8b\", \"granite-3.1-2b\", \"granite-3.1-8b-instruct\", \"mistral-123b-v2\", \"granite-3.1-3b-a800m-instruct\", \"granite-vision-3.2-2b\", \"smollm2-135m\", \"llava-v1.6-mistral-7b\"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and   possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to   <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1,   number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for   example</li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options   are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127     (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127     (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127     (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least     16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2,     llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096     entries. Each entry includes at least 16384 tokens when tokenized with     <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image     with dimensions 384\u00d7384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset     also contains 4096 entries with a minimum of 16384 tokens per entry     (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies     of a single image sized 384\u00d7768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient   checkpointing to save memory (i.e. higher batchsizes) at the expense of slower   backward pass</li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to   accumulate before performing a backward/update pass. Only takes effect when   gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set   to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if   max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked   to stop after the specified time elapses. The check is performed after   the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code>   (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend   to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple   nodes. Each Node will use number_gpus/number_nodes GPUs.   Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to   use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>,   <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node   runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on   or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding.   number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list   with strings of boolean values for   <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>r: Default is <code>4</code>. The LORA rank</li> <li>lora_alpha: Default is <code>16</code>. Scales the learning weights.</li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are   <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>,   <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>,   <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>,   <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>,   <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>,   <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>,   <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>,   <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>,   <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>,   <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of   32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for   NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental   API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to   use the activation checkpoint variant that requires reentrant autograd. This   parameter should be passed explicitly. Torch version 2.5 will raise an   exception if use_reentrant is not passed. If use_reentrant=False, checkpoint   will use an implementation that does not require reentrant autograd. This   allows checkpoint to support additional functionality, such as working as   expected with torch.autograd.grad and support for keyword arguments input into   the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards   optimizer states, gradients and parameters), \" [2] SHARD_GRAD_OP (shards   optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards   optimizer states, gradients and parameters within each node while each node   has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients   within each node while each node has full copy). For more information, please   refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2]   LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform   <code>requires_grad</code> during init, which means support for interspersed frozen and   trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed   precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code>   requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None.   List of transformer layer class names (case-sensitive) to wrap, e.g,   <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>,   <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing   single sequence. Either the dataset_text_field or data_formatter_template need   to be supplied. For running vision language model tuning pass the column name   for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning   pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the   model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures   trl to skip preparing the dataset</li> </ul> <p>Hardcoded:</p> <p>Sets the <code>--target_modules</code> layer names based on the <code>model_name</code>:</p> <ul> <li><code>smollm2-135m</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-3.1-3b-a800m-instruct</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-vision-3.2-2b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-3b-code-base-128k</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-7b-base</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-8b-code-base-128k</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-8b-code-base</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-8b-japanese</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-13b-v2</code>: <code>[\"c_attn\", \"c_proj\"]</code></li> <li><code>granite-20b-v2</code>: <code>[\"c_attn\", \"c_proj\"]</code></li> <li><code>granite-34b-code-base</code>: <code>[\"c_attn\", \"c_proj\"]</code></li> <li><code>llama-7b</code>: <code>[\"q_proj\", \"k_proj\"]</code></li> <li><code>llama-13b</code>: <code>[\"q_proj\", \"k_proj\"]</code></li> <li><code>llama2-70b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llama3-8b</code>: <code>[\"q_proj\", \"k_proj\"]</code></li> <li><code>llama3-70b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llama3.1-8b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llama3.1-70b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llama3.1-405b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>allam-1-13b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>hf-tiny-model-private/tiny-random-BloomForCausalLM</code>:   <code>[\"dense_h_to_4h\", \"dense_4h_to_4h\"]</code></li> <li><code>mistral-7b-v0.1</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>mistral-123b-v2</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>mixtral-8x7b-instruct-v0.1</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-3-8b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-3.1-2b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-3.1-8b-instruct</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llava-v1.6-mistral-7b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> </ul> <p>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p>"},{"location":"actuators/sft-trainer/#finetune_pt_benchmark-v100","title":"finetune_pt_benchmark-v1.0.0","text":"<p>Runs prompt-tuning, a lightweight fine-tuning strategy that prepends trainable prompts to the input. Similar to LoRA, this benchmark is useful for compute or memory constrained environments.</p> Experiment documentation <p>An experiment instance:</p> <ul> <li>performs prompt-tuning fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default,   can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via   <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: ${fsdp_sharding_strategy}\n  fsdp_state_dict_type: ${fsdp_state_dict_type}\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: ${accelerate_config_fsdp_transformer_layer_cls_to_wrap}\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>For DDP runs we use this instead:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndebug: False\ndowncast_bf16: no\ndistributed_type: MULTI_GPU\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>Commandline:</p> <p> <pre><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \\\n  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \\\n  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \\\n  --response_template \"\\n### Response:\" --dataset_text_field output --log_level debug \\\n  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \\\n  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \\\n  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \\\n  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \\\n  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \\\n  --packing False --peft_method none \\\n  --fast_moe ${FAST_MOE}\n</code></pre> </p> <p>Note: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:</li> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul>"},{"location":"actuators/sft-trainer/#prompt-tuning-requirements","title":"Prompt Tuning Requirements","text":"<ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the   models:</li> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic   datasets of the SFTTrainer actuator</li> </ul>"},{"location":"actuators/sft-trainer/#prompt-tuning-entity-space","title":"Prompt Tuning Entity space","text":"<p>Required:</p> <ul> <li>model_name: Supported models:   <code>[\"granite-3b-1.5\", \"hf-tiny-model-private/tiny-random-BloomForCausalLM\", \"llama-7b\", \"granite-13b-v2\", \"llama-13b\", \"granite-20b-v2\", \"granite-7b-base\", \"granite-8b-japanese\", \"granite-8b-code-base\", \"granite-34b-code-base\", \"mistral-7b-v0.1\", \"llama3-8b\", \"llama3-70b\", \"mixtral-8x7b-instruct-v0.1\", \"llama2-70b\", \"llama3.1-8b\", \"llama3.1-70b\", \"llama3.1-405b\", \"granite-3b-code-base-128k\", \"granite-8b-code-base-128k\", \"allam-1-13b\", \"granite-3-8b\", \"granite-3.1-2b\", \"granite-3.1-8b-instruct\", \"mistral-123b-v2\", \"granite-3.1-3b-a800m-instruct\", \"granite-vision-3.2-2b\", \"smollm2-135m\", \"llava-v1.6-mistral-7b\"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and   possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to   <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1,   number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for   example</li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options   are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127     (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127     (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127     (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least     16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2,     llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096     entries. Each entry includes at least 16384 tokens when tokenized with     <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image     with dimensions 384\u00d7384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset     also contains 4096 entries with a minimum of 16384 tokens per entry     (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies     of a single image sized 384\u00d7768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient   checkpointing to save memory (i.e. higher batchsizes) at the expense of slower   backward pass</li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to   accumulate before performing a backward/update pass. Only takes effect when   gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set   to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if   max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked   to stop after the specified time elapses. The check is performed after   the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code>   (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend   to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple   nodes. Each Node will use number_gpus/number_nodes GPUs.   Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to   use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>,   <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node   runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on   or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding.   number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list   with strings of boolean values for   <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are   <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>,   <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>,   <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>,   <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>,   <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>,   <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>,   <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>,   <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>,   <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>,   <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of   32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for   NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental   API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to   use the activation checkpoint variant that requires reentrant autograd. This   parameter should be passed explicitly. Torch version 2.5 will raise an   exception if use_reentrant is not passed. If use_reentrant=False, checkpoint   will use an implementation that does not require reentrant autograd. This   allows checkpoint to support additional functionality, such as working as   expected with torch.autograd.grad and support for keyword arguments input into   the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards   optimizer states, gradients and parameters), \" [2] SHARD_GRAD_OP (shards   optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards   optimizer states, gradients and parameters within each node while each node   has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients   within each node while each node has full copy). For more information, please   refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2]   LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform   <code>requires_grad</code> during init, which means support for interspersed frozen and   trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed   precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code>   requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None.   List of transformer layer class names (case-sensitive) to wrap, e.g,   <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>,   <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing   single sequence. Either the dataset_text_field or data_formatter_template need   to be supplied. For running vision language model tuning pass the column name   for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning   pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the   model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures   trl to skip preparing the dataset</li> </ul> <p>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p>"},{"location":"actuators/sft-trainer/#finetune_gtpq-lora_benchmark-v100","title":"finetune_gtpq-lora_benchmark-v1.0.0","text":"<p>Combines LoRA with GPTQ quantization to enable fine-tuning on quantized models. This benchmark is tailored for scenarios where model size and inference efficiency are critical, and it leverages fused kernels and quantized weights for performance.</p> Experiment documentation <p>An experiment instance:</p> <ul> <li>performs LORA fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>float16</code>, cannot be a different value</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default,   can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via   <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> <li>uses fms-acceleration plugins to perform GPTQ LoRA. Specifically:</li> <li><code>auto_gptq</code> is set to <code>triton_v2</code></li> <li><code>fast_kernels</code> is set to <code>True True True</code></li> <li><code>fused_lora</code> is set to <code>auto_gptq True</code></li> <li><code>torch_dtype</code> is set to <code>float16</code></li> <li>loads GPTQ compatible pre-quantized weights from a PVC</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: ${fsdp_sharding_strategy}\n  fsdp_state_dict_type: ${fsdp_state_dict_type}\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: ${accelerate_config_fsdp_transformer_layer_cls_to_wrap}\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>For DDP runs we use this instead:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndebug: False\ndowncast_bf16: no\ndistributed_type: MULTI_GPU\nmachine_rank: { $THE MACHINE RANK - always 0 for single-node runs }\nmain_training_function: main\nmixed_precision: ${accelerate_config_mixed_precision}\nnum_machines: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\nmain_process_port: { $SOME_PORT }\nnum_processes: { $NUM_GPUS }\n</code></pre> <p>Commandline:</p> <p> <pre><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \\\n  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \\\n  --torch_dtype float16 --use_flash_attn True --training_data_path ${DATASET_PATH} \\\n  --response_template \"\\n### Response:\" --dataset_text_field output --log_level debug \\\n  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \\\n  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \\\n  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \\\n  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \\\n  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \\\n  --packing False --peft_method lora --target_modules ${SPACE SEPARATED LAYER NAMES} \\\n  --fp16 true --fast_kernels true true true --fused_lora auto_gptq true --auto_gptq triton_v2 \\\n  --optim ${OPTIM} --bf16 ${BF16} \\\n  --gradient_checkpointing_kwargs='{\"use_reentrant\": ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}' \\\n  --fast_moe ${FAST_MOE}\n</code></pre> </p> <p>Note: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:</li> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul>"},{"location":"actuators/sft-trainer/#gptq-lora-requirements","title":"GPTQ LoRA Requirements","text":"<ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the   models:</li> <li>LLaMa/models/hf/7B-gptq/</li> <li>LLaMa/models/hf/llama3-70b-gptq/</li> <li>LLaMa/models/hf/llama3.1-405b-gptq/</li> <li>granite-20b-code-base-v2/step_280000_ckpt-gptq/</li> <li>granite-34b-gptq/</li> <li>granite-7b-base-gtpq/</li> <li>granite-8b-code-instruct-gptq/</li> <li>mistral-7B-v0.3-gptq/</li> <li>mixtral_8x7b_instruct_v0.1_gptq/</li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic   datasets of the SFTTrainer actuator</li> </ul>"},{"location":"actuators/sft-trainer/#gptq-lora-entity-space","title":"GPTQ LoRA Entity space","text":"<p>Required:</p> <ul> <li>model_name: Supported models:   <code>[\"llama-7b\", \"granite-20b-v2\", \"granite-7b-base\", \"granite-8b-code-instruct\", \"granite-34b-code-base\", \"mistral-7b-v0.1\", \"llama3-70b\", \"mixtral-8x7b-instruct-v0.1\", \"llama3.1-405b\"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and   possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to   <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1,   number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for   example</li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options   are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127     (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127     (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127     (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least     16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2,     llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096     entries. Each entry includes at least 16384 tokens when tokenized with     <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image     with dimensions 384\u00d7384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset     also contains 4096 entries with a minimum of 16384 tokens per entry     (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies     of a single image sized 384\u00d7768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient   checkpointing to save memory (i.e. higher batchsizes) at the expense of slower   backward pass</li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to   accumulate before performing a backward/update pass. Only takes effect when   gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>float16</code>. One of <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set   to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if   max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked   to stop after the specified time elapses. The check is performed after   the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code>   (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend   to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple   nodes. Each Node will use number_gpus/number_nodes GPUs.   Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to   use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>,   <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node   runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on   or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding.   number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list   with strings of boolean values for   <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>r: Default is <code>4</code>. The LORA rank</li> <li>lora_alpha: Default is <code>16</code>. Scales the learning weights.</li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are   <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>,   <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>,   <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>,   <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>,   <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>,   <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>,   <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>,   <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>,   <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>,   <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of   32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for   NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental   API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to   use the activation checkpoint variant that requires reentrant autograd. This   parameter should be passed explicitly. Torch version 2.5 will raise an   exception if use_reentrant is not passed. If use_reentrant=False, checkpoint   will use an implementation that does not require reentrant autograd. This   allows checkpoint to support additional functionality, such as working as   expected with torch.autograd.grad and support for keyword arguments input into   the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards   optimizer states, gradients and parameters), \" [2] SHARD_GRAD_OP (shards   optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards   optimizer states, gradients and parameters within each node while each node   has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients   within each node while each node has full copy). For more information, please   refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2]   LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform   <code>requires_grad</code> during init, which means support for interspersed frozen and   trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed   precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code>   requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None.   List of transformer layer class names (case-sensitive) to wrap, e.g,   <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>,   <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing   single sequence. Either the dataset_text_field or data_formatter_template need   to be supplied. For running vision language model tuning pass the column name   for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning   pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the   model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures   trl to skip preparing the dataset</li> </ul> <p>Hardcoded:</p> <p>Sets the <code>--target_modules</code> layer names based on the <code>model_name</code>:</p> <ul> <li><code>granite-8b-code-instruct</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-7b-base</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>granite-20b-v2</code>: <code>[\"c_attn\", \"c_proj\"]</code></li> <li><code>granite-34b-code-base</code>: <code>[\"c_attn\", \"c_proj\"]</code></li> <li><code>llama-7b</code>: <code>[\"q_proj\", \"k_proj\"]</code></li> <li><code>llama3-70b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>mistral-7b-v0.1</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>mixtral-8x7b-instruct-v0.1</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>llama3.1-405b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> <li><code>allam-1-13b</code>: <code>[\"q_proj\", \"v_proj\"]</code></li> </ul> <p>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p>"},{"location":"actuators/sft-trainer/#actuator-parameters","title":"Actuator Parameters","text":"<p>This section describes the fields you may optionally configure in your <code>actuatorconfiguration</code> resource for the <code>SFTTrainer</code> actuator.</p>"},{"location":"actuators/sft-trainer/#example-actuator-configuration-yaml","title":"Example Actuator Configuration YAML","text":"<pre><code>actuatorIdentifier: SFTTrainer\nparameters:\n  match_exact_dependencies: true\n  output_dir: \"output\"\n  data_directory: \"/data/fms-hf-tuning/artificial-dataset/\"\n  hf_home: \"/hf-models-pvc/huggingface_home\"\n  model_map:\n    granite-3.1-2b:\n      Vanilla: \"ibm-granite/granite-3.1-2b-base\"\n  num_tokens_cache_directory: \"cache\"\n</code></pre>"},{"location":"actuators/sft-trainer/#configuration-fields","title":"Configuration Fields","text":""},{"location":"actuators/sft-trainer/#match_exact_dependencies-bool-default-true","title":"<code>match_exact_dependencies</code> (bool, default: <code>true</code>)","text":"<ul> <li>Description: If <code>true</code>, the measurement runs in a virtual environment that   exactly matches the Python packages of the selected <code>fms-hf-tuning</code> version.   This enables all optional features like <code>fast_kernels</code>, <code>fast_moe</code>, and   <code>flash_attn</code>.</li> <li>Set to <code>false</code> if running on devices with limited support (e.g., MacBooks   or ARM CPUs), to avoid incompatible packages and features that depend on using   NVIDIA GPUs.</li> </ul>"},{"location":"actuators/sft-trainer/#output_dir-str-default-output","title":"<code>output_dir</code> (str, default: <code>\"output\"</code>)","text":"<ul> <li>Description: Directory prefix where the fine-tuned model weights will be   saved.</li> </ul>"},{"location":"actuators/sft-trainer/#data_directory-str-default-datafms-hf-tuningartificial-dataset","title":"<code>data_directory</code> (str, default: <code>\"/data/fms-hf-tuning/artificial-dataset/\"</code>)","text":"<ul> <li>Description: Path to the directory containing the dataset files used for   fine-tuning.</li> </ul>"},{"location":"actuators/sft-trainer/#aim_db-str-default-none","title":"<code>aim_db</code> (str, default: None)","text":"<ul> <li>Description: Endpoint of the AIM server used to log training metrics. When   set to None the measurement will use a temporary AIM repository that will be   garbage collected after the termination of the measurement.</li> </ul>"},{"location":"actuators/sft-trainer/#aim_dashboard_url-str-or-null-optional","title":"<code>aim_dashboard_url</code> (str or null, optional)","text":"<ul> <li>Description: URL of the AIM dashboard. If set, this will be included in   the metadata of the measurement results.</li> <li>Example: <code>\"http://aim-dashboard.example.com\"</code></li> </ul>"},{"location":"actuators/sft-trainer/#hf_home-str-default-hf-models-pvchuggingface_home","title":"<code>hf_home</code> (str, default: <code>\"/hf-models-pvc/huggingface_home\"</code>)","text":"<ul> <li>Description: Directory where Hugging Face stores authentication tokens and   model cache.</li> </ul>"},{"location":"actuators/sft-trainer/#model_map-dict-optional","title":"<code>model_map</code> (dict, optional)","text":"<ul> <li>Description: Maps model identifiers to their corresponding Hugging Face   model ids and absolute paths. The contents of this dictionary will override   the defaults that ship with the Actuator.</li> <li>Example:</li> </ul> <pre><code>model_map:\n  granite-3.1-2b:\n    Vanilla: \"ibm-granite/granite-3.1-2b-base\"\n</code></pre>"},{"location":"actuators/sft-trainer/#num_tokens_cache_directory-str-or-null-default-cache","title":"<code>num_tokens_cache_directory</code> (str or null, default: <code>\"cache\"</code>)","text":"<ul> <li>Description: Directory used to cache token counts for datasets. This   avoids recomputing token counts, which can be time-consuming. Relative paths   are resolved under <code>@data_directory</code>.</li> <li>Set to <code>null</code> to disable caching.</li> </ul>"},{"location":"actuators/sft-trainer/#measured-properties","title":"Measured Properties","text":"<p>Each experiment collects detailed runtime and system-level metrics using AIM. The AIM metrics are aggregated into the following before being stored in ado's database:</p>"},{"location":"actuators/sft-trainer/#gpu-metrics","title":"GPU Metrics","text":"<ul> <li>Compute Utilization: <code>min</code>, <code>avg</code>, <code>max</code> (%)</li> <li>Memory Utilization: <code>min</code>, <code>avg</code>, <code>max</code>, <code>peak</code> (%)</li> <li>Power Usage: <code>min</code>, <code>avg</code>, <code>max</code> (Watts and %)</li> </ul>"},{"location":"actuators/sft-trainer/#cpu-metrics","title":"CPU Metrics","text":"<ul> <li>Compute Utilization: Average CPU usage per core (%)</li> <li>Memory Utilization: Average memory usage of the training process (%)</li> </ul>"},{"location":"actuators/sft-trainer/#training-performance","title":"Training Performance","text":"<ul> <li><code>train_runtime</code>: Duration in seconds from the start of the first training   step to the end of the last training step.</li> <li><code>train_samples_per_second</code>: May be inaccurate, as HuggingFace uses a   heuristic to estimate this.</li> <li><code>train_steps_per_second</code>: May be inaccurate due to HuggingFace's   heuristic-based measurement.</li> <li><code>train_tokens_per_second</code>: May be inaccurate, as it relies on   HuggingFace's heuristic.</li> <li><code>train_tokens_per_gpu_per_second</code>: May be inaccurate for the same reason,   HuggingFace uses a heuristic.</li> <li><code>dataset_tokens_per_second</code>: The actuator computes this in an accurate   way.</li> <li><code>dataset_tokens_per_second_per_gpu</code>: The actuator computes this in an   accurate way.</li> </ul> <p>Info</p> <p>We report all system metrics as min/avg/max over the duration of the run. GPU metrics are collected per device; CPU metrics are collected for the training process. Token throughput accounts for padding and sequence truncation.</p>"},{"location":"actuators/sft-trainer/#validation","title":"Validation","text":"<p>Each experiment includes a computed <code>is_valid</code> flag that indicates whether the run was structurally and functionally valid. A run is marked invalid if any of the following conditions are met:</p>"},{"location":"actuators/sft-trainer/#configuration-errors","title":"Configuration Errors","text":"<ul> <li><code>batch_size</code> is not evenly divisible by <code>number_gpus</code></li> <li><code>number_gpus</code> is not evenly divisible by <code>number_nodes</code></li> <li><code>number_nodes</code> is less than 1</li> <li><code>batch_size</code> is less than 1</li> <li><code>gpu_model</code> is missing or empty when <code>number_gpus &gt; 0</code></li> </ul>"},{"location":"actuators/sft-trainer/#incompatible-mixture-of-experts-moe-settings","title":"Incompatible Mixture of Experts (MoE) Settings","text":"<ul> <li><code>fast_moe</code> is set but <code>number_gpus</code> is not divisible by it</li> <li><code>fast_moe</code> is set but the model\u2019s <code>num_local_experts</code> is not divisible by   <code>fast_moe</code></li> </ul>"},{"location":"actuators/sft-trainer/#runtime-failures","title":"Runtime Failures","text":"<ul> <li>The run raises a <code>torch.cuda.OutOfMemoryError</code> (considered invalid due to GPU   memory exhaustion)</li> <li>The run raises a   <code>RuntimeError: CUDA error: an illegal memory access was encountered</code> exception   (considered invalid due to GPU memory exhaustion)</li> <li>The run raises other exceptions (e.g., <code>RuntimeError</code> with <code>NCCL Error</code>) -   these are marked as failed and do not record any metrics</li> </ul> <p>Note: Failed runs are not persisted into ado's database. Restarting an operation will cause ado to retry them.</p> <p>This validation logic ensures that only meaningful and resource-compatible runs are included in the information we store in ado's database.</p>"},{"location":"actuators/sft-trainer/#configure-your-raycluster","title":"Configure your RayCluster","text":"<p>Running SFTTrainer experiments requires:</p> <ul> <li>GPU workers with custom resources indicating the available GPU devices</li> <li>A dataset</li> <li>The model weights</li> </ul> <p>Use the information below to deploy your RayCluster.</p>"},{"location":"actuators/sft-trainer/#annotating-gpu-workers-with-custom-resources","title":"Annotating GPU workers with custom resources","text":"<p>The <code>SFTTrainer</code> actuator leverages Ray's custom resource scheduling to efficiently allocate GPU-powered tasks to workers equipped with the appropriate hardware. It uses the following custom resources:</p>"},{"location":"actuators/sft-trainer/#custom-resource-types","title":"Custom Resource Types","text":"<ul> <li> <p><code>full-worker</code>   Some Ray tasks require exclusive access to an entire node. These tasks request   the <code>full-worker</code> resource. GPU workers that occupy a full node should have   exactly one <code>full-worker</code> custom resource.</p> </li> <li> <p><code>${GPU_MODEL}</code>   This custom resource key corresponds to the specific GPU model available on   the node, with the value indicating the number of devices. Supported GPU   models include: - <code>NVIDIA-A100-SXM4-80GB</code> - <code>NVIDIA-A100-80GB-PCIe</code> -   <code>NVIDIA-H100-80GB-HBM3</code> - <code>NVIDIA-H100-PCIe</code> - <code>Tesla-V100-PCIE-16GB</code> -   <code>Tesla-T4</code> - <code>L40S</code></p> </li> <li> <p><code>RoCE</code>   Tasks that utilize RDMA over Converged Ethernet (RoCE) request the <code>RoCE</code>   resource. For guidance on configuring RoCE in your RayCluster, refer to the   instructions linked at the bottom of this page.</p> </li> </ul>"},{"location":"actuators/sft-trainer/#creating-the-datasets","title":"Creating the datasets","text":"<p>The SFTTrainer actuator supports both text-to-text and image-to-text tuning experiments. Installing the actuator provides access to 2 command-line utilities for generating synthetic datasets.</p> <p>By default, the actuator expects the dataset files under <code>/data/fms-hf-tuning/artificial-dataset/</code></p> <p>You can override this path by setting the <code>data_directory</code> parameter via an ActuatorConfiguration resource and referencing it in the Operations you create. We include a link to the relevant documentation at the bottom of this page.</p>"},{"location":"actuators/sft-trainer/#dataset-for-text-to-text-tasks","title":"Dataset for text-to-text tasks","text":"<p>For text-to-text tasks, create a dataset file with the name <code>news-tokens-16384plus-entries-4096.jsonl</code>.</p> <p>Use the following command:</p> <pre><code>sfttrainer_generate_dataset_text -o /data/fms-hf-tuning/artificial-dataset/news-tokens-16384plus-entries-4096.jsonl\n</code></pre> <p>If you are working with a remote RayCluster, run this as a remote Ray job using a Ray runtime environment that contains the python package for the SFTTrainer actuator. At the bottom of this page you will find a link to our documentation on submitting remote Ray jobs that use the code of Actuators.</p> <p>Note</p> <p>If your RayCluster Worker nodes already have the SFTTrainer wheel installed, you can skip building the wheel and using a ray runtime environment. Go directly to the <code>ray job submit</code> step. Just change the commandline so that it does not use  the <code>ray_runtime.yaml</code> file.</p> <p>For example, build the wheel file for SFTTrainer and create the following <code>ray_runtime_env.yaml</code>:</p> <pre><code>pip:\n  - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/ado_sfttrainer-1.0.2.dev84+g1ab8f43d-py3-none-any.whl\nenv_vars:\n  PYTHONUNBUFFERED: \"x\"\n</code></pre> <p>Note</p> <p>Your wheel file will have a different name so update the <code>ray_runtime_env.yaml</code> file accordingly. Make sure you keep the <code>${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/</code> prefix.</p> <p>Then start a Ray job that executes <code>sfttrainer_generate_dataset_text</code> and pointing it to your remote RayCluster and references your <code>ray_runtime_env.yaml</code> file. For example, if your RayCluster is listening on <code>http://localhost:8265</code> run the following command in the same directory as your <code>ray_runtime_env.yaml</code> file:</p> <p>Info</p> <p>If you are using a remote RayCluster on Kubernetes remember to start a port-forward to the RayCluster head node.</p> <pre><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \\\n  sfttrainer_generate_dataset_text \\\n  -o /data/fms-hf-tuning/artificial-dataset/news-tokens-16384plus-entries-4096.jsonl\n</code></pre>"},{"location":"actuators/sft-trainer/#dataset-for-image-to-text-tasks","title":"Dataset for image-to-text tasks","text":"<p>SFTTrainer supports 2 datasets for text-to-image tasks:</p> <ul> <li><code>vision-384x384-16384plus-entries-4096.parquet</code></li> <li><code>vision-384x768-16384plus-entries-4096.parquet</code></li> </ul> <p>To create the dataset files use the same <code>ray_runtime_env.yaml</code> file as above but this time start 2 Ray Jobs:</p> <pre><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \\\n  sfttrainer_generate_dataset_vision --image-width 384  --image-height 384 \\\n  -o /data/fms-hf-tuning/artificial-dataset/vision-384x384-16384plus-entries-4096.parquet\n\n\nray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \\\n  sfttrainer_generate_dataset_vision --image-width 384  --image-height 768 \\\n  -o /data/fms-hf-tuning/artificial-dataset/vision-384x768-16384plus-entries-4096.parquet\n</code></pre>"},{"location":"actuators/sft-trainer/#model-weights","title":"Model Weights","text":"<p>The actuator supports model weights from both the HuggingFace repository and local directories. You can find the full list of supported models in the models.yaml file.</p> <p>Note</p> <p>The actuator attempts to cache Hugging Face model weights the first time it runs an operation that references them. To avoid race conditions when running multiple experiments with the same weights, we recommend pre-fetching the weights in advance.</p> <p>Identify the models you want to cache and then create a <code>models.yaml</code> file structured as a double-nested dictionary.</p> <ul> <li>The outer dictionary keys are the names of the models.</li> <li>Each inner dictionary maps model weight types to their corresponding   Hugging Face identifiers.</li> </ul> <p>Supported model weight types include:</p> <ul> <li><code>Vanilla</code></li> <li><code>QPTQ-Quantized</code></li> </ul> <p>Here\u2019s a simple example that caches the <code>HuggingFaceTB/SmolLM2-135M</code> model weights from HuggingFace:</p> <pre><code>smollm2-135m:\n  Vanilla: HuggingFaceTB/SmolLM2-135M\n</code></pre> <p>Next, choose a directory to use as your HuggingFace home. By default, SFTTrainer uses <code>/hf-models-pvc/huggingface_home</code>. To override this, set the <code>hf_home</code> parameter in your ActuatorConfiguration resource just like you did for overriding the location of dataset files.</p> <p>For example, to cache the model weights under <code>/my/hf_home/</code> use the following command:</p> <pre><code>sfttrainer_download_hf_weights -i models.yaml -o /my/hf_home\n</code></pre> <p>If you are working with a remote RayCluster then submit a Ray job similar to the above section for generating datasets:</p> <p>Info</p> <p>If you are using a remote RayCluster on Kubernetes remember to start a port-forward to the RayCluster head node.</p> <pre><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \\\n  sfttrainer_download_hf_weights -i models.yaml -o /my/hf_home\n</code></pre>"},{"location":"actuators/sft-trainer/#configure-your-raycluster-for-rdma-over-converged-ethernet-roce","title":"Configure your RayCluster for RDMA over Converged Ethernet (RoCE)","text":"<p>RoCE enables high-throughput, low-latency communication between GPU nodes distributed on multiple nodes by bypassing the kernel and reducing CPU overhead. This is especially beneficial for multi-node AI workloads that rely on fast inter-GPU communication, such as distributed training with NVIDIA NCCL.</p> <p>To enable RoCE in a RayCluster on Kubernetes, you need to:</p> <ol> <li>Build a GPU worker image with the necessary OFED and NCCL libraries.</li> <li>Configure the RayCluster custom resource to:</li> </ol> <ul> <li>Set environment variables for NCCL that switch on the RoCE feature.</li> <li>Mount the NCCL topology file to ensure optimal GPU-to-GPU communication      paths are used during collective operations.</li> </ul> <ol> <li>Ensure the Kubernetes nodes and network are RoCE-capable and properly    configured.</li> </ol> <p>Here\u2019s the revised and improved list of prerequisites for enabling RoCE in GPU workers of a RayCluster on Kubernetes, incorporating clarity, completeness, and technical accuracy:</p>"},{"location":"actuators/sft-trainer/#prerequisites","title":"Prerequisites","text":"<ul> <li>The system administrator has configured the GPU nodes and network   infrastructure to support RoCE, including BIOS, firmware, switch settings, and   lossless Ethernet features.</li> <li>The NCCL topology file is provided by the system administrator to optimize   GPU communication paths.</li> <li>The Kubernetes administrator has granted the RayCluster service account   appropriate RBAC permissions and PodSecurity settings necessary for RoCE. In   this example we will:</li> <li>Run containers as <code>root</code>.</li> <li>Use the <code>IPC_LOCK</code> capability to lock memory.</li> <li>The device plugin for RoCE-capable NICs (e.g., NVIDIA Network Operator or   custom RDMA plugin) is installed and configured on the cluster.</li> <li>The GPU worker has the required drivers and libraries. In this example, we   will deploy Ray on a Kubernetes cluster. Thus, our image will contain:</li> <li>OFED modules</li> <li>the NVIDIA and NCCL runtime binaries</li> <li>The system administrator has shared the number of GPUs and RoCE-capable   NICs available per node to guide resource requests and topology mapping.</li> <li>The Kubernetes administrator has explained how to:</li> <li>Request RoCE devices (e.g., <code>nvidia.com/roce_gdr: 2</code>)</li> <li>Enable pods to access the RDMA-enabled network zones</li> <li>Schedule GPU workers on the correct nodes e.g via labels, taints, affinity     rules, etc</li> </ul>"},{"location":"actuators/sft-trainer/#install-the-required-libraries-and-drivers","title":"Install the required libraries and drivers","text":"<p>This example walks you through deploying a RayCluster on Kubernetes, including building a custom image for the GPU worker nodes. We\u2019ll use the <code>mirror.gcr.io/rayproject/ray:latest-py310-cu121</code> base image, which includes both Ray and the necessary NVIDIA libraries.</p> <pre><code>ARG base_image=mirror.gcr.io/rayproject/ray:latest-py310-cu121\nFROM $base_image\n\nUSER 0\n\nENV MOFED_VER=24.10-1.1.4.0\nENV OS_VER=ubuntu22.04\nENV PLATFORM=x86_64\n\nRUN mkdir app &amp;&amp; \\\n    cd app &amp;&amp; \\\n    wget -q http://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VER}/MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM}.tgz &amp;&amp; \\\n    tar -xvzf MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM}.tgz &amp;&amp; \\\n    cd MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM} &amp;&amp; \\\n    ./mlnxofedinstall --user-space-only --without-fw-update --without-ucx-cuda --all --force --distro $OS_VER &amp;&amp; \\\n    cd .. &amp;&amp; \\\n    rm -rf MLNX* &amp;&amp; \\\n    apt-get -y clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Note</p> <p>Mellanox OFED is now in long-term support and will reach end-of-life in Q4 2027. NVIDIA has replaced it with DOCA-OFED, which will receive all future updates and features. This example currently uses MLNX_OFED, but we'll update it with DOCA-OFED installation steps in a future revision.</p>"},{"location":"actuators/sft-trainer/#collect-all-necessary-information","title":"Collect all necessary information","text":""},{"location":"actuators/sft-trainer/#identify-roce-capable-network-devices","title":"Identify RoCE-Capable Network Devices","text":"<p>To determine which network interfaces support RoCE v2, run the <code>show_gids</code> command on a GPU node. Look for entries where the <code>VER</code> column is <code>v2</code>, which indicates RoCE v2 support.</p> <p>For example, given the following output:</p> <pre><code>DEV    PORT INDEX GID                                   IPv4         VER DEV\n---    ---- ----- ---                                   ------------ --- ---\nmlx5_0 1       0 fe80:0000:0000:0000:0000:60ff:fe68:d096              v1 net1-0\nmlx5_0 1       1 fe80:0000:0000:0000:0000:60ff:fe68:d096              v2 net1-0\n...\nmlx5_3 1       1 fe80:0000:0000:0000:0000:5fff:fe68:d09a              v2 net1-1\n</code></pre> <p>You should select the devices with <code>v2</code> under the <code>VER</code> column. In this case, the RoCE-capable devices are:</p> <ul> <li><code>mlx5_0_1</code></li> <li><code>mlx5_3_1</code></li> </ul> <p>You will use these device names to set the <code>NCCL_IB_HCA</code> environment variable in your Ray GPU worker pods. For the above example you will set <code>NCCL_IB_HCA=\"=mlx5_0,mlx5_3\"</code></p> <p>You also need to configure <code>NCCL_IB_GID_INDEX</code>. Select the GID index such that it maps to a v2 entry across all nodes to ensure consistent behavior. For the above example you will set <code>NCCL_IB_GID_INDEX=1</code></p>"},{"location":"actuators/sft-trainer/#putting-it-all-together","title":"Putting it all together","text":"<p>In this section we will use the information we gathered above to define a Ray GPU worker with support for RoCE.</p>"},{"location":"actuators/sft-trainer/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Enable memory locking in containers</li> </ol> <ul> <li>Request the <code>IPC_LOCK</code> capability in your container\u2019s security context.</li> <li>Use a <code>ServiceAccount</code> (e.g. <code>gdr</code>) that grants permission to request      <code>IPC_LOCK</code>.</li> <li>To allow unlimited memory locking:<ul> <li>Option A: Run the container as root (in the example we assume that    the <code>roce</code> service account has adequate RBAC to request this).</li> <li>Option B: Configure the node with <code>ulimit -l unlimited</code> (not    available on Vela).</li> </ul> </li> </ul> <ol> <li>Attach and request RoCE-capable NICs</li> </ol> <ul> <li>On our cluster:<ul> <li>We add the annotation: <code>k8s.v1.cni.cncf.io/networks: multi-nic-network</code></li> <li>Request RoCE devices: <code>nvidia.com/roce_gdr: 2</code></li> </ul> </li> </ul> <ol> <li>Set NCCL environment variables</li> </ol> <ul> <li>Configure variables like <code>NCCL_IB_HCA</code>, <code>NCCL_IB_GID_INDEX</code>, and others to      enable RoCE and optimize performance.</li> </ul> <ol> <li>Mount the NCCL topology file</li> </ol> <ul> <li>Mount the <code>topology-roce</code> ConfigMap at <code>/var/run/nvidia-topologyd</code>.</li> </ul> <pre><code># ... trimmed ...\nworkerGroupSpecs:\n  - rayStartParams:\n    block: \"true\"\n    num-gpus: \"8\"\n    # VV: We'll use the RoCE custom resource to ensure the jobs land on a properly configured node for RoCE\n    # we support running up to 1 RoCE measurement. Similarly, we have a custom resource called\n    # \"full-worker\" for reserving the entire GPU worker if necessary.\n    resources:\n      \"\\\"{\\\\\\\"NVIDIA-A100-SXM4-80GB\\\\\\\": 8, \\\\\\\"full-worker\\\\\\\": 1,\n      \\\\\\\"RoCE\\\\\\\": 1}\\\"\"\n# Here, we configure an eightou GPU worker with A100 that can have up to 4 replicas\nreplicas: 4\nminReplicas: 4\nmaxReplicas: 4\nnumOfHosts: 1\ngroupName: eight-A100-80G-gpu-WG\ntemplate:\n  metadata:\n    annotations:\n      # We use this annotation on our cluster to get access to the appropriate network zone\n      k8s.v1.cni.cncf.io/networks: multi-nic-network\n    labels:\n      helm.sh/chart: ray-cluster-1.1.0\n      app.kubernetes.io/instance: ray-disorch\n  # ServiceAccount gives your pod adequate RBAC to request the IPC_LOCK capability and run as root\n  serviceAccountName: roce\n  # RoCE requires root privileges.\n  # An alternative to using a root account is to request the capability CAP_SYS_RESOURCE and\n  # run `ulimit -l unlimited` before starting up the Ray worker\n  securityContext:\n    fsGroup: 0\n    runAsGroup: 0\n    runAsNonRoot: false\n    runAsUser: 0\n  volumes:\n    volumes:\n      - name: topology-volume\n        configMap:\n          name: topology-roce\n      - name: dshm\n        emptyDir:\n          medium: Memory\n    # Add your remaining PVCs here e.g. a GPFS volume for storing the\n    # HF_HOME path that you will use with \"accelerate launch\" etc\n  containers:\n    - name: pytorch\n      image: $YOUR_ROCE_ENABLED_IMAGE_HERE\n      imagePullPolicy: Always\n      securityContext:\n        allowPrivilegeEscalation: false\n        capabilities:\n          add:\n            - IPC_LOCK # for RoCE to work\n      env:\n        # To enable RoCE\n        - name: NCCL_IB_HCA\n          value: =mlx5_0,mlx5_3\n        - name: NCCL_IB_GID_INDEX\n          value: \"1\"\n        - name: NCCL_IB_DISABLE\n          value: \"0\" # Set this to \"1\" to disable RoCE\n        # To visually verify that RoCE is On based on the logs that NCCL prints\n        - name: NCCL_DEBUG\n          value: INFO\n        - name: NCCL_DEBUG_SUBSYS\n          value: \"INIT,BOOTSTRAP,ENV\"\n        # Remaining NCCL environment variables we use on our cluster\n        - name: NCCL_IB_QPS_PER_CONNECTION\n          value: \"8\"\n        - name: NCCL_IB_SPLIT_DATA_ON_QPS\n          value: \"0\"\n        - name: NCCL_IB_PCI_RELAXED_ORDERING\n          value: \"1\"\n        - name: NCCL_ALGO\n          value: Ring\n        - name: NCCL_IGNORE_CPU_AFFINITY\n          value: \"1\"\n        - name: NCCL_SOCKET_NTHREADS\n          value: \"2\"\n        - name: NCCL_CROSS_NIC\n          value: \"0\"\n        - name: OMP_NUM_THREADS\n          value: \"16\"\n  volumeMounts:\n    - name: topology-volume\n      mountPath: /var/run/nvidia-topologyd\n    # Your other volumemounts here\n    - name: dshm\n      mountPath: \"/dev/shm\"\n  resources:\n    # Here we are requesting an entire node\n    requests:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 720Gi\n      nvidia.com/roce_gdr: 2\n    limits:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 720Gi\n      nvidia.com/roce_gdr: 2\n</code></pre> <p>Note</p> <p>We recommend enabling RoCE only for GPU workers that occupy an entire Kubernetes node. This ensures that multi-node jobs are using separate Kubernetes nodes, allowing RoCE to be effectively utilized.</p>"},{"location":"actuators/sft-trainer/#verify-youre-using-roce","title":"Verify you're using RoCE","text":"<p>Remember, RoCE only applies to multi-node jobs. To verify that it\u2019s working, run a multi-node NCCL job and inspect the logs. If your GPU workers are properly configured for RoCE, you should see output similar to the snippet below. We\u2019ve annotated the important lines with <code>&lt;--</code> and added comments to highlight what to look for.</p> <pre><code>&lt;!-- markdownlint-disable line-length --&gt;\n[0] NCCL INFO NCCL_IB_DISABLE set by environment to 0. &lt;-- double check that this is set to 0\n[4] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_3 &lt;-- This does not confirm that you are using RoCE\n[3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_3:1/RoCE [RO]; OOB net1-0:1.2.3.21&lt;0&gt; &lt;-- Name of the NICs\n                                                                                                and /RoCE\n[3] NCCL INFO Using non-device net plugin version 0\n[3] NCCL INFO Using network IB &lt;-- Uses the IB network\n&lt;!-- markdownlint-enable line-length --&gt;\n</code></pre> <p>NCCL falls back to \"Socket\" network when RoCE is unavailable. In this scenario your log output will be similar to the snippet below.</p> <pre><code>&lt;!-- markdownlint-disable line-length --&gt;\n[1] NCCL INFO NCCL_IB_DISABLE set by environment to 0. &lt;-- if this is set to 1, you will NOT use RoCE even\n                                                           if it is properly configured\n[1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to net1-0,net1-1\n[1] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_3\n[1] NCCL INFO NET/IB : No device found. # &lt;-- No network Infiniband network found\n[1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to net1-0,net1-1\n[1] NCCL INFO NET/Socket : Using [0]net1-0:1.2.3.30&lt;0&gt; [1]net1-1:1.2.4.30&lt;0&gt; # &lt;-- No mention of /RoCE\n                                                                                   or the NICs\n[1] NCCL INFO Using non-device net plugin version 0\n[2] NCCL INFO Using network Socket # &lt;-- Switches to TCP\n&lt;!-- markdownlint-enable line-length --&gt;\n</code></pre> <p>Note</p> <p>You might see warnings indicating that NCCL failed to load certain .so files. These messages are harmless and unrelated to RoCE configuration. You can ignore them safely.</p>"},{"location":"actuators/sft-trainer/#next-steps","title":"Next steps","text":"<ul> <li> <p>\u2699\ufe0f Customize Actuators using ActuatorConfiguration resources</p> <p>Learn how to use ActuatorConfiguration resources to customize the SFTTrainer Operations</p> <p>ActuatorConfiguration documentation</p> </li> <li> <p>\ud83d\udda5\ufe0f Ready to try it out?</p> <p>The SFTTrainer actuator can run experiments locally as well. Just follow the example below to get started:</p> <p> Run a local fine-tuning experiment</p> </li> <li> <p> Take it to the next level</p> <p>Do you have a RayCluster with GPUs in it?</p> <p> Run a fine-tuning experiment on a remote RayCluster</p> </li> </ul>"},{"location":"actuators/working-with-actuators/","title":"Actuators overview","text":"<p>An actuator is a code module that provides experiment protocols that can measure properties of entities. See actuators for more details on what an actuator is and read discoveryspaces to learn how they are used to create <code>discoveryspaces</code>.</p> <p>This section covers how you install and configure actuators, create new actuators to extend <code>ado</code> as well as specific documentation for various actuators available.</p> <p>You can also add your own custom experiments using the special actuator custom_experiments</p> <p>Info</p> <p>Most actuators are plugins. They are pieces of code that can be installed independently from <code>ado</code> and that <code>ado</code> can dynamically discover. Custom experiments are also plugins.</p>"},{"location":"actuators/working-with-actuators/#listing-available-actuators","title":"Listing available Actuators","text":"<p>To see a list of available actuators execute</p> <pre><code>ado get actuators\n</code></pre> <p>to see the experiments each provides</p> <pre><code>ado get actuators --details\n</code></pre>"},{"location":"actuators/working-with-actuators/#special-actuators-replay-and-custom_experiments","title":"Special actuators: replay and custom_experiments","text":"<p><code>ado</code> has two special builtin actuators: <code>custom_experiments</code> and <code>replay</code>.</p> <p><code>custom_experiments</code> allows users to create experiments from python functions without having to write a full Actuator. The creating custom experiments page describes this in detail.</p> <p>The <code>replay</code> actuator allows you to use property values from experiments that were performed outside of <code>ado</code> i.e. no Actuator exists to measure them. Often you might want to perform some analysis on a <code>discoveryspace</code> using these values or to perform a search using an objective-function defined on these values. See the replay actuator page to learn more about how to do this.</p>"},{"location":"actuators/working-with-actuators/#actuator-plugins","title":"Actuator Plugins","text":"<p>Anyone can extend <code>ado</code> with actuator plugins. All actuator plugins are python packages (see creating actuator classes) and can be installed in the usual ways with <code>pip</code>.</p>"},{"location":"actuators/working-with-actuators/#actuator-plugins-distributed-with-ado","title":"Actuator plugins distributed with <code>ado</code>","text":"<p>The following actuators are distributed with <code>ado</code>:</p> <ul> <li>SFTTrainer: An actuator for testing foundation model   fine-tuning performance</li> <li>vllm_performance:   An actuator for testing foundation model inference performance</li> </ul>"},{"location":"actuators/working-with-actuators/#installing-actuator-plugins","title":"Installing actuator plugins","text":"<p>See our installing plugins documentation.</p>"},{"location":"actuators/working-with-actuators/#dynamic-installation-of-actuators-on-a-remote-ray-cluster","title":"Dynamic installation of actuators on a remote Ray cluster","text":"<p>If you are running <code>ado</code> operations on a remote ray cluster, as ray job, you may want, or need, to dynamically install an actuator plugin or its latest version. This is described in the running ado on a remote ray cluster.</p> <p>Some additional notes about this process when you are developing an actuator:</p> <ul> <li>Make sure plugin code changes are committed (if using <code>setuptools_scm</code> for   versioning)</li> <li>If they are not committed then the version of the built wheel will not     change i.e. it will be same as for a wheel built before the changes</li> <li>If a wheel with this version was already installed in ray cluster by a     previous job, ray will use the cached version, and not your new version</li> <li>Make sure new files you want to package with the wheel are committed</li> <li>The setup.py for the plugins only adds committed non-python files</li> </ul>"},{"location":"actuators/working-with-actuators/#whats-next","title":"What's next","text":"<ul> <li> <p> Try our examples</p> <p>Explore using some of these actuators with our examples.</p> <p>Our examples </p> </li> <li> <p> Learn about Operators</p> <p>Learn about extending ado with new Operators.</p> <p>Creating new Operators </p> </li> </ul>"},{"location":"core-concepts/actuators/","title":"Actuators, Experiments & Measurement Spaces","text":""},{"location":"core-concepts/actuators/#experiments","title":"Experiments","text":"<p>To find the values of certain properties of Entities we need to perform measurements on them. We use the term \"experiment\" to describe a particular type of measurement. This could also be called an \"experiment protocol\".</p> <p>An experiment will define its inputs - the set of constitutive and observed properties it requires entities to have. It will also define the properties it measures.</p>"},{"location":"core-concepts/actuators/#actuators","title":"Actuators","text":"<p>Experiments are provided by Actuators. An Actuator usually provides sets of experiments that work on the same types of entities i.e. have the same or similar input requirements. As such Actuators usually are related to a particular domain e.g. computational chemistry, foundation model inference, robotic biology lab.</p> <p><code>ado get actuators --details</code> lists the available actuators and experiments. Here is a truncated example of the output:</p> <pre><code>                  ACTUATOR ID                 CATALOG ID                                  EXPERIMENT ID  SUPPORTED\n0         molecule-embeddings                 Embeddings                   calculate-morgan-fingerprint       True\n1          molformer-toxicity         molformer-toxicity                               predict-toxicity       True\n2                     mordred         Mordred Descriptor                  mordred-descriptor-calculator       True\n3                       st4sd                      ST4SD                      toxicity-prediction-opera       True\n4                       st4sd                      ST4SD                   band-gap-pm3-gamess-us:1.0.0       True\n5   materials-model-evaluator  materials-model-evaluator                          evaluate_with_clintox       True\n6   materials-model-evaluator  materials-model-evaluator                      evaluate_sider_for_target       True\n7      caikit-config-explorer     caikit-config-explorer                                     fmaas-perf       True\n8      caikit-config-explorer     caikit-config-explorer                   fmaas-perf-composable-gigaio       True\n</code></pre> <p>One of the primary ways to extend <code>ado</code> is to develop new Actuators providing the ability to do experiments on entities in a new domain.</p>"},{"location":"core-concepts/actuators/#example-experiment-from-the-sfttrainer-actuator","title":"Example: Experiment from the SFTTrainer actuator","text":"<p>Here is an example description of an experiment from the SFTTrainer actuator.</p> <pre><code>Identifier: SFTTrainer.finetune-full-fsdp-v1.6.0\n\nMeasures the performance of full-fine tuning a model with FSDP+flash-attention for a given (GPU model, number GPUS, batch_size, model_max_length) combination.\n\nInputs:\n  Constitutive Properties:\n      dataset_id\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['news-chars-1024-entries-1024', 'news-chars-1024-entries-256', 'news-chars-1024-entries-4096', 'news-chars-2048-entries-1024', 'news-chars-2048-entries-256', 'news-chars-2048-entries-4096', 'news-chars-512-entries-1024', 'news-chars-512-entries-256', 'news-chars-512-entries-4096', 'news-tokens-128kplus-entries-320', 'news-tokens-128kplus-entries-4096', 'news-tokens-16384plus-entries-4096']\n\n\n      model_name\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['granite-13b-v2', 'granite-20b-v2', 'granite-34b-code-base', 'granite-3b-1.5', 'granite-3b-code-base-128k', 'granite-7b-base', 'granite-8b-code-base', 'granite-8b-code-base-128k', 'granite-8b-japanese', 'hf-tiny-model-private/tiny-random-BloomForCausalLM', 'llama-13b', 'llama-7b', 'llama2-70b', 'llama3-70b', 'llama3-8b', 'llama3.1-405b', 'llama3.1-70b', 'llama3.1-8b', 'mistral-7b-v0.1', 'mixtral-8x7b-instruct-v0.1']\n\n\n      model_max_length\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [1, 131073]\n\n      torch_dtype\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE Values: ['bfloat16']\n\n      number_gpus\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [2, 9]\n\n      gpu_model\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['NVIDIA-A100-SXM4-80GB', 'NVIDIA-A100-80GB-PCIe', 'Tesla-T4', 'L40S', 'Tesla-V100-PCIE-16GB']\n\n\n      batch_size\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [1, 129]\n\n\nOutputs:\n  finetune-full-fsdp-v1.6.0-gpu_compute_utilization_min\n  finetune-full-fsdp-v1.6.0-gpu_compute_utilization_avg\n  finetune-full-fsdp-v1.6.0-gpu_compute_utilization_max\n  finetune-full-fsdp-v1.6.0-gpu_memory_utilization_min\n  finetune-full-fsdp-v1.6.0-gpu_memory_utilization_avg\n  finetune-full-fsdp-v1.6.0-gpu_memory_utilization_max\n  finetune-full-fsdp-v1.6.0-gpu_memory_utilization_peak\n  finetune-full-fsdp-v1.6.0-gpu_power_watts_min\n  finetune-full-fsdp-v1.6.0-gpu_power_watts_avg\n  finetune-full-fsdp-v1.6.0-gpu_power_watts_max\n  finetune-full-fsdp-v1.6.0-gpu_power_percent_min\n  finetune-full-fsdp-v1.6.0-gpu_power_percent_avg\n  finetune-full-fsdp-v1.6.0-gpu_power_percent_max\n  finetune-full-fsdp-v1.6.0-cpu_compute_utilization\n  finetune-full-fsdp-v1.6.0-cpu_memory_utilization\n  finetune-full-fsdp-v1.6.0-train_runtime\n  finetune-full-fsdp-v1.6.0-train_samples_per_second\n  finetune-full-fsdp-v1.6.0-train_steps_per_second\n  finetune-full-fsdp-v1.6.0-train_tokens_per_second\n  finetune-full-fsdp-v1.6.0-train_tokens_per_gpu_per_second\n  finetune-full-fsdp-v1.6.0-model_load_time\n  finetune-full-fsdp-v1.6.0-dataset_tokens_per_second\n  finetune-full-fsdp-v1.6.0-dataset_tokens_per_second_per_gpu\n  finetune-full-fsdp-v1.6.0-is_valid\n</code></pre> <p>The SFTTrainer actuator provides experiments which measure the performance of different fine-tuning techniques on a foundation model fine-tuning deployment configuration. Therefore, the entities it takes as input represent fine-tuning deployment configuration.</p>"},{"location":"core-concepts/actuators/#example-experiment-from-the-st4sd-actuator","title":"Example: Experiment from the ST4SD actuator","text":"<p>Here is an example description of an experiment from the ST4SD actuator.</p> <pre><code>Identifier: st4sd.band-gap-pm3-gamess-us:1.0.0\n\nRequired Inputs:\n  Constitutive Properties:\n      smiles\n      Domain:\n        Type: UNKNOWN_VARIABLE_TYPE\n\n\n\nOutputs:\n  band-gap-pm3-gamess-us:1.0.0-band-gap\n  band-gap-pm3-gamess-us:1.0.0-homo\n  band-gap-pm3-gamess-us:1.0.0-lumo\n  band-gap-pm3-gamess-us:1.0.0-electric-moments\n  band-gap-pm3-gamess-us:1.0.0-total-energy\n</code></pre> <p>The ST4SD actuator provides experiments which perform computational measurements of entities, often molecules. Therefore, the entities it takes as input often represent molecules.</p>"},{"location":"core-concepts/actuators/#experiment-inputs","title":"Experiment Inputs","text":"<p>Experiments define their inputs they require along with valid values for those inputs.</p>"},{"location":"core-concepts/actuators/#required-inputs","title":"Required Inputs","text":"<p>Experiments can define required inputs. There are properties an Entity must have values for, for it to be a valid input to the Experiment.</p> <p>For example for <code>SFTTrainer.finetune-full-fsdp-v1.6.0</code> shown above we can see it requires an Entity to have 7 constitutive properties defined: <code>dataset_id</code>, <code>model_name</code>, <code>model_max_length</code>, <code>torch_dtype</code>, <code>number_gpus</code>, <code>gpu_model</code>, and <code>batch_size</code>. Each one has a domain which defines the allowed values for that property - if an Entity has a value for a property that is not in the defined domain the experiment cannot run on it.</p> <p>For example, the <code>number_gpu</code> property can only have the values 2,3,4,5,6,7 and 8 (range is exclusive of upper bound)</p> <pre><code>      number_gpus\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [2, 9]\n</code></pre> <p>All the required inputs in the examples above are constitutive properties. However, they can also be observed properties (see next section) i.e. properties measured by other experiments. If an Experiment, <code>B</code> has a required input that is an observed property it means the experiment measuring that property has to be run on an Entity before Experiment <code>B</code> can be run on it.</p>"},{"location":"core-concepts/actuators/#optional-properties","title":"Optional Properties","text":"<p>Experiment can also define optional properties. These are properties an Entity can have but if they don't the Experiment will give it a default value. In addition, the default values of optional properties can be overridden to create parameterized experiments. This is described further in the <code>discoveryspace</code> resource documentation.</p> <p>An example experiment with optional properties is</p> <pre><code>Identifier: robotic_lab.peptide_mineralization\n\nMeasures adsorption of peptide lanthanide combinations\n\nRequired Inputs:\n  Constitutive Properties:\n      peptide_identifier\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['test_peptide', 'test_peptide_new']\n\n\n      peptide_concentration\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE\n        Values: [0.1, 0.4, 0.6, 0.8]\n        Range: [0.1, 1.8]\n\n\n      lanthanide_concentration\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE\n        Values: [0.1, 0.4, 0.6, 0.8]\n        Range: [0.1, 1.8]\n\n\n\nOptional Inputs and Default Values:\n  temperature\n  Domain:\n    Type: CONTINUOUS_VARIABLE_TYPE Range: [0, 100]\n\n  Default value: 23.0\n\n  replicas\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [1, 4]\n\n  Default value: 1.0\n\n  robot_identifier\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['harry', 'hermione']\n\n  Default value: hermione\n\n\nOutputs:\n  peptide_mineralization-adsorption_timeseries\n  peptide_mineralization-adsorption_plateau_value\n</code></pre> <p>Here you can see three optional properties, <code>temperature</code>, <code>replicas</code> and <code>robot_identifier</code> that are given default values.</p>"},{"location":"core-concepts/actuators/#target-and-observed-properties","title":"Target and Observed Properties","text":"<p>Experiments define properties the properties they measure. However, there may be many experiments that measure the same property in different ways so we need a way to differentiate them.</p> <p>The properties the experiment targets measuring are called <code>target properties</code>, and the properties it actually measures <code>observed properties</code>. If experiment <code>A</code> has target property <code>X</code>, then the observed property is <code>A-X</code> i.e. the value of target property <code>X</code> measured by experiment <code>A</code>.</p> <p>Looking at the definitions above for the <code>st4sd.band-gap-pm3-gamess-us:1.0.0</code> experiment we can see a target is <code>band-gap</code> and the corresponding observed property is <code>band-gap-pm3-gamess-us:1.0.0-band-gap</code></p>"},{"location":"core-concepts/actuators/#measurement-space","title":"Measurement Space","text":"<p>A measurement space is simply a set of experiments.</p> <p>Since each experiment has a set of observed properties, a measurement space also defines a set of observed properties.</p> <p>Since each observed property is an observation of a target property, a measurement space also defines a set of target properties.</p>"},{"location":"core-concepts/concepts/","title":"Concepts","text":""},{"location":"core-concepts/concepts/#discovery-space","title":"Discovery Space","text":"<p>The core concept in <code>ado</code> is called a Discovery Space. In <code>ado</code> you are often creating and performing operations on Discovery Spaces.</p> <p>For users familiar with <code>pandas</code> and <code>dataframes</code>, a Discovery Space combines:</p> <ul> <li>the schema of a <code>dataframe</code> i.e. the columns and what they mean</li> <li>instructions on how to fill the <code>dataframe</code> rows</li> <li>the current data in the <code>dataframe</code> (and what's missing!)</li> </ul> <p>So a Discovery Space allows expressing the hidden metadata and contextual information necessary to understand and extend a dataframe. See Discovery Space for more details.</p> <p>A Discovery Space is built from:</p> <ul> <li>Entities and Entity Spaces: The set of things in a   Discovery Space</li> <li>Measurement Spaces: The set of experiments   in a Discovery Space</li> <li>Experiments and Actuators: The available experiments and the   tools that execute them</li> </ul>"},{"location":"core-concepts/concepts/#sample-store","title":"Sample Store","text":"<p>In <code>ado</code> data on sampled entities, and the results of experiments on them, are kept in a sample store.</p> <p>A single sample store can be used by multiple Discovery Spaces, allowing them to share data. This means, for example, if an experiment has already been run, <code>ado</code> can reuse the existing results instead of running the experiment again, saving time and compute resources.</p> <p>This ability to transparently share and reuse data is a core feature of <code>ado</code>. See Shared Sample Stores for more details.</p>"},{"location":"core-concepts/concepts/#whats-next","title":"What's next","text":"<ul> <li> <p> Learn about resources</p> <p>Next go to resources to learn more about working with these core-concepts in <code>ado</code>.</p> <p>ado resources </p> </li> <li> <p> Try our examples</p> <p>Try some or our examples if you want to dive straight in.</p> <p>Our examples </p> </li> </ul>"},{"location":"core-concepts/data-sharing/","title":"Shared Sample Stores","text":"<p>Note</p> <p>We recommend to read about Discovery Spaces before reading this document.</p> <p>In <code>ado</code> Entities and measurement results are stored in a database called Sample Store. This document describes how Sample Stores enable sharing of data. For more general information about these databases see their dedicated page.</p> <p>From the point-of-view of understanding data reuse in <code>ado</code>, the following points are key:</p> <ul> <li>You can share a Sample Store between multiple Discovery Spaces</li> <li>This allows a Discovery Space to (re)use relevant Entities and Measurements     placed in the Sample Store by operations on other Discovery Spaces</li> <li>Entities are always shared. There is only one entry in a Sample Store for   an Entity</li> </ul> <p>Note</p> <p>To maximise the chance of data-reuse, similar Discovery Spaces should use the same Sample Store. However, Discovery Spaces do not have to be similar to use the same Sample Store.</p>"},{"location":"core-concepts/data-sharing/#when-data-can-be-shared-in-ado","title":"When data can be shared in <code>ado</code>","text":"<p>There are two situations where data can be shared between Discovery Spaces in <code>ado</code>:</p> <ul> <li>Data Retrieval: retrieving data about entities and measurements from the   Discovery Space e.g. <code>ado show entities space</code></li> <li>Data Generation: When performing an explore operation on a Discovery   Space - we call data reuse in this case <code>memoization</code></li> </ul>"},{"location":"core-concepts/data-sharing/#how-ado-determines-what-data-can-be-shared","title":"How <code>ado</code> determines what data can be shared","text":"<p>As a quick recap, a Discovery Space is composed of:</p> <ul> <li>an Entity Space which describes a set of Entities (points)   to be measured</li> <li>a Measurement Space which describes a set of   Experiments to apply to the points</li> </ul>"},{"location":"core-concepts/data-sharing/#entities","title":"Entities","text":"<p>Each Entity in the Entity Space has a unique identifier, usually determined by its set of constitutive property values. For example, if an Entity has two constitutive properties <code>X</code> an <code>Y</code> with values 4 and 10, its id will be 'X:4-Y:10'. Since the identifiers of all the Entities in the Entity Space are known, the Sample Store can be searched to see if it contains a record for any of the Entities.</p>"},{"location":"core-concepts/data-sharing/#measurements","title":"Measurements","text":"<p>Each Experiment in a Measurement Space also has a unique identifier, determined from its base name plus any optional properties that have been explicitly set. When an Entity is retrieved from the Sample Store, it contains results of all the experiments that have been applied to it. If the identifier of a result matches the identifier of an Experiment in the Measurement Space, <code>ado</code> determines it can be reused.</p>"},{"location":"core-concepts/data-sharing/#data-sharing-and-data-retrieval","title":"Data sharing and data retrieval","text":"<p>When retrieving data from a Discovery Space, e.g. via <code>ado show entities</code>, you are actually retrieving data from the Sample Store that matches the Discovery Space. When determining what data to retrieve there are two situations to consider:</p> <ul> <li>measured: retrieve only Entities and measurements that were sampled via an   operation on the given Discovery Space</li> <li>this can be considered the \"no sharing\" mode. If an Entity or measurement     exists in the Sample Store that's compatible with the Discovery Space, but     no operation on the Discovery Space ever visited it, the \"measured\" mode     will not show it</li> <li>matching: retrieve all Entities and measurements that match the Discovery   Space</li> <li>this can be considered the \"sharing\" mode.</li> </ul>"},{"location":"core-concepts/data-sharing/#data-sharing-and-memoization","title":"Data sharing and memoization","text":"<p>Important</p> <p>Each explore operator should provide a way to turn memoization on and off. Check the operator documentation.</p> <p>This section explains how data sharing and reuse works during an explore operation - a feature called memoization. It's recommended you check the documentation on operations and explore operators.</p> <p>Briefly, an explore operation samples a point in the Entity Space of a Discovery Space and applies the experiments in the Measurement Space to it. In detail, the sampling process is as follows:</p> <ul> <li>An Entity is sampled from the Entity Space</li> <li>The Entity's record is retrieved from the Sample Store if present (via its   unique identifier)</li> <li>If memoization is on</li> <li>for each experiment in the MeasurementSpace, <code>ado</code> checks if a result for it     already exists (via the experiment's unique identifier)<ul> <li>if it does, the result is reused. If there is more than one result, they   are all reused</li> </ul> </li> <li>if memoization is off</li> <li>Existing results are ignored. Each experiment in the Measurement Space is     applied again to the Entity. The new results are added to any existing.</li> </ul>"},{"location":"core-concepts/discovery-spaces/","title":"Discovery Spaces","text":"<p>A Discovery Space is made up of an <code>Entity Space</code> and a <code>Measurement Space</code>. The <code>Entity Space</code> defines the things you want to measure and the <code>Measurement Space</code> how you want to measure them.</p> <p>A Discovery Space is also associated with a Sample Store where measurement results and entities are recorded.</p>"},{"location":"core-concepts/discovery-spaces/#example-fine-tuning-deployment-configuration-discovery-space","title":"Example: Fine-Tuning Deployment Configuration Discovery Space","text":"<p>We can combine the fine-tuning deployment configuration Entity Space example here with one of the experiments from the <code>SFTTrainer</code> actuator to create the following Discovery Space:</p> <pre><code>Identifier: space-edf5e2-2351e8\n\nEntity Space:\n\n  Number entities: 80\n  Categorical properties:\n              name                                values\n    0   dataset_id  [news-tokens-16384plus-entries-4096]\n    1   model_name                           [llama3-8b]\n    2  torch_dtype                            [bfloat16]\n    3    gpu_model               [NVIDIA-A100-80GB-PCIe]\n\n  Discrete properties:\n                   name        range interval                         values\n    0       number_gpus       [2, 5]     None                         [2, 4]\n    1  model_max_length  [512, 8193]     None  [512, 1024, 2048, 4096, 8192]\n    2        batch_size     [1, 129]     None  [1, 2, 4, 8, 16, 32, 64, 128]\n\n\n\nMeasurement Space:\n\n                                                    experiment  supported                    target-property\n  0   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True        gpu_compute_utilization_min\n  1   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True        gpu_compute_utilization_avg\n  2   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True        gpu_compute_utilization_max\n  3   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True         gpu_memory_utilization_min\n  4   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True         gpu_memory_utilization_avg\n  5   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True         gpu_memory_utilization_max\n  6   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True        gpu_memory_utilization_peak\n  7   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True            cpu_compute_utilization\n  8   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True             cpu_memory_utilization\n  9   SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True                      train_runtime\n  10  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True           train_samples_per_second\n  11  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True             train_steps_per_second\n  12  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True            train_tokens_per_second\n  13  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True    train_tokens_per_gpu_per_second\n  14  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True                    model_load_time\n  15  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True          dataset_tokens_per_second\n  16  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True  dataset_tokens_per_second_per_gpu\n  17  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0       True                           is_valid\n\n\n\nSample Store identifier: '2351e8'\n</code></pre> <p>Here we can see:</p> <ul> <li>A unique id for the discovery space</li> <li>The entity space</li> <li>For each experiment in the measuerment space (in this case just one) the   target properties it measures.</li> </ul>"},{"location":"core-concepts/discovery-spaces/#sampling-and-measurement","title":"Sampling and Measurement","text":"<p>A Discovery Space created with an empty Sample Store has no data associated with it i.e. no sampled and measured entities. Adding data requires applying an operation, like a Random Walk, to the Discovery Space. This operation samples entities from the Entity Space, measures them according to the Measurement Space experiments, and places the results into the Sample Store.</p> <p>Therefore, at a given point of time a Discovery Space will have some number of</p> <ul> <li>sampled and measured entities</li> <li>sampled and unmeasured entities (because the measurements failed)</li> <li>unsampled entities</li> </ul> <p>The first two will have corresponding data in the Sample Store.</p>"},{"location":"core-concepts/discovery-spaces/#comparison-discovery-space-and-a-dataframe","title":"Comparison: Discovery Space and a DataFrame","text":"<p>Comparing a Discovery Space and a DataFrame can help understand the concept and also illustrate the benefits</p>"},{"location":"core-concepts/discovery-spaces/#a-discovery-space-defines-a-dataframe-schema","title":"A Discovery Space defines a DataFrame schema","text":"<p>When you create a Discovery Space you can imagine you have created a DataFrame schema where:</p> <ol> <li>There are Columns for each entity space dimension</li> <li>There are Columns for each measurement space property</li> <li>Each row is an entity</li> </ol> <p>If we were to look at the example fine-tuning deployment configuration Discovery Space this would look like (the rows and columns have been truncated)</p> model_id gpu_type batch_size model_max_length number_gpus ... finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0.dataset_tokens_per_second finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0.gpu_memory_utilization_peak ... lama3-8b NVIDIA-A100-80GB-PCIe 2 512 2 ... UNK UNK ... lama3-8b NVIDIA-A100-80GB-PCIe 4 512 2 ... UNK UNK ... lama3-8b NVIDIA-A100-80GB-PCIe 8 512 2 ... UNK UNK ... ... ... ... ... ... ... ... ... ... <p>This DataFrame has 80 rows, one for each entity, and (4+3+17) columns, one for each of the 7 constitutive properties and the 17 target properties of <code>finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0.</code></p> <p>We can fill all the entity space columns for all the rows as we know the full space. No measurements have taken place so all the measurement values are unknown</p>"},{"location":"core-concepts/discovery-spaces/#a-discovery-space-defines-how-to-fill-all-the-data-in-the-dataframe","title":"A Discovery Space defines how to fill all the data in the DataFrame","text":"<p>In the above example the columns associated with the measurement space have no data. However, the DiscoverySpace defines exactly how to get this data, as it defines the actual experiments, supplied by actuators, that you can execute to get it.</p> <p>Using the Discovery Space at any point we can choose a row (entity) with no measurement and get the measurements</p>"},{"location":"core-concepts/discovery-spaces/#a-discovery-space-populates-the-schema-from-a-shared-external-source","title":"A Discovery Space populates the schema from a shared external source","text":"<p>A Discovery Space is a view rather than a container.</p> <p>This means when you generate a DataFrame from a DiscoverySpace the data in the rows is fetched from a shared-source. If someone else measured an entity that corresponds to one of the rows in your DataFrame it will be automatically populated.</p> <p>As operations are run on a Discovery Space the rows in the table become filled in. You can choose to look at:</p> <ol> <li>Rows filled in by operations on this space (Entities sampled and measured via    this Discovery Space)</li> <li>Rows filled in by operations on on other spaces (Entities sampled and    measured via any Discovery Space using same Sample Store)</li> <li>Rows not filled in at all (Unmeasured entities)</li> </ol>"},{"location":"core-concepts/discovery-spaces/#summary","title":"Summary","text":"Method Column Definition Defines how to acquire missing data? Data Sharing DataFrame Ad-Hoc. The data-frame creator defines the columns when its is created. The meaning of the columns must be communicated separately, Not defined. The DataFrame just holds data Not possible. A DataFrame is a static object Discovery Space Defined by the discovery space. A set of Entity Space columns and Measurement Space columns. Yes ,defined by the MeasurementSpace Yes, values are loaded from a distributed shared db on demand"},{"location":"core-concepts/entity-spaces/","title":"Entities and Entity Spaces","text":""},{"location":"core-concepts/entity-spaces/#entities","title":"Entities","text":"<p>Entities represent things that can be measured. Examples are molecules or points in an application configuration space.</p> <p>Entities all have a set of constitutive properties which define them. A molecule's constitutive properties might be a SMILES or INCHI string. The constitutive properties of a fine-tuning deployment configuration might be GPU model, number of GPUs and batch size.</p> <p>An entity will also have observed properties. These are properties measured by an experiment (or experiment protocol). For example, a molecule might have an observed properties for its <code>band-gap</code> while a fine-tuning deployment configuration might have an observed properties related to <code>tokens throughput</code>.</p>"},{"location":"core-concepts/entity-spaces/#example-fm-fine-tuning-deployment-configuration","title":"Example: FM Fine-tuning Deployment Configuration","text":"<p>Here's an example of an entity that represents a FM fine-tuning deployment</p> <pre><code>Identifier: dataset_id.news-tokens-16384plus-entries-4096-model_name.llama3-8b-number_gpus.4.0-model_max_length.2048.0-torch_dtype.bfloat16-batch_size.16.0-gpu_model.NVIDIA-A100-80GB-PCIe\nGenerator: explicit_grid_sample_generator\n\nConstitutive properties:\n                 name                               value\n  0        dataset_id  news-tokens-16384plus-entries-4096\n  1        model_name                           llama3-8b\n  2       number_gpus                                 4.0\n  3  model_max_length                              2048.0\n  4       torch_dtype                            bfloat16\n  5        batch_size                                16.0\n  6         gpu_model               NVIDIA-A100-80GB-PCIe\n\nObserved properties:\n                                                   name                                         experiment                    target-property                values\n  0   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...        gpu_compute_utilization_min   [98.14772727272727]\n  1   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...        gpu_compute_utilization_avg   [98.26988636363636]\n  2   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...        gpu_compute_utilization_max   [98.38636363636364]\n  3   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...         gpu_memory_utilization_min  [33.709723284090906]\n  4   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...         gpu_memory_utilization_avg  [33.709723284090906]\n  5   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...         gpu_memory_utilization_max  [33.709723284090906]\n  6   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...        gpu_memory_utilization_peak           [34.065475]\n  7   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...            cpu_compute_utilization   [98.94999999999999]\n  8   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...             cpu_memory_utilization  [6.3182326931818205]\n  9   finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...                      train_runtime            [887.5672]\n  10  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...           train_samples_per_second               [4.615]\n  11  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...             train_steps_per_second               [0.072]\n  12  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...            train_tokens_per_second            [9451.236]\n  13  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...    train_tokens_per_gpu_per_second            [2362.809]\n  14  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...                    model_load_time                [-1.0]\n  15  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...          dataset_tokens_per_second   [9451.237044361262]\n  16  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...  dataset_tokens_per_second_per_gpu  [2362.8092610903154]\n  17  finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0-...  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-defa...                           is_valid                 [1.0]\n\nAssociated experiments:\n\n  SFTTrainer.finetune-lora-fsdp-r-4-a-16-tm-default-v1.2.0\n</code></pre> <p>For more about the meaning of <code>observed properties</code> see target &amp; observed properties</p>"},{"location":"core-concepts/entity-spaces/#entity-spaces","title":"Entity Spaces","text":"<p>An Entity Space describes a set of entities. The set could be discrete or continuous, bounded or unbounded. In <code>ado</code> you normally define Entity Spaces and then sample Entities from them.</p>"},{"location":"core-concepts/entity-spaces/#example-molecules","title":"Example: Molecules","text":"<p>This space has a single dimension with type identifier. This is a property whose values are a, potential very large, set of unique-ids generated in some fashion.</p> <pre><code>  Space with non-discrete dimensions. Cannot count entities\n  Identifier properties:\n         name\n    0  smiles\n</code></pre>"},{"location":"core-concepts/entity-spaces/#example-fine-tuning-deployment-configuration","title":"Example: Fine-tuning Deployment Configuration","text":"<p>This space has 7 dimensions, 4 categorical and 3 discrete. Each of the 4 categorical dimensions has only a single value. The discrete dimensions each have a range of values they can take.</p> <pre><code> Number entities: 80\n  Categorical properties:\n              name                                values\n    0   dataset_id  [news-tokens-16384plus-entries-4096]\n    1   model_name                [granite-8b-code-base]\n    2  torch_dtype                            [bfloat16]\n    3    gpu_model               [NVIDIA-A100-80GB-PCIe]\n\n  Discrete properties:\n                   name        range interval                         values\n    0       number_gpus       [2, 5]     None                         [2, 4]\n    1  model_max_length  [512, 8193]     None  [512, 1024, 2048, 4096, 8192]\n    2        batch_size     [1, 129]     None  [1, 2, 4, 8, 16, 32, 64, 128]\n</code></pre>"},{"location":"core-concepts/entity-spaces/#property-domains","title":"Property Domains","text":"<p>Each property in an entity space can be associated with a domain. The domain is the range of values the property can take and also the probability of those values. In the <code>Fine-tuning Deployment Configuration</code> example we can see the domains for each property. The categorical properties have a set of values and the discrete properties a range and also a set of values.</p> <p>In the <code>Molecules</code> example we see there is no domain, which means any value of <code>smiles</code> is allowed. When there is no domain it also means the Entity Space alone does not contain sufficient information by itself on how to sample the entities.</p> <p>By default, the probability is uniform, every value is equally likely, but it could also be more complex.</p>"},{"location":"examples/best-configuration-search/","title":"Optimizations with ado","text":"<p>Note</p> <p>This example demonstrates:</p> <ol> <li> <p>Creating and installing custom experiments</p> </li> <li> <p>Performing optimizations with <code>ray_tune</code></p> </li> <li> <p>Parameterizable and parameterized experiments</p> </li> </ol> <p>Note</p> <p>We recommend trying the talking a random walk example first to get familiar with some basic concepts and commands.</p>"},{"location":"examples/best-configuration-search/#the-scenario","title":"The scenario","text":"<p>Finding the best entity, or point, according to some metric, is a common task. For example, finding the configuration of an LLM fine-tuning workload that gives the highest throughput. Many optimization methods have been developed to address this problem and you can access a variety of them via <code>ado</code>'s <code>ray_tune</code> operator, which provides access to the RayTune framework.</p> <p>This example demonstrates running optimizations in <code>ado</code> using the problem of finding the minimum of standard optimization test functions.</p> <p>Caution</p> <p>The commands below assume you are in the directory <code>examples/optimization_test_functions</code> in the ado source repository. See the instructions for cloning the repository.</p>"},{"location":"examples/best-configuration-search/#setup","title":"Setup","text":""},{"location":"examples/best-configuration-search/#install-the-ray_tune-ado-operator","title":"Install the ray_tune ado operator","text":"<p>If you haven't already installed the ray_tune operator, run (assumes you are in <code>examples/optimization-test-functions/</code> ):</p> <pre><code>pip install ../../plugins/operators/ray_tune\n</code></pre> <p>then executing</p> <pre><code>ado get operators\n</code></pre> <p>should show an entry for <code>ray_tune</code> like below</p> <pre><code>Available operators by type:\n      OPERATOR     TYPE\n0  random_walk  explore\n1     ray_tune  explore\n</code></pre>"},{"location":"examples/best-configuration-search/#install-the-custom-nevergrad_opt_3d_test_func-experiment","title":"Install the custom <code>nevergrad_opt_3d_test_func</code> experiment","text":"<p>The <code>nevergrad_opt_3d_test_func</code> experiment enables measuring the following optimization test functions on a 3d space: 'discus', 'sphere', 'cigar', 'griewank', 'rosenbrock', 'st1'. See the nevergrad docs for definitions of these functions.</p> <p>To install it:</p> <pre><code>pip install custom_experiments/\n</code></pre> <p>after this running <code>ado get actuators --details</code> should show the following line:</p> <pre><code>1   custom_experiments  CustomExperiments                             nevergrad_opt_3d_test_func       True\n</code></pre> <p>and <code>ado describe experiment nevergrad_opt_3d_test_func</code> should output</p> <pre><code>Identifier: custom_experiments.nevergrad_opt_3d_test_func\n\nRequired Inputs:\n  Constitutive Properties:\n      x0\n      Domain:\n        Type: CONTINUOUS_VARIABLE_TYPE\n\n      x1\n      Domain:\n        Type: CONTINUOUS_VARIABLE_TYPE\n\n      x2\n      Domain:\n        Type: CONTINUOUS_VARIABLE_TYPE\n\n\nOptional Inputs and Default Values:\n  name\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['discus', 'sphere', 'cigar', 'griewank', 'rosenbrock', 'st1']\n\n\n  Default value: rosenbrock\n\n  num_blocks\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [1, 10]\n\n  Default value: 1\n\n\nOutputs: nevergrad_opt_3d_test_func-function_value\n</code></pre>"},{"location":"examples/best-configuration-search/#running-the-example","title":"Running the example","text":""},{"location":"examples/best-configuration-search/#set-active-context","title":"Set active context","text":"<p>You can use any context, for examples <code>ado</code>'s default local context:</p> <pre><code>ado context local\n</code></pre>"},{"location":"examples/best-configuration-search/#create-the-discovery-space","title":"Create the discovery space","text":"<p>The file \"space.yaml\" contains an example space describing the rosenbrock function in 3d, from [-10,10] in each dimension. To create the space execute</p> <p>Then:</p> <pre><code>ado create space -f space.yaml --set \"sampleStoreIdentifier=$SAMPLE_STORE_IDENTIFIER\"\n</code></pre> <p>where <code>$SAMPLE_STORE_IDENTIFIER</code> is the identifier of the samplestore you want to use to store the results.</p> <p>Note</p> <p>This can be the same <code>samplestore</code> used in another example.</p> <p><code>samplestores</code> can store samples and measurement from multiple different experiments and <code>discoveryspaces</code>.</p> <p>This will output a <code>discoveryspace</code> id you can use to run an optimization operation. In the following sections we will refer to this id as <code>$DISCOVERY_SPACE_IDENTIFIER</code>.</p> <p>Assuming you did not modify <code>space.yaml</code> running <code>ado describe space $DISCOVERY_SPACE_IDENTIFIER</code> will output (identifiers will be different):</p> <pre><code>Identifier: space-f529ab-85161d\n\nEntity Space:\n\n  Space with non-discrete dimensions. Cannot count entities\n  Continuous properties:\n      name      range\n    0   x2  [-10, 10]\n    1   x1  [-10, 10]\n    2   x0  [-10, 10]\n\n\nMeasurement Space:\n\n                                         experiment  supported\n  0  custom_experiments.nevergrad_opt_3d_test_func       True\n\n\n  'custom_experiments.nevergrad_opt_3d_test_func'\n\n\n  Inputs:\n      parameter      type       value parameterized\n  0          x0  required        None            na\n  1          x1  required        None            na\n  2          x2  required        None            na\n  3        name  optional  rosenbrock         False\n  4  num_blocks  optional           1         False\n\n\n  Outputs:\n\n    target property\n  0  function_value\n\n\n\nSample Store identifier: '85161d'\n</code></pre> <p>Here we see,</p> <ul> <li>the Entity Space is a 3-dimensional space, with continuous dimensions,   spanning [-10,10] in each dimension.</li> <li>the Measurement Space, describing the measurements to apply to each point in   the space, contains one experiment - in this case the   <code>custom_experiments.nevergrad_opt_3d_test_func</code>.</li> <li>The <code>custom_experiments.nevergrad_opt_3d_test_func</code> experiment defines one   metric, <code>function_value</code>.</li> <li>Since the default function used by   <code>custom_experiments.nevergrad_opt_3d_test_func</code> is <code>rosenbrock</code>, for a given   point <code>function_value</code> will be the value of the 3d <code>rosenbrock</code> function at   that point.</li> </ul> <p>Also try:</p> <pre><code>ado get spaces\n</code></pre> <p>This will output a list of the spaces created. If this is the first time you have are following this example it will contain one entry, the identifier of the space you just created above.</p>"},{"location":"examples/best-configuration-search/#run-an-optimization","title":"Run an optimization","text":"<p>The file <code>operationn_ax.yaml</code> is an example of running the Ax optimizer via RayTune. To run execute the following,</p> <pre><code>ado create operation -f operation_ax.yaml --set \"spaces[0]=$DISCOVERY_SPACE_IDENTIFIER\"\n</code></pre> <p>replacing <code>$DISCOVERY_SPACE_IDENTIFIER</code> with the identifier obtained in the previous step.</p> <p>This will run the optimization for 40 steps. You will see a lot of information from RayTune on the progress of the optimization. At the end you will see output like the following containing the identifier of the operation, here its <code>raytune-0.9.3.dev16+b415dca6.dirty-nevergrad-af2af4</code>.</p> <pre><code>Space ID: space-ccf2bf-a50274\nSample Store ID: sqlite:////Users/michaelj/Library/Application%20Support/ado/databases/local-test.db/sqlsource_a50274\nOperation:\n  config:\n    actuatorConfigurationIdentifiers: []\n    metadata: {}\n    operation:\n      module:\n        moduleClass: RayTune\n        moduleName: ado_ray_tune.operator\n        modulePath: .\n        moduleType: operation\n      parameters:\n        tuneConfig:\n          max_concurrent_trials: 4\n          metric: function_value\n          mode: min\n          num_samples: 40\n          search_alg:\n            name: nevergrad\n            params:\n              optimizer: CMA\n    spaces:\n      - space-ccf2bf-a50274\ncreated: \"2025-05-27T11:30:16.285703Z\"\nidentifier: raytune-0.9.3.dev16+b415dca6.dirty-nevergrad-af2af4\nkind: operation\nmetadata:\n  entities_submitted: 40\n  experiments_requested: 40\noperationType: search\noperatorIdentifier: raytune-0.9.3.dev16+b415dca6.dirty\nstatus:\n  - event: created\n    recorded_at: \"2025-05-27T11:30:07.576406Z\"\n  - event: added\n    recorded_at: \"2025-05-27T11:30:16.286967Z\"\n  - event: started\n    recorded_at: \"2025-05-27T11:30:16.289563Z\"\n  - event: finished\n    exit_state: success\n    recorded_at: \"2025-05-27T11:32:16.640484Z\"\n  - event: updated\n    recorded_at: \"2025-05-27T11:32:16.641605Z\"\nversion: v1\n</code></pre>"},{"location":"examples/best-configuration-search/#specifying-the-property-to-optimize","title":"Specifying the property to optimize","text":"<p>In this case there is one experiment with one property in the measurement space, so there is only one choice for the property to optimize against i.e. <code>function_value</code>. However, usually an experiment will measure many properties and there may be many measurements.</p> <p>The target property to optimize against is set by the <code>metric</code> field, under the operations <code>parameters</code> field.</p> <pre><code>parameters:\n  tuneConfig:\n    metric: \"function_value\" # The experiment property/metric to optimize against\n    mode: \"min\"\n    num_samples: 40\n    max_concurrent_trials: 4 # This is set for debugging. Increase if you want multiple measurements at once.\n    search_alg:\n      name: nevergrad\n      params:\n        optimizer: \"CMA\"\n</code></pre>"},{"location":"examples/best-configuration-search/#see-the-optimization-results","title":"See the optimization results","text":""},{"location":"examples/best-configuration-search/#best-configuration-found","title":"Best configuration found","text":"<p>The <code>ray_tune</code> operation will create a <code>datacontainer</code> resource containing information on the best configuration found.</p> <p>To get the id of the <code>datacontainer</code> related to the <code>operation</code> use:</p> <pre><code>ado show related operation $OPERATION_IDENTIFIER\n</code></pre> <p>This will output something like:</p> <pre><code>datacontainer\n  - datacontainer-d6a6501b\ndiscoveryspace\n  - space-047b6a-f60613\n</code></pre> <p>To see the best point found (and in general the contents of the datacontainer) use the <code>describe</code> CLI command:</p> <pre><code>ado describe datacontainer $DATACONTAINER_ID\n</code></pre> <p>In this case the output will be something like:</p> <pre><code>Identifier: datacontainer-d6a6501b\nBasic Data:\n\n  Label: best_result\n\n  {'config': {'x2': -1.1192905253425014,\n    'x1': 2.081208150586974,\n    'x0': 0.5621591414422049},\n   'metrics': {'function_value': 20.788056393697595,\n    'timestamp': 1756804287,\n    'checkpoint_dir_name': None,\n    'done': True,\n    'training_iteration': 1,\n    'trial_id': '7a7153ed',\n    'date': '2025-09-02_10-11-27',\n    'time_this_iter_s': 1.0576610565185547,\n    'time_total_s': 1.0576610565185547,\n    'pid': 52036,\n    'hostname': 'Michaels-MacBook-Pro-2.local',\n    'node_ip': '127.0.0.1',\n    'config': {'x2': -1.1192905253425014,\n     'x1': 2.081208150586974,\n     'x0': 0.5621591414422049},\n    'time_since_restore': 1.0576610565185547,\n    'iterations_since_restore': 1,\n    'experiment_tag': '40_x0=0.5622,x1=2.0812,x2=-1.1193'},\n   'error': None}\n</code></pre> <p>We can see here that the point found is <code>{'x2': -1.1192905253425014, 'x1': 2.081208150586974, 'x0': 0.5621591414422049}</code> where <code>function_value</code> was ~20.8.</p>"},{"location":"examples/best-configuration-search/#configurations-visited","title":"Configurations visited","text":"<p>To see the configurations visited during the optimization, execute:</p> <pre><code>ado show entities operation $OPERATION_IDENTIFIER\n</code></pre> <p>where <code>$OPERATION_IDENTIFIER</code> is the identifier of the operation you just ran. This will output a dataframe containing the results of that operation.</p>"},{"location":"examples/best-configuration-search/#operation-resource-yaml","title":"Operation resource YAML","text":"<p>If at any point you want to see the details for an operation, for example the options used, execute:</p> <pre><code>ado get operation $OPERATION_IDENTIFIER -o yaml\n</code></pre> <p>This will output the details of this operation in YAML format - this will be the same YAML as shown in the previous section.</p>"},{"location":"examples/best-configuration-search/#parameterizable-experiments","title":"Parameterizable experiments","text":"<p>The <code>nevergrad_opt_3d_test_func</code> is an example of a parameterizable experiment. A parameterizable experiment has optional inputs that have default values. In this case the optional inputs are <code>name</code> and <code>num_blocks</code> which you can see are listed in the output of <code>ado describe experiment</code> here. In particular the \"name\" parameter defines the optimization test function the experiment will use and its default value is 'rosenbrock'.</p> <p>If you want to set a different value for an optional parameter of an experiment you do this when creating the <code>discoveryspace</code>. For example to set the function to <code>cigar</code> you would write (snippet from full <code>discoveryspace</code> yaml)</p> <pre><code>- actuatorIdentifier: custom_experiments\n  experimentIdentifier: nevergrad_opt_3d_test_func\n  parameterization:\n    - value: \"cigar\"\n      property:\n        identifier: \"name\"\n</code></pre> <p>When you set an optional property of a parameterizable experiment we call the result a parameterized experiment.</p> <p>Note</p> <p>You can't change the parameterization of an experiment in an existing <code>discoveryspace</code> as this changes the measurement and hence the entire space. Using an experiment with a new parameterization requires creating a new <code>discoveryspace</code>.</p>"},{"location":"examples/best-configuration-search/#exploring-further","title":"Exploring Further","text":"<p>Try the following:</p> <ul> <li>change optimizer: The file <code>optimization_nevergrad.yaml</code> shows using the CMA   optimizer from nevergrad. Modify and run in the same way as the Ax example</li> <li>different results views: Use <code>ado show entities space $SPACE_ID</code> where   <code>SPACE_ID</code> is the identifier of the space the operations run on. Compare to   the output of <code>ado show entities operation</code></li> <li>modify the entity space: Extending or limiting the dimensions of the entity   space considered</li> <li>change optimizer options: Change the optimization options and run another   optimization. See   the ray tune operator documentation   for details and further examples on what can be configured.</li> </ul> <ul> <li>parameterize the experiment: Perform an optimization on the <code>discus</code>   function - this involves parameterizing the   <code>nevergrad_opt_3d_test_func</code>.</li> <li>See how this changes the description of <code>discoveryspace</code>.</li> <li>discretize the space: Run the optimization on a discretized version of one   of the functions and see if memoization works. Hint: change the entity   space.</li> <li>find the minimum across all test-functions: It's possible to search for   which test function has the minimum value across the entity space in a single   run. Hint: you can use any experiment parameters as entity-space dimensions.</li> </ul>"},{"location":"examples/best-configuration-search/#extending-the-nevergrad_opt_3d_test_func-experiment","title":"Extending the <code>nevergrad_opt_3d_test_func</code> experiment","text":"<p>The <code>nevergrad_opt_3d_test_func</code> experiment can be expanded to include more functions or options. It is also straightforward to add custom experiment for more dimensions. See the documentation for custom experiments to find out more.</p> <p>If you change what the function does consider the name of the</p> <p>experiment. If it is not changed in some way the experiment will have the same name as an existing used experiment but do something different which is problematic.</p>"},{"location":"examples/best-configuration-search/#takeaways","title":"Takeaways","text":"<ul> <li>create-explore-view pattern: A common pattern in <code>ado</code> is to create a   <code>discoveryspace</code> to describe a set of points to measure, create <code>operations</code>   on it to explore or analyse it, and then view the results</li> <li>optimization: <code>ado</code> provides an interface to RayTune allowing all the   optimizers supported by RayTune to be used to explore <code>discoveryspaces</code></li> <li>parameterized experiments: Experiments can have optional parameters you   can set to change what they do. When experiment is parameterized it will have   a different id including the parameterization to differentiate it from the   base experiment.</li> <li>custom experiments: You can add your own python functions as experiments   using <code>ado</code>'s custom experiments feature.</li> <li>continuous dimensions: <code>ado</code> supports <code>discoveryspaces</code> with continuous   dimensions - however in this case memoization is unlikely to provide benefit   as the chances of visiting the same space twice are remote.</li> </ul>"},{"location":"examples/best-configuration-search/#whats-next","title":"What's next","text":"<ul> <li> <p> Creating custom objective functions</p> <p>Try the Search a space based on a custom objective function example to see how you can define a custom experiment which uses inputs from another experiment.</p> <p>Search a space based on a custom objective function </p> </li> <li> <p> Discovering important entity space dimensions</p> <p>Try the Identify the important dimensions of a space example to see how you can use <code>ado</code> to discover which entity space dimensions most influence a target metric.</p> <p>Identify the important dimensions of a space </p> </li> </ul>"},{"location":"examples/examples/","title":"Examples","text":""},{"location":"examples/examples/#tutorial","title":"Tutorial","text":"<p>Our short tutorial, Taking a random walk, introduces core <code>ado</code> concepts and is the recommend place to start.</p>"},{"location":"examples/examples/#general-examples","title":"General Examples","text":"<p>The following examples illustrate general features of <code>ado</code>. They build on the concepts learned in the tutorial and leverage pre-existing data and/or toy measurements so they run quickly.</p> <ul> <li>Search a space with an optimizer</li> <li>Search a space based on a custom objective function</li> <li>Identify the important dimensions of a space</li> </ul> <p>After following these examples you can also try applying capabilities learned in one example to another.</p>"},{"location":"examples/examples/#foundation-models-characterization","title":"Foundation Models Characterization","text":"<p>The following examples illustrate using the vllm_performance and SFTTrainer actuators which provide benchmarking experiments for foundation model inference and fine-tuning respectively.</p> <ul> <li>Measure throughput of fine-tuning locally</li> </ul>"},{"location":"examples/examples/#adding-experiments-or-analysis-tools-to-ado","title":"Adding experiments or analysis tools to <code>ado</code>","text":"<p>The search a space based on a custom objective function example, combines with the creating a custom experiment documentation to illustrate a simple method for adding your own experiments to <code>ado</code>.</p> <p>For adding actuators, we provide an example template actuator repository that can be used with our documentation on writing actuators.</p> <p>For adding operators, we have an example template operator repository that can be used with our documentation on writing operators.</p>"},{"location":"examples/examples/#whats-next","title":"What's next","text":"<ul> <li> <p> Learn about Core Concepts</p> <p>Find out more about the core concepts underpinning ado.</p> <p>Core concepts </p> </li> <li> <p> Extend ado with new Actuators</p> <p>Learn about how ado can be extended with custom Actuators that provide ability to run experiments in new domains.</p> <p>Creating new Actuators </p> </li> </ul>"},{"location":"examples/finetune-locally/","title":"Measure throughput of finetuning locally","text":"<p>Note</p> <p>This example illustrates:</p> <ol> <li> <p>Set up a local environment for running finetuning performance benchmarks    with SFTTrainer</p> </li> <li> <p>Benchmarking a set of finetuning configurations for a small model using a    local context and only the CPU</p> </li> </ol>"},{"location":"examples/finetune-locally/#the-scenario","title":"The scenario","text":"<p>When you run a finetuning workload, you can choose values for parameters like the model name, batch size, and number of GPUs. To understand how these choices affect performance, a common strategy is to measure changes in system behavior by exploring the workload parameter space.</p> <p>This approach applies to many machine learning workloads where performance depends on configuration.</p> <p>In this example, <code>ado</code> is used to explore the parameter space for finetuning a small language model on your laptop without using GPUs.</p> <p>To explore this space, you will:</p> <ul> <li>define the parameters to test - such as the batch size and the model max   length</li> <li>define what to test them with - In this case we will use SFTTrainer's   <code>finetune_full_benchmark-v1.0.0</code> experiment</li> <li>explore the parameter space - the sampling method</li> </ul> <p>Here, you'll use the <code>finetune_full_benchmark-v1.0.0</code> experiment that the SFTTrainer actuator provides to run four measurements on your laptop without using GPUs. Each measurement records metrics like <code>dataset tokens per second</code> and stores the results in <code>ado</code>'s database.</p>"},{"location":"examples/finetune-locally/#pre-requisites","title":"Pre-requisites","text":""},{"location":"examples/finetune-locally/#set-active-context","title":"Set Active Context","text":"<p>You should use the <code>local</code> context for the example.</p> <pre><code>ado context local\n</code></pre>"},{"location":"examples/finetune-locally/#install-the-sfttrainer-actuator","title":"Install the SFTTrainer actuator","text":"Install the SFTTrainer Actuator plugin from PyPiInstall SFTTrainer from the <code>ado</code> sources <p> Run <code>pip install ado-sfttrainer</code> to install the SFTTrainer actuator plugin using the wheel that we push to PyPi. </p> <p>Info</p> <p>This step assumes you are in the root directory of the ado source repository.</p> <p>If you haven't already installed the <code>SFTTrainer</code> actuator, run (assumes you are in the root directory of ado):</p> <pre><code>pip install plugins/actuators/sfttrainer\n</code></pre> <p>then executing</p> <pre><code>ado get actuators\n</code></pre> <p>should show an entry for <code>SFTTrainer</code> like below</p> <pre><code>           ACTUATOR ID\n0   custom_experiments\n1                 mock\n2               replay\n3           SFTTrainer\n</code></pre>"},{"location":"examples/finetune-locally/#configure-the-parameters-of-the-sfttrainer-actuator","title":"Configure the parameters of the SFTTrainer actuator","text":"<p>SFTTrainer includes parameters that control its behavior. For example, it pushes any training metrics it collects, like system profiling metadata, to an AIM server by default. It also features parameters that define important paths, such as the location of the Hugging Face cache and the directory where the actuator expects to find files like the test dataset.</p> <p>In this section you will configure the actuator for running experiments locally and storing data under the path <code>/tmp/ado-sft-trainer-hello-world/</code>.</p> <p>Create a file called <code>actuator_configuration.yaml</code> with the following contents:</p> <pre><code>actuatorIdentifier: SFTTrainer\nparameters:\n  match_exact_dependencies: False\n  data_directory: /tmp/ado-sft-trainer-hello-world/data-files\n  cache: /tmp/ado-sft-trainer-hello-world/cache\n  hf_home: ~/.cache/huggingface\n</code></pre> <p>To create the <code>actuatorconfiguration</code> resource run:</p> <pre><code>ado create actuatorconfiguration -f actuator_configuration.yaml\n</code></pre> <p>The command will print the ID of the resource. Make a note of it, you will need it in a later step.</p> <p>See the full list of parameters you can set in an <code>actuatorconfiguration</code> resource for the SFTTrainer actuator in its reference docs.</p>"},{"location":"examples/finetune-locally/#environment-setup","title":"Environment setup","text":""},{"location":"examples/finetune-locally/#create-the-dataset","title":"Create the Dataset","text":"<p>The finetuning measurements require a synthetic dataset which is a file named <code>news-tokens-16384plus-entries-4096.jsonl</code> in the directory <code>/tmp/ado-sft-trainer-hello-world/data-files</code> which is under the path specified by the <code>data_directory</code> actuator parameter.</p> <p>You can reuse this Dataset for any future measurements you run on this device.</p> <p>To generate the dataset run the following command:</p> <pre><code>sfttrainer_generate_dataset_text \\\n  -o /tmp/ado-sft-trainer-hello-world/data-files/news-tokens-16384plus-entries-4096.jsonl\n</code></pre>"},{"location":"examples/finetune-locally/#download-model-weights","title":"Download model weights","text":"<p>Next download the weights of the model we use in this example (<code>smollm2-135m</code>) in the appropriate path under the directory specified by the <code>hf_home</code> parameter of the SFTTrainer actuator.</p> <p>First, store the below YAML to the file <code>models.yaml</code> inside your working directory:</p> <pre><code>smollm2-135m:\n  Vanilla: HuggingFaceTB/SmolLM2-135M\n</code></pre> <p>Then, run the command:</p> <pre><code>sfttrainer_download_hf_weights -i models.yaml -o ~/.cache/huggingface\n</code></pre>"},{"location":"examples/finetune-locally/#run-the-example","title":"Run the example","text":"<p>This section explains the process of using <code>ado</code> to define and launch a set of finetuning measurements which store their results in the <code>local</code> context.</p>"},{"location":"examples/finetune-locally/#define-the-finetuning-workload-configurations-to-test-and-how-to-test-them","title":"Define the finetuning workload configurations to test and how to test them","text":"<p>A <code>discoveryspace</code> defines what you want to measure (Entity Space) and how you want to measure it (Measurement Space). It also links to the <code>samplestore</code> which is where Entities and their measured properties are stored in.</p> <p>In this example, we create a <code>discoveryspace</code> that runs the finetune_full_benchmark-v1.0.0 experiment to finetune the <code>smollm2-135m</code> model without using any GPUs.</p> <p>The <code>entitySpace</code> defined below includes four dimensions:</p> <ul> <li><code>model_name</code> and <code>number_gpus</code> each contain a single value.</li> <li><code>model_max_length</code> and <code>batch_size</code> each contain two values.</li> </ul> <p>The total number of entities in the <code>entitySpace</code> is the number of unique combinations of values across all dimensions. In this case, the configuration contains 4 entities.</p> <p>You can find the complete list of the entity space properties in the documentation of the finetune_full_benchmark-v1.0.0 experiment.</p> <p>To create the Discovery Space:</p> <ol> <li>Create the file <code>space.yaml</code> with the following content</li> </ol> <p><pre><code># if you do not have a Sample Store we provide a command-line that will create one for you\nsampleStoreIdentifier: Replace this with the identifier of your sample store\n\nexperiments:\n  - experimentIdentifier: finetune_full_benchmark-v1.0.0\n    actuatorIdentifier: SFTTrainer\n    parameterization:\n      - property:\n          identifier: fms_hf_tuning_version\n        value: \"2.8.2\"\n      - property:\n          identifier: stop_after_seconds\n        value: 30\n      - property:\n          identifier: flash_attn\n        value: False\n\nentitySpace:\n  - identifier: \"model_name\"\n    propertyDomain:\n      values: [\"smollm2-135m\"]\n  - identifier: \"number_gpus\"\n    propertyDomain:\n      values: [0]\n  - identifier: \"model_max_length\"\n    propertyDomain:\n      values: [512, 1024]\n  - identifier: \"batch_size\"\n    propertyDomain:\n      values: [1, 2]\n</code></pre> </p> <ol> <li>Create the space:</li> </ol> <ul> <li> <p>If you have an <code>samplestore</code> ID, run:</p> <pre><code>ado create space -f space.yaml --set \"sampleStoreIdentifier=$SAMPLE_STORE_IDENTIFIER\"\n</code></pre> </li> <li> <p>If you do not have a <code>samplestore</code> then run</p> <pre><code>ado create space -f space.yaml --new-sample-store\n</code></pre> </li> </ul> <p>This will print a <code>discoveryspace</code> ID (e.g., <code>space-ea937f-831dba</code>). Make a    note of this ID, you'll need it in the next step.</p>"},{"location":"examples/finetune-locally/#create-a-random-walk-operation-to-explore-the-space","title":"Create a random walk <code>operation</code> to explore the space","text":"<ol> <li>Create the file <code>operation.yaml</code> with the following content:</li> </ol> <pre><code>spaces:\n  - The identifier of the DiscoverySpace resource\nactuatorConfigurationIdentifiers:\n  - The identifier of the Actuator Configuration resource\n\noperation:\n  module:\n    operatorName: \"random_walk\"\n    operationType: \"search\"\n  parameters:\n    numberEntities: all\n    singleMeasurement: True\n    mode: sequential\n    samplerType: generator\n</code></pre> <ol> <li>Replace the placeholders with your <code>discoveryspace</code> ID and    <code>actuatorconfiguration</code> ID and save it in a file with the name    <code>operation.yaml</code></li> <li>Create the operation</li> </ol> <pre><code>ado create operation -f operation.yaml\n</code></pre> <p>The operation will execute the measurements (i.e. apply the experiment finetune_full_benchmark-v1.0.0 on the 4 entities) based on the definition of your <code>discoveryspace</code>. The remaining three measurements will reuse both the cached model weights and the cached data, making them faster to complete.</p> <p>Info</p> <p> Each measurement takes about two minutes to complete, with a total of four measurements. Ray will also take a couple of minutes to build the Ray runtime environment on participating ray workers so expect the operation to take O(10) minutes to complete.</p>"},{"location":"examples/finetune-locally/#examine-the-results-of-the-exploration","title":"Examine the results of the exploration","text":"<p>After the operation completes, you can download the results of your measurements:</p> <pre><code>ado show entities --output-format csv --property-format=target space $yourDiscoverySpaceID\n</code></pre> <p>The command will generate a CSV file. Open it to explore the data that your operation has collected!</p> <p>It should look similar to this:</p> <pre><code>,identifier,generatorid,experiment_id,model_name,number_gpus,model_max_length,batch_size,gpu_compute_utilization_min,gpu_compute_utilization_avg,gpu_compute_utilization_max,gpu_memory_utilization_min,gpu_memory_utilization_avg,gpu_memory_utilization_max,gpu_power_watts_min,gpu_power_watts_avg,gpu_power_watts_max,gpu_power_percent_min,gpu_power_percent_avg,gpu_power_percent_max,cpu_compute_utilization,cpu_memory_utilization,train_runtime,train_samples_per_second,train_steps_per_second,dataset_tokens_per_second,dataset_tokens_per_second_per_gpu,is_valid\n0,model_name.smollm2-135m-number_gpus.0-model_max_length.512-batch_size.1,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30-flash_attn.0,smollm2-135m,0,512,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,79.55,49.11366699999999,30.4385,134.566,33.642,2624.0452059069926,2624.0452059069926,1\n1,model_name.smollm2-135m-number_gpus.0-model_max_length.512-batch_size.2,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30-flash_attn.0,smollm2-135m,0,512,2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,76.3,49.163925750000004,30.095,136.103,17.013,4355.274962618375,4355.274962618375,1\n2,model_name.smollm2-135m-number_gpus.0-model_max_length.1024-batch_size.1,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30-flash_attn.0,smollm2-135m,0,1024,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,68.775,49.0008355,30.3635,134.899,33.725,3912.0654733479346,3912.0654733479346,1\n3,model_name.smollm2-135m-number_gpus.0-model_max_length.1024-batch_size.2,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30-flash_attn.0,smollm2-135m,0,1024,2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,46.85,49.00481125,30.1101,136.034,17.004,4353.090823344991,4353.090823344991,1\n</code></pre> <p>In the above CSV file you will find 1 column per:</p> <ul> <li>entity space property (input to the experiment) such as <code>batch_size</code> and   <code>model_max_length</code></li> <li>measured property (output to the experiment) such as   <code>dataset_tokens_per_second_per_gpu</code> and <code>gpu_memory_utilization_peak</code></li> </ul> <p>For a complete list of the entity space properties check out the documentation for the finetune_full_benchmark-v1.0.0 experiment in the SFTTrainer docs. The complete list of measured properties is available there too.</p>"},{"location":"examples/finetune-locally/#next-steps","title":"Next steps","text":"<ul> <li> <p>\ud83d\udd2c\ufe0f Find out more about the SFTTrainer actuator</p> <p>The actuator supports several experiments, each with a set of configurable parameters.</p> <p>Reference docs for the SFTTrainer actuator</p> </li> <li> <p>\u2699\ufe0f Configure your RayCluster for SFTTrainer measurements</p> <p>Learn how to configure your RayCluster for SFTTrainer measurements.</p> <p>Configure the RayCluster for SFTTrainer</p> </li> <li> <p> Scale it up!</p> <p>Take it to the next level by running an experiment on your remote RayCluster.</p> <p>run Operations remotely</p> </li> </ul>"},{"location":"examples/finetune-remotely/","title":"Measure throughput of finetuning on a Remote RayCluster","text":"<p>Note</p> <p>This example illustrates:</p> <ol> <li> <p>Set a remote RayCluster environment for running finetuning performance    benchmarks with SFTTrainer</p> </li> <li> <p>Benchmarking a set of finetuning configurations using GPUs on a remote    RayCluster</p> </li> </ol>"},{"location":"examples/finetune-remotely/#the-scenario","title":"The scenario","text":"<p>When you run a finetuning workload, you can choose values for parameters like the model name, batch size, and number of GPUs. To understand how these choices affect performance, a common strategy is to measure changes in system behavior by exploring the workload parameter space.</p> <p>This approach applies to many machine learning workloads where performance depends on configuration.</p> <p>In this example, <code>ado</code> is used to explore LLM fine-tuning throughput across a fine-tuning workload parameter space on a remote RayCluster.</p> <p>To explore this space, you will:</p> <ul> <li>define the parameters to test - such as the batch size and the model max   length</li> <li>define what to test them with - In this case we will use SFTTrainer's   <code>finetune_full_benchmark-v1.0.0</code> experiment</li> <li>explore the parameter space - the sampling method</li> </ul> <p>Info</p> <p>This example assumes you have already followed the Measure throughput of finetuning locally example.</p>"},{"location":"examples/finetune-remotely/#pre-requisites","title":"Pre-requisites","text":"<ol> <li> <p>A remote shared context is available (see    shared contexts for more information). Here we    call it <code>finetuning</code> but it can have any name.</p> </li> <li> <p>A remote RayCluster    with a GPU worker that has at least one <code>NVIDIA-A100-SXM4-80GB</code> GPU. The    RayCluster should also contain the NVIDIA development and runtime packages.    We recommend deploying the RayCluster following our    docs.</p> </li> <li> <p>If you host your RayCluster on Kubernetes or OpenShift, make sure you're    logged in to the Kubernetes or Openshift cluster.</p> </li> <li> <p>Activate the <code>finetuning</code> shared context for the example.</p> </li> </ol> <pre><code>ado context finetuning\n</code></pre>"},{"location":"examples/finetune-remotely/#install-and-configure-the-sfttrainer-actuator","title":"Install and Configure the SFTTrainer actuator","text":""},{"location":"examples/finetune-remotely/#install-the-sfttrainer-actuator","title":"Install the SFTTrainer actuator","text":"Install the SFTTrainer Actuator plugin from PyPiInstall SFTTrainer from the <code>ado</code> sources <p>Run <code>pip install ado-sfttrainer</code> to install the SFTTrainer actuator plugin using the wheel that we push to PyPi.</p> <p>Info</p> <p>This step assumes you are in the root directory of the ado source repository.</p> <p>If you haven't already installed the <code>SFTTrainer</code> actuator, run (assumes you are in the root directory of ado):</p> <pre><code>```commandline\npip install plugins/actuators/sfttrainer\n```\n\n then executing\n</code></pre> <pre><code> ```commandline\n ado get actuators\n ```\n\n should show an entry for `SFTTrainer` like below\n\n ```\n            ACTUATOR ID\n 0   custom_experiments\n 1                 mock\n 2               replay\n 3           SFTTrainer\n ```\n</code></pre>"},{"location":"examples/finetune-remotely/#configure-the-sfttrainer-actuator","title":"Configure the SFTTrainer Actuator","text":"<p>SFTTrainer includes parameters that control its behavior. For example, it pushes any training metrics it collects, like system profiling metadata, to an AIM server by default. It also features parameters that define important paths, such as the location of the Hugging Face cache and the directory where the actuator expects to find files like the test Dataset.</p> <p>In this section you will configure the actuator for experiments on your remote RayCluster.</p> If you do not have an AIM serverIf you have an AIM server <p>Create the file <code>actuator_configuration.yaml</code> with the following contents:</p> <pre><code>actuatorIdentifier: SFTTrainer\nparameters:\n  hf_home: /hf-models-pvc/huggingface_home\n  data_directory: /data/fms-hf-tuning/artificial-dataset/\n</code></pre> <p>Create the file <code>actuator_configuration.yaml</code> with the following contents:</p> <pre><code>actuatorIdentifier: SFTTrainer\nparameters:\n  aim_db: aim://$the-aim-server-domain-or-ip:port\n  aim_dashboard_url: https://$the-aim-dashboard-domain-or-ip:port\n  hf_home: /hf-models-pvc/huggingface_home\n  data_directory: /data/fms-hf-tuning/artificial-dataset/\n</code></pre> <p>Info</p> <p>If you have deployed a custom RayCluster then make sure that the <code>hf_home</code> and <code>data_directory</code> parameters point to paths that can be created by your remote RayCluster workers. We recommend deploying a remote RayCluster following our instructions.</p> <p>Next, create the <code>actuatorconfiguration</code> resource like so:</p> <pre><code>ado create actuatorconfiguration -f actuator_configuration.yaml\n</code></pre> <p>The command will print the ID of the resource. Make a note of it, you will need it in a later step.</p> <p>See the full list of the actuator parameters you can set in the SFTTrainer reference docs.</p>"},{"location":"examples/finetune-remotely/#prepare-the-remote-raycluster","title":"Prepare the remote RayCluster","text":"<p>Info</p> <p>This section assumes you have configured your RayCluster for use with SFTTrainer and that you have configured your SFTTrainer actuator with the values we provided above for the <code>hf_home</code> and <code>data_directory</code> parameters.</p>"},{"location":"examples/finetune-remotely/#for-rayclusters-on-kubernetesopenshift-create-a-port-forward","title":"For RayClusters on Kubernetes/OpenShift - create a port-forward","text":"<p>Info</p> <p>If your remote RayCluster is not hosted on Kubernetes or OpenShift then you can skip this step.</p> <p>In a terminal, start a <code>kubectl port-forward</code> process to the service that connects to the head of your RayCluster. Keep this process running until your experiments finish.</p> <p>For example, if the name of your RayCluster is <code>ray-disorch</code>, run:</p> <pre><code>kubectl port-forward svc/ray-disorch-head-svc 8265\n</code></pre> <p>Verify that the port forward is active by visiting http://localhost:8265 you should see the landing page of the Ray web dashboard.</p>"},{"location":"examples/finetune-remotely/#prepare-files-for-the-ray-jobs-you-will-run-later","title":"Prepare files for the Ray jobs you will run later","text":"<p>Create a directory called <code>my-remote-measurements</code> and <code>cd</code> into it. You will keep all the files for this example in there.</p> <p>Similar to how you installed <code>ado</code> and <code>SFTTrainer</code> on your laptop, it's important to ensure these Python packages are also available on your remote RayCluster.</p> <p>You have two options for installing the required packages:</p> <ol> <li>Pre-install the packages in the virtual environment of your RayCluster    before deployment.</li> <li>Use a    Ray Runtime environment YAML,    which instructs Ray to dynamically install Python packages during runtime.</li> </ol> <p>In this section, we\u2019ll focus on the second approach.</p> Use the SFTTrainer plugin wheel from PyPiBuild the python wheel yourself <p>Create the <code>ray_runtime_env.yaml</code> file under the directory <code>my-remote-measurements</code> with the following contents:</p> <p> <pre><code>pip:\n   - sfttrainer\nenv_vars:\n  env_vars:\n    AIM_UI_TELEMETRY_ENABLED: \"0\"\n    # We set HOME to /tmp because \"import aim.utils.tracking\" tries to write under $HOME/.aim_profile.\n    # However, the process lacks permissions to do so and that leads to an ImportError exception.\n    HOME: \"/tmp/\"\n    OMP_NUM_THREADS: \"1\"\n    OPENBLAS_NUM_THREADS: \"1\"\n    RAY_AIR_NEW_PERSISTENCE_MODE: \"0\"\n    PYTHONUNBUFFERED: \"x\"\n</code></pre> </p> <p>If your RayCluster doesn't already have <code>ado</code> installed in its virtual environment then include the <code>ado-core</code> package too.</p> <p>Build the python wheel for the Actuator plugin <code>SFTTrainer</code>.</p> <p>Briefly, if you are in the top level of the <code>ado</code> repository execute:</p> <pre><code>python -m build -w plugins/actuators/sfttrainer\nmv plugins/actuators/sfttrainer/dist/*.whl ${path to my-remote-measurements}\n</code></pre> <p>Then create a <code>ray_runtime_env.yaml</code> file under <code>my-remote-measurements</code> with the following contents:</p> <p> <pre><code>pip:\n   - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/sfttrainer-1.1.0.dev152+g23c7ba34e-py3-none-any.whl\nenv_vars:\n   env_vars:\n   AIM_UI_TELEMETRY_ENABLED: \"0\"\n   # We set HOME to /tmp because \"import aim.utils.tracking\" tries to write under $HOME/.aim_profile.\n   # However, the process lacks permissions to do so and that leads to an ImportError exception.\n   HOME: \"/tmp/\"\n   OMP_NUM_THREADS: \"1\"\n   OPENBLAS_NUM_THREADS: \"1\"\n   RAY_AIR_NEW_PERSISTENCE_MODE: \"0\"\n   PYTHONUNBUFFERED: \"x\"\n</code></pre> </p> <p>If your RayCluster doesn't already have <code>ado</code> installed in its virtual environment then build the wheel for <code>ado-core</code> by repeating the above in the root directory of <code>ado</code>. Then add an entry under <code>pip</code> pointing to the the resulting <code>ado</code> wheel file.</p> <p>Info</p> <p>Your wheel will filenames may vary.</p> <p>For convenience, you can run the script below from inside the <code>my-remote-measurements</code> directory. It will build the wheels of both <code>ado</code> and <code>sfttrainer</code> and then automatically generate the <code>ray_runtime_env.yaml</code> file under your working directory. The script builds the wheel for <code>ado</code> too.</p> <pre><code>$path_to_ado_root/plugins/actuators/sfttrainer/examples/build_wheels.sh\n</code></pre> <p>Reference docs on using ado with remote RayClusters.</p> <p>You will use the files you created during this step in later steps when launching jobs on your remote RayCluster.</p>"},{"location":"examples/finetune-remotely/#create-the-test-dataset-on-the-remote-raycluster","title":"Create the test Dataset on the remote RayCluster","text":"<p>Use the <code>.whl</code> and <code>ray_runtime_env.yaml</code> files with <code>ray job submit</code> to launch a job on your remote RayCluster. This job will create the synthetic dataset and place it in the correct location under the directory specified by the <code>data_directory</code> parameter of the SFTTrainer actuator.</p> <p>Info</p> <p>You can find instructions for generating the <code>.whl</code> and <code>ray_runtime_env.yaml</code> files in the Prepare files for the Ray jobs you will run later section.</p> <p>To submit the job to your remote RayCluster run the command:</p> <pre><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml \\\n--working-dir $PWD -v -- sfttrainer_generate_dataset_text \\\n-o /data/fms-hf-tuning/artificial-dataset/news-tokens-16384plus-entries-4096.jsonl\n</code></pre> <p>Reference docs on creating the datasets</p>"},{"location":"examples/finetune-remotely/#download-model-weights-on-the-remote-raycluster","title":"Download model weights on the remote RayCluster","text":"<p>Next, submit a ray job that downloads the model weights for <code>granite-3.1-2b</code> in the appropriate path under the directory specified by the <code>hf_home</code> parameter of the SFTTrainer actuator.</p> <p>First, save the following YAML to a file <code>models.yaml</code> inside your working directory (<code>my-remote-measurements</code>):</p> <pre><code>granite-3.1-2b:\n  Vanilla: ibm-granite/granite-3.1-2b-base\n</code></pre> <p>To start the ray job run:</p> <pre><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml \\\n--working-dir $PWD -v -- \\\nsfttrainer_download_hf_weights -i models.yaml -o /hf-models-pvc/huggingface_home\n</code></pre> <p>Reference docs on pre-fetching weights</p>"},{"location":"examples/finetune-remotely/#run-the-example","title":"Run the example","text":""},{"location":"examples/finetune-remotely/#define-the-finetuning-workload-configurations-to-test-and-how-to-test-them","title":"Define the finetuning workload configurations to test and how to test them","text":"<p>In this example, we create a <code>discoveryspace</code> that runs the finetune_full_benchmark-v1.0.0 experiment to finetune the <code>granite-3.1-2b</code> using 1 GPU.</p> <p>The <code>entitySpace</code> defined below includes four dimensions:</p> <ul> <li><code>model_name</code> and <code>number_gpus</code> each contain a single value.</li> <li><code>model_max_length</code> and <code>batch_size</code> each contain two values.</li> </ul> <p>The total number of entities in the <code>entitySpace</code> is the number of unique combinations of values across all dimensions. In this case, the configuration contains 4 entities.</p> <p>You can find the complete list of the entity space properties in the documentation of the finetune_full_benchmark-v1.0.0 experiment.</p> <ol> <li>Create the file <code>space.yaml</code> with the contents:</li> </ol> <pre><code>experiments:\n  - experimentIdentifier: finetune_full_benchmark-v1.0.0\n    actuatorIdentifier: SFTTrainer\n    parameterization:\n      - property:\n          identifier: fms_hf_tuning_version\n        value: \"2.8.2\"\n      - property:\n          identifier: stop_after_seconds\n        # Set training duration to at least 30 seconds.\n        # For meaningful system metrics, we recommend a minimum of 300 seconds.\n        value: 30\n\nentitySpace:\n  - identifier: \"model_name\"\n    propertyDomain:\n      values: [\"granite-3.1-2b\"]\n  - identifier: \"number_gpus\"\n    propertyDomain:\n      values: [1]\n  - identifier: \"gpu_model\"\n    propertyDomain:\n      values: [\"NVIDIA-A100-SXM4-80GB\"]\n  - identifier: \"model_max_length\"\n    propertyDomain:\n      values: [512, 1024]\n  - identifier: \"batch_size\"\n    propertyDomain:\n      values: [1, 2]\n</code></pre> <ol> <li>Create the space:</li> </ol> <ul> <li> <p>If you have an <code>samplestore</code> ID, run:</p> <p> <pre><code>ado create space -f space.yaml --set \"sampleStoreIdentifier=$SAMPLE_STORE_IDENTIFIER\"\n</code></pre></p> </li> <li> <p>If you do not have a <code>samplestore</code> then run</p> <p> <pre><code>ado create space -f space.yaml --new-sample-store\n</code></pre></p> </li> </ul> <p>This will print a <code>discoveryspace</code> ID (e.g., <code>space-ea937f-831dba</code>). Make a    note of this ID, you'll need it in the next step.</p>"},{"location":"examples/finetune-remotely/#create-a-random-walk-operation-to-explore-the-space","title":"Create a random walk <code>operation</code> to explore the space","text":"<ol> <li> <p>Create the file <code>operation.yaml</code> with the following contents:</p> <p> <pre><code>spaces:\n  - The identifier of the DiscoverySpace resource\nactuatorConfigurationIdentifiers:\n  - The identifier of the Actuator Configuration resource\n\noperation:\n  module:\n    operatorName: \"random_walk\"\n    operationType: \"search\"\n  parameters:\n    numberEntities: all\n    singleMeasurement: True\n    batchSize: 1 # you may increase this number if you have more than 1 GPU\n    samplerConfig:\n      mode: sequential\n      samplerType: generator\n</code></pre></p> </li> <li> <p>Replace the placeholders with your <code>discoveryspace</code> ID and     <code>actuatorconfiguration</code> ID and save it in a file with the name     <code>operation.yaml</code>.</p> </li> <li> <p>Export the <code>finetuning</code> context so you can supply it to the remote     operation.</p> <p> <pre><code>ado get context --output yaml finetuning &gt;context.yaml\n</code></pre></p> </li> <li> <p>For the next step your <code>my-remote-measurements</code> directory needs the     following files, although the wheels may have different ids.</p> <p> <pre><code>my-remote-measurements\n\u251c\u2500\u2500 ado_core-1.1.0.dev133+f4b639c1.dirty-py3-none-any.whl\n\u251c\u2500\u2500 context.yaml\n\u251c\u2500\u2500 operation.yaml\n\u251c\u2500\u2500 ray_runtime_env.yaml\n\u2514\u2500\u2500 ado_sfttrainer-1.1.0.dev133+gf4b639c10.d20250812-py3-none-any.whl\n</code></pre></p> <p>Info</p> <p>You can find instructions for generating the <code>.whl</code> and <code>ray_runtime_env.yaml</code> files in the Prepare files for the Ray jobs you will run later section.</p> </li> <li> <p>Create the operation on the remote RayCluster</p> <p>Use the <code>.whl</code> and <code>ray_runtime_env.yaml</code> files to submit a job to your remote RayCluster which creates the <code>operation</code> that runs your finetuning measurements.</p> <p>Run the command:</p> <p> <pre><code>ray job submit --no-wait --address http://localhost:8265  --working-dir . \\\n--runtime-env ray_runtime_env.yaml -v -- \\\nado -c context.yaml create operation -f operation.yaml\n</code></pre> </p> <p>The operation will execute the measurements (i.e. apply the experiment finetune_full_benchmark-v1.0.0 on the 4 entities) as defined in your <code>discoveryspace</code>.</p> <p>Info</p> <p>Each measurement finetunes the   <code>granite-3.1-2b</code>   model and takes about two minutes to complete. There is a total of four measurements.   It will also take a couple of minutes for Ray to create the ray environment   on participating GPU worker nodes so expect the <code>operation</code> to take O(10) minutes   to complete.</p> <p>Reference docs for submitting ado operations to remote RayClusters.</p> </li> </ol>"},{"location":"examples/finetune-remotely/#examine-the-results-of-the-exploration","title":"Examine the results of the exploration","text":"<p>After the operation completes, you can download the results of your measurements:</p> <pre><code>ado show entities --output-format csv --property-format=target space $yourDiscoverySpaceID\n</code></pre> <p>Info</p> <p>Notice that because the context we are using refers to a remote project we can access the data created by the operation on the remote ray cluster. Anyone that has access to the <code>finetuning</code> context can also download the results of your measurements!</p> <p>The command will generate a CSV file. Open it to explore the data that your operation has collected!</p> <p>It should look similar to this:</p> <pre><code>,identifier,generatorid,experiment_id,model_name,number_gpus,gpu_model,model_max_length,batch_size,gpu_compute_utilization_min,gpu_compute_utilization_avg,gpu_compute_utilization_max,gpu_memory_utilization_min,gpu_memory_utilization_avg,gpu_memory_utilization_max,gpu_memory_utilization_peak,gpu_power_watts_min,gpu_power_watts_avg,gpu_power_watts_max,gpu_power_percent_min,gpu_power_percent_avg,gpu_power_percent_max,cpu_compute_utilization,cpu_memory_utilization,train_runtime,train_samples_per_second,train_steps_per_second,dataset_tokens_per_second,dataset_tokens_per_second_per_gpu,is_valid\n0,model_name.granite-3.1-2b-number_gpus.1-gpu_model.NVIDIA-A100-SXM4-80GB-model_max_length.512-batch_size.2,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30,granite-3.1-2b,1,NVIDIA-A100-SXM4-80GB,512,2,40.0,40.0,40.0,25.49774175,25.49774175,25.49774175,31.498108,169.3855,169.3855,169.3855,42.346375,42.346375,42.346375,74.075,2.6414139999999997,31.3457,130.672,16.334,2744.108442306281,2744.108442306281,1\n1,model_name.granite-3.1-2b-number_gpus.1-gpu_model.NVIDIA-A100-SXM4-80GB-model_max_length.512-batch_size.1,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30,granite-3.1-2b,1,NVIDIA-A100-SXM4-80GB,512,1,27.25,27.25,27.25,25.71380625,25.71380625,25.71380625,31.786194,141.78924999999998,141.78924999999998,141.78924999999998,35.447312499999995,35.447312499999995,35.447312499999995,74.325,2.6420105,30.5903,133.899,33.475,1405.9358685596414,1405.9358685596414,1\n2,model_name.granite-3.1-2b-number_gpus.1-gpu_model.NVIDIA-A100-SXM4-80GB-model_max_length.1024-batch_size.1,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30,granite-3.1-2b,1,NVIDIA-A100-SXM4-80GB,1024,1,43.25,43.25,43.25,25.43853775,25.43853775,25.43853775,31.498108,181.79625,181.79625,181.79625,45.4490625,45.4490625,45.4490625,74.32499999999999,2.64201475,30.6802,133.506,33.377,2670.126009608803,2670.126009608803,1\n3,model_name.granite-3.1-2b-number_gpus.1-gpu_model.NVIDIA-A100-SXM4-80GB-model_max_length.1024-batch_size.2,explicit_grid_sample_generator,SFTTrainer.finetune_full_benchmark-v1.0.0-fms_hf_tuning_version.2.8.2-stop_after_seconds.30,granite-3.1-2b,1,NVIDIA-A100-SXM4-80GB,1024,2,63.75,63.75,63.75,25.67718525,25.67718525,25.67718525,31.737366,238.53825,238.53825,238.53825,59.6345625,59.6345625,59.6345625,74.1,2.6399939999999997,30.1566,135.824,16.978,5161.324552502603,5161.324552502603,1\n</code></pre> <p>In the above CSV file you will find 1 column per:</p> <ul> <li>entity space property (input to the experiment) such as <code>batch_size</code> and   <code>model_max_length</code></li> <li>measured property (output to the experiment) such as   <code>dataset_tokens_per_second_per_gpu</code> and <code>gpu_memory_utilization_peak</code></li> </ul> <p>For a complete list of the entity space properties check out the documentation for the finetune_full_benchmark-v1.0.0 experiment in the SFTTrainer docs. The complete list of measured properties is available there too.</p>"},{"location":"examples/finetune-remotely/#next-steps","title":"Next steps","text":"<ul> <li> <p>\ud83d\udd2c\ufe0f Find out more about the SFTTrainer actuator</p> <p>The actuator supports several experiments, each with a set of configurable parameters.</p> <p>Reference docs for the SFTTrainer actuator</p> </li> </ul>"},{"location":"examples/lhu/","title":"Identify the important dimensions of a space","text":"<p>Note</p> <p>This example shows</p> <ol> <li> <p>Using the Latin Hyper-Cube Sampler from the <code>ray_tune</code> operator to explore    the space</p> </li> <li> <p>Using a stopper to halt a <code>ray_tune</code> exploration when certain conditions    are met</p> </li> <li> <p>Specifically, stopping the exploration when the dimensions/properties that    have the greatest influence on the target metric are known</p> </li> </ol>"},{"location":"examples/lhu/#the-scenario","title":"The scenario","text":"<p>When working with a high-dimensional configuration space, it's natural to ask which dimensions have the greatest influence on a specific experimental outcome. For instance, if a workload has 20 tunable parameters, you might want to identify which ones most significantly affect a particular throughput metric. Understanding this can help narrow future explorations to only the most impactful parameters, potentially reducing the time and resources spent by ignoring those that are less relevant.</p> <p>The workloads in the <code>ml_multi_cloud</code> data set are defined by four parameters, <code>provider</code>, <code>cpu_family</code>, <code>vcpu_size</code> and <code>nodes</code>, and hence the <code>entityspace</code> in the related examples have 4 dimensions. Here we will try to find which dimensions have most influence over the <code>wallClockRuntime</code> property.</p> <p>Caution</p> <p>The commands below assume you are in the directory <code>examples/ml-multi-cloud</code> in the ado source repository. See the instructions for cloning the repository.</p>"},{"location":"examples/lhu/#pre-requisites","title":"Pre-requisites","text":""},{"location":"examples/lhu/#install-the-ray_tune-ado-operator","title":"Install the ray_tune ado operator","text":"<p>If you haven't already installed the ray_tune operator, run (assumes you are in <code>examples/ml-multi-cloud/</code> ):</p> <pre><code>pip install ../../plugins/operators/ray_tune\n</code></pre> <p>then executing</p> <pre><code>ado get operators\n</code></pre> <p>should show an entry for <code>ray_tune</code> like below</p> <pre><code>Available operators by type:\n      OPERATOR     TYPE\n0  random_walk  explore\n1     ray_tune  explore\n</code></pre>"},{"location":"examples/lhu/#creating-the-discoveryspace","title":"Creating the <code>discoveryspace</code>","text":"<p>This example uses the same <code>discoveryspace</code> created in the talking a random walk example. You can use <code>ado get spaces</code> to find the identifier.</p>"},{"location":"examples/lhu/#discovering-what-workload-parameters-most-impact-cost","title":"Discovering what workload parameters most impact cost","text":"<p>We will use the latin-hyper-cube sampler to sample points. This is a sampling method which tries maintaining properties similar to true random sampling, while ensuring the samples are more evenly spread across the space. An example operation file is given in <code>lhc-sampler.yaml</code>.</p> <p>The operation also configures the exploration to monitor the relationship between the four parameters and the cost metric and stop when it has determined which are most important using the InformationGain stopper.</p> <p>To execute this operation run (replacing <code>$DISCOVERY_SPACE_IDENTIFIER</code> with the identifier of the space created in the talking a random walk example):</p> <pre><code>ado create operation -f lhc_sampler.yaml --set \"spaces[0]=$DISCOVERY_SPACE_IDENTIFIER\"\n</code></pre> <p>You will see a lot of RayTune-related output as it samples different entities using the latin-hyper-cube sampler. The number of samples to obtain is set to 32 in <code>lhc_sampler.yaml</code>, however the operation will stop before reaching that due to the InformationGain stopper. If you look back through the log of the operation, within the logs for the last sample you will lines like:</p> <pre><code>(tune pid=7959) Stopping criteria reached after 13 samples.\n(tune pid=7959) Total search space size is 48, search coverage is 0.2708333333333333.\n(tune pid=7959) Entropy of target variable clusters: 1.2711814802605799 nats.\n(tune pid=7959) Result:\n(tune pid=7959)     dimension  rank        mi  uncertainty%\n(tune pid=7959) 3       nodes     1  0.524715      0.412778\n(tune pid=7959) 2   vcpu_size     2  0.396410      0.311844\n(tune pid=7959) 0    provider     3  0.246428      0.193858\n(tune pid=7959) 1  cpu_family     4  0.142884      0.112403\n(tune pid=7959)\n(tune pid=7959) Pareto selection:['provider', 'vcpu_size', 'nodes']\n</code></pre> <p>In this table the dimensions are ranked in order of importance, as determined by their mutual information with the target variable, wallClockRuntime. The <code>uncertainty%</code> is the ratio of the dimensions mutual information with the entropy of the target variable (or clusters of the target variable to be more exact) i.e. how much of the entropy or variance of the target variable is explained by the dimension.</p> <p>At the end of the output we can see the stopper has identified a \"pareto selection\" of three dimensions: ['provider', 'vcpu_size', 'nodes']. This is the smallest number of dimensions, whose total mutual information exceeds a threshold, which is <code>0.8</code> by default.</p> <p>This is chosen as follows:</p> <ul> <li>For each possible dimension set size the stopper determines which set explains   the most mutual information.</li> <li>For example, for a set of size 2 dimensions, it evaluates the 6 possible     pairs: [nodes,vcpu_size], [nodes, provider], [nodes, cpu_family] ,     [vcpu_size, provider], [vcpu_size, nodes], [provider, nodes].</li> <li>This gives one set for each possible dimensions set size: 1,2,3 and 4 in this   case - the pareto optimal sets</li> <li>Then the smallest of these sets which exceeds the threshold value is selected.</li> </ul> <p>Note</p> <p>Since latin hypercube sampling is random the pareto set can change slightly from run to run as different entities are used. In this example over multiple runs you should see the pareto set being 2 or 3 and always including <code>nodes</code>.</p>"},{"location":"examples/lhu/#whats-next","title":"What's next","text":"<ul> <li> <p> Search using an optimizer</p> <p>Try the Search a space with an optimizer example to see how you can use RayTune, and define custom experiments, via <code>ado</code>.</p> <p>Search a space with an optimizer </p> </li> <li> <p> Creating custom objective functions</p> <p>Try the Search a space based on a custom objective function example to see how you can define a custom experiment which uses inputs from another experiment.</p> <p>Search a space based on a custom objective function </p> </li> </ul>"},{"location":"examples/random-walk/","title":"Taking a random walk","text":"<p>Note</p> <p>This example illustrates:</p> <ol> <li> <p>Describing a set of points and how to measure them using a <code>discoveryspace</code></p> </li> <li> <p>Exploring the <code>discoveryspace</code> by creating an operation that samples and    measures the points</p> </li> <li> <p>Getting the results of an operation</p> </li> </ol>"},{"location":"examples/random-walk/#the-scenario","title":"The scenario","text":"<p>When you deploy a workload you have to choose values for workload parameters like the number of CPUs or the node type. To choose a combination of parameters that, for example, maximizes performance, a common strategy is to measure changes in performance by exploring the workload parameter space. This pattern applies to many domains where there is a parameter space to explore.</p> <p>In this example <code>ado</code> is used to explore the workload parameter space for a cloud application. To explore a workload parameter space you have to:</p> <ul> <li>define the values of the parameters to test - the parameter space</li> <li>define what to test them with - the experiment</li> <li>select points from the parameter space and perform the test - the sampling   method</li> </ul> <p>Here, we will use the simplest sampling method, random walk, where some number of points are randomly selected without replacement.</p> <p>Caution</p> <p>The commands below assume you are in the directory <code>examples/ml-multi-cloud</code> in the ado source repository. See the instructions for cloning the repository.</p>"},{"location":"examples/random-walk/#using-pre-existing-data-with-ado","title":"Using pre-existing data with <code>ado</code>","text":"<p>For this example we will use some pre-existing data. This makes the example simpler and quicker to execute but can also be useful in other situations. The data is in the file <code>ml_export.csv</code> and is consists of results of running a benchmark on different cloud hardware configurations from different providers.</p> <p>In <code>ado</code> such configurations are called <code>entities</code>, and are stored, along with the results of measurements executed on them, in a <code>samplestore</code>. Let's start by copying the data in <code>ml_export.csv</code> into a new <code>samplestore</code>.</p> <p>To do this execute,</p> <pre><code>ado create samplestore -f ml_multicloud_sample_store.yaml --set \"copyFrom[0].storageLocation.path\"=ml_export.csv\n</code></pre> <p>and it will report that a <code>samplestore</code> has been created:</p> <pre><code>Success! Created sample store with identifier $SAMPLE_STORE_IDENTIFIER\n</code></pre> <p>Note the <code>samplestore</code> resource identifier printed by this command for the next section.</p> <p>Also try <code>ado get samplestores</code> and you will see an entry for the one you just created</p> <p>Info</p> <p> You only need to create this <code>samplestore</code> once. It can be reused in multiple <code>discoveryspaces</code> or examples that require the <code>ml_export.csv</code> data.</p>"},{"location":"examples/random-walk/#creating-a-discoveryspace-for-the-ml-multi-cloud-data","title":"Creating a <code>discoveryspace</code> for the <code>ml-multi-cloud</code> data","text":"<p>A <code>discoveryspace</code> describes a set of points and how to measure them. Here we will create a <code>discoveryspace</code> to describe the space explored in <code>ml_export.csv</code>.</p> <p>Execute:</p> <pre><code>ado create space -f ml_multicloud_space.yaml --set \"sampleStoreIdentifier=$SAMPLE_STORE_IDENTIFIER\"\n</code></pre> <p>where <code>$SAMPLE_STORE_IDENTIFIER</code> is the identifier you copied in last step.</p> <p>This will confirm the creation of the <code>discoveryspace</code> with:</p> <pre><code>Success! Created space with identifier: $DISCOVERY_SPACE_IDENTIFIER\n</code></pre> <p>You can now describe the <code>discoveryspace</code> with:</p> <pre><code>ado describe space $DISCOVERY_SPACE_IDENTIFIER\n</code></pre> <p>where $DISCOVERY_SPACE_IDENTIFIER is the identifier of the <code>discoveryspace</code> resource that was just created. This will output:</p> <pre><code>Identifier: space-65cf33-a8df39\n\nEntity Space:\n\n  Number entities: 48\n\n  Categorical properties:\n           name     values\n    0  provider  [A, B, C]\n\n  Discrete properties:\n             name   range interval        values\n    0  cpu_family  [0, 2]     None        [0, 1]\n    1   vcpu_size  [0, 2]     None        [0, 1]\n    2       nodes  [2, 6]     None  [2, 3, 4, 5]\n\n\nMeasurement Space:\n                        experiment  supported\n  0  replay.benchmark_performance       True\n\n  'replay.benchmark_performance'\n\n  Inputs:\n      parameter      type value parameterized\n  0  cpu_family  required  None            na\n  1   vcpu_size  required  None            na\n  2       nodes  required  None            na\n  3    provider  required  None            na\n\n  Outputs:\n       target property\n  0  wallClockRuntime\n  1            status\n\nSample Store identifier: 'a8df39'\n</code></pre> <p>Note</p> <p>The set of points is defined by the properties in the <code>Entity Space</code> - here 'cpu_family', 'provider', 'vcpu_size' and 'nodes' - and the values those properties can take.</p> <p>Tip</p> <p>Consider why the size of the entityspace is 48. Compare this to the number of rows in <code>ml_export.csv</code>.</p>"},{"location":"examples/random-walk/#exploring-the-discoveryspace","title":"Exploring the <code>discoveryspace</code>","text":"<p>Next we will run an operation that will \"explore\" the <code>discoveryspace</code> we just created. Since we already have the data, <code>ado</code> will transparently identify and reuse it. An example operation file is given in <code>randomwalk_ml_multicloud_operation.yaml</code>. The contents are:</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\n\nmetadata:\n  name: 'randomwalk-all'\n  description: 'Perform a random walk on all points in a space'\nspaces:\n- 'space-630588-bfebfe'\noperation:\n  module:\n    operatorName: \"random_walk\"\n    operationType: \"search\"\n  parameters:\n    numberEntities: 48\n    batchSize: 1\n    singleMeasurement: True\n    samplerConfig:\n      samplerType: generator\n      mode: random\n</code></pre> <p>To run the operation execute (replacing <code>$DISCOVERY_SPACE_IDENTIFIER</code> with the identifier of the space you created):</p> <pre><code>ado create operation -f randomwalk_ml_multicloud_operation.yaml --set \"spaces[0]=$DISCOVERY_SPACE_IDENTIFIER\"\n</code></pre> <p>This will output a lot of information as it samples all the entities. Typically, you will see the following lines for each entity (point in the entity space) sampled and measured:</p> <pre><code>(RandomWalk pid=14797) Continuous batching: SUBMIT EXPERIMENT. Submitting experiment replay.benchmark_performance for provider.B-cpu_family.1-vcpu_size.1-nodes.4\n(RandomWalk pid=14797)\n(RandomWalk pid=14797) Continuous batching: SUMMARY. Entities sampled and submitted: 2. Experiments completed: 1 Waiting on 1 active requests. There are 0 dependent experiments\n(RandomWalk pid=14797) Continuous Batching: EXPERIMENT COMPLETION. Received finished notification for experiment in measurement request in group 1: request-randomwalk-0.9.6.dev91+884f713b.dirty-c5ed4b-579021-experiment-benchmark_performance-entities-provider.B-cpu_family.1-vcpu_size.1-nodes.4 (explicit_grid_sample_generator)-requester-randomwalk-0.9.6.dev91+884f713b.dirty-c5ed4b-time-2025-07-29 20:03:00.976809+01:00\n</code></pre> <p>The first line, \"SUBMIT EXPERIMENT\", indicates the entity - <code>provider.B-cpu_family.1-vcpu_size.1-nodes.4</code> - and experiment - <code>replay.benchmark_performance</code> submitted. The next line gives a summary of what has happened so far: this is the second entity sampled and submitted; one experiment has completed; and the sampler is waiting on one active experiment before submitting a new one. Finally, the \"EXPERIMENT COMPLETION\" line indicates the experiment has finished.</p> <p>The operation will end with information like:</p> <pre><code>config:\n  operation:\n    module:\n      moduleClass: RandomWalk\n      moduleName: orchestrator.modules.operators.randomwalk\n      modulePath: .\n      moduleType: operation\n    parameters:\n      batchSize: 1\n      numberEntities: 48\n      samplerConfig:\n        mode: sequential\n        samplerType: generator\n  spaces:\n    - space-65cf33-a8df39\ncreated: \"2025-06-20T13:03:46.763154Z\"\nidentifier: randomwalk-0.9.4.dev30+564196d4.dirty-b8a233\nkind: operation\nmetadata:\n  entities_submitted: 48\n  experiments_requested: 74\noperationType: search\noperatorIdentifier: randomwalk-0.9.4.dev30+564196d4.dirty\nstatus:\n  - event: created\n    recorded_at: \"2025-06-20T13:03:40.267005Z\"\n  - event: added\n    recorded_at: \"2025-06-20T13:03:46.764750Z\"\n  - event: started\n    recorded_at: \"2025-06-20T13:03:46.769169Z\"\n  - event: finished\n    exit_state: success\n    recorded_at: \"2025-06-20T13:03:48.369516Z\"\n  - event: updated\n    recorded_at: \"2025-06-20T13:03:48.374765Z\"\nversion: v1\n</code></pre> <p>Note the value of the <code>identifier</code> field: in above it is <code>randomwalk-0.9.4.dev30+564196d4.dirty-b8a233</code></p> <p>Note</p> <p>The operation \"reuses\" existing measurements: this is an <code>ado</code> feature called memoization.</p> <p><code>ado</code> transparently executes experiments or memoizes data as appropriate - so the operator does not need to know if a measurement needs to be performed at the time it requests it, or if previous data can be reused.</p> <p>Tip</p> <p>Operations are domain agnostic. If you look in <code>randomwalk_ml_multicloud_operation.yaml</code> you will see there is no reference to characteristics of the discoveryspace we created. Indeed, this operation file could work on any discoveryspace.</p> <p>This shows that operators, like randomwalk, don't have to know domain specific details. All information about what to explore and how to measure is captured in the <code>discoveryspace</code>.</p>"},{"location":"examples/random-walk/#looking-at-the-operation-output","title":"Looking at the <code>operation</code> output","text":"<p>The command</p> <pre><code>ado show entities operation $OPERATION_IDENTIFIER\n</code></pre> <p>displays the results of the operation i.e. the entities sampled and the measurement results. You will see something like the following (the sampling is random so the order can be different):</p> <pre><code>               result_index                                   identifier                        benchmark_performance-wallClockRuntime benchmark_performance-status                                                                                        reason  valid\nrequest_index\n0                         0                               C_f1.0-c1.0-n4                                                    114.014369                           ok                                                                                                 True\n1                         0                               A_f0.0-c0.0-n2                                                    335.208518                           ok                                                                                                 True\n2                         0  provider.B-cpu_family.0-vcpu_size.1-nodes.5                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n3                         0                               C_f1.0-c0.0-n4                                                    177.723598                           ok                                                                                                 True\n4                         0                               B_f1.0-c0.0-n5                      [168.79178500175476, 141.99024295806885]                     [ok, ok]                                                                                                 True\n5                         0                               A_f1.0-c1.0-n4                                                    116.314171                           ok                                                                                                 True\n6                         0                               C_f1.0-c1.0-n2                                                    363.285671                           ok                                                                                                 True\n7                         0                               A_f0.0-c0.0-n5                       [106.0709307193756, 130.30512285232544]                     [ok, ok]                                                                                                 True\n8                         0                               C_f0.0-c0.0-n5                        [150.9471504688263, 138.0605161190033]                     [ok, ok]                                                                                                 True\n9                         0                               B_f1.0-c0.0-n4                      [202.48239731788635, 193.55997109413147]                     [ok, ok]                                                                                                 True\n10                        0                               C_f0.0-c0.0-n2                                                    415.829285                           ok                                                                                                 True\n11                        0                               B_f0.0-c0.0-n4                       [113.87676978111269, 132.5415120124817]                     [ok, ok]                                                                                                 True\n12                        0                               C_f1.0-c0.0-n2                                                    463.396539                           ok                                                                                                 True\n13                        0                               A_f1.0-c1.0-n5                        [96.8471610546112, 105.63729166984558]                     [ok, ok]                                                                                                 True\n14                        0                               A_f0.0-c0.0-n3                         [221.5101969242096, 216.394127368927]                     [ok, ok]                                                                                                 True\n15                        0                               B_f1.0-c1.0-n2                                                    298.819305                           ok                                                                                                 True\n16                        0                               C_f1.0-c1.0-n3                       [154.9813470840454, 168.34859228134155]                     [ok, ok]                                                                                                 True\n17                        0                               C_f0.0-c1.0-n2                                                    309.842324                           ok                                                                                                 True\n18                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.3                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n19                        0                               B_f0.0-c0.0-n5   [113.88505148887634, 103.90595746040344, 112.7056987285614]                 [ok, ok, ok]                                                                                                 True\n20                        0                               C_f0.0-c1.0-n3                        [168.9163637161255, 174.0335624217987]                     [ok, ok]                                                                                                 True\n21                        0                               B_f0.0-c0.0-n2                       [228.14362454414368, 225.1791422367096]                     [ok, ok]                                                                                                 True\n22                        0                               B_f0.0-c1.0-n2                        [166.74843192100525, 184.935049533844]                     [ok, ok]                                                                                                 True\n23                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.5                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n24                        0                               B_f1.0-c0.0-n2                                                    346.070996                           ok                                                                                                 True\n25                        0                               C_f0.0-c0.0-n4                                                    188.090878                           ok                                                                                                 True\n26                        0                               A_f1.0-c1.0-n2                                                    291.904456                           ok                                                                                                 True\n27                        0                               C_f1.0-c0.0-n3                       [244.33887457847595, 598.8834657669067]             [ok, Timed out.]                                                                                                 True\n28                        0                               A_f0.0-c1.0-n2                                                    272.997822                           ok                                                                                                 True\n29                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.4                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n30                        0                               A_f1.0-c1.0-n3                      [155.02856159210205, 151.58562421798706]                     [ok, ok]                                                                                                 True\n31                        0                               A_f0.0-c1.0-n4                                                    106.670121                           ok                                                                                                 True\n32                        0                               A_f1.0-c0.0-n3                       [206.74496150016785, 236.1715066432953]                     [ok, ok]                                                                                                 True\n33                        0                               C_f0.0-c0.0-n3                       [269.0906641483307, 240.07358503341675]                     [ok, ok]                                                                                                 True\n34                        0                               A_f1.0-c0.0-n2                                                     378.31657                           ok                                                                                                 True\n35                        0  provider.B-cpu_family.0-vcpu_size.1-nodes.3                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n36                        0                               B_f1.0-c0.0-n3                       [273.7120273113251, 220.19828414916992]                     [ok, ok]                                                                                                 True\n37                        0                               A_f1.0-c0.0-n4                                                    158.706395                           ok                                                                                                 True\n38                        0                               A_f0.0-c0.0-n4                                                    145.129484                           ok                                                                                                 True\n39                        0                               A_f0.0-c1.0-n3                      [170.15659737586975, 168.36590766906738]                     [ok, ok]                                                                                                 True\n40                        0                               A_f0.0-c1.0-n5                        [86.23016095161438, 84.45346999168396]                     [ok, ok]                                                                                                 True\n41                        0                               B_f0.0-c0.0-n3  [184.44801592826843, 153.51639366149902, 176.28814435005188]                 [ok, ok, ok]                                                                                                 True\n42                        0                               C_f1.0-c1.0-n5                       [100.97977471351624, 92.17141437530518]                     [ok, ok]                                                                                                 True\n43                        0  provider.B-cpu_family.0-vcpu_size.1-nodes.4                                                                                             Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n44                        0                               C_f1.0-c0.0-n5                       [136.3071050643921, 135.47050046920776]                     [ok, ok]                                                                                                 True\n45                        0                               C_f0.0-c1.0-n4                                                    121.424925                           ok                                                                                                 True\n46                        0                               A_f1.0-c0.0-n5                      [117.94136571884157, 135.91092538833618]                     [ok, ok]                                                                                                 True\n47                        0                               C_f0.0-c1.0-n5                        [95.86326050758362, 85.67946743965149]                     [ok, ok]                                                                                                 True\n</code></pre> <p>Some things to note and consider:</p> <ul> <li>The table is in the order the points were measured</li> <li>Some points have multiple measurements c.f. size of entityspace versus the   number of rows in <code>ml_export.csv</code>.</li> <li>Some points were not measured - these are points in the discoveryspace for   which no data was present to replay.</li> </ul>"},{"location":"examples/random-walk/#exploring-further","title":"Exploring Further","text":"<p>Here are a variety of commands you can try after executing the example above:</p>"},{"location":"examples/random-walk/#viewing-entities","title":"Viewing entities","text":"<p>There are multiple ways to few the entities related to a <code>discoveryspace</code>. Try:</p> <pre><code>ado show entities space $DISCOVERY_SPACE_IDENTIFIER\nado show entities space $DISCOVERY_SPACE_IDENTIFIER --aggregate mean\nado show entities space $DISCOVERY_SPACE_IDENTIFIER --include unmeasured\nado show entities space $DISCOVERY_SPACE_IDENTIFIER --property-format target\n</code></pre> <p>Also,</p> <pre><code>ado show details space $DISCOVERY_SPACE_IDENTIFIER\n</code></pre> <p>will give you a summary of what has been measured.</p>"},{"location":"examples/random-walk/#resource-provenance","title":"Resource provenance","text":"<p>The <code>related</code> sub-command shows resource provenance e.g.</p> <pre><code>ado show related operation $OPERATION_IDENTIFIER\n</code></pre>"},{"location":"examples/random-walk/#operation-timeseries","title":"Operation timeseries","text":"<p>The following commands give more details of the operation timeseries</p> <pre><code>ado show results operation $OPERATION_IDENTIFIER\nado show requests operation $OPERATION_IDENTIFIER\n</code></pre>"},{"location":"examples/random-walk/#resource-templates","title":"Resource templates","text":"<p>Another helpful command is <code>template</code> which will output a default example of a resource YAML along with an (optional) description of its fields. Try:</p> <pre><code>ado template operation --include-schema --operator-name random_walk\n</code></pre>"},{"location":"examples/random-walk/#rerun","title":"Rerun","text":"<p>An interesting thing to try is to run the operation again and compare the output of <code>show entities operation</code> for the two operations, and <code>show entities space</code>.</p>"},{"location":"examples/random-walk/#takeaways","title":"Takeaways","text":"<ul> <li>create-explore-view pattern: A common pattern in <code>ado</code> is to create a   <code>discoveryspace</code> to describe a set of points to measure, create <code>operations</code>   on it to explore or analyse it, and then view the results</li> <li>entity space and measurement space: A <code>discoveryspace</code> consists of an   <code>entityspace</code> - the set of points to measure - and a <code>measurementspace</code> - the   set of experiments to apply to them.</li> <li>operations are domain agnostic: <code>ado</code> enables operations to run on   multiple different domains without modification</li> <li>memoization: By default <code>ado</code> will identify if a measurement has already   been completed on an entity and reuse it</li> <li>provenance: <code>ado</code> stores the relationship between the resources it creates</li> <li>results viewing: <code>ado show entities</code> outputs the data in a   <code>discoveryspace</code> or measured in an <code>operation</code></li> <li>measurement timeseries: The sequence (timeseries) of measurements,   successful or not, of each <code>operation</code> is preserved</li> <li><code>discoveryspace</code> views: By default <code>ado show entities space</code> only shows   successfully measured entities , but you can see what has not been measured if   you want</li> </ul>"},{"location":"examples/random-walk/#whats-next","title":"What's next","text":"<ul> <li> <p> Search using an optimizer</p> <p>Try the Search a space with an optimizer example to see how you can use RayTune, and define custom experiments, via <code>ado</code>.</p> <p>Search a space with an optimizer </p> </li> <li> <p> Discovering important entity space dimensions</p> <p>Try the Identify the important dimensions of a space example to see how you can use <code>ado</code> to discover which entity space dimensions most influence a target metric.</p> <p>Identify the important dimensions of a space </p> </li> </ul>"},{"location":"examples/search-custom-objective/","title":"Search based on a custom objective function","text":"<p>Note</p> <p>This example shows how to create and use a custom objective function, an experiment which requires the output of another experiment, with <code>ado</code>.</p>"},{"location":"examples/search-custom-objective/#the-scenario","title":"The scenario","text":"<p>Often experiments will not directly produce the value that you are interested in. For example, an experiment might measure the run time of an application, while the meaningful metric is the associated cost, which requires knowing information like the cost per hour of the GPUs used. Another common scenario involves aggregating data points from one or more experiments into a single value.</p> <p>In this example we will install a custom objective function that calculates a cost for the application workload configurations used in the talking a random walk example. When the workload configuration space is explored using a random walk, both the <code>wallClockRuntime</code> and the <code>cost</code>, as defined by the custom function, will be measured.</p> <p>Caution</p> <p>The commands below assume you are in the directory <code>examples/ml-multi-cloud</code> in the ado source repository. See the instructions for cloning the repository.</p>"},{"location":"examples/search-custom-objective/#pre-requisites","title":"Pre-requisites","text":""},{"location":"examples/search-custom-objective/#install-the-ray_tune-ado-operator","title":"Install the ray_tune ado operator","text":"<p>If you haven't already installed the ray_tune operator, run (assumes you are in <code>examples/ml-multi-cloud/</code> ):</p> <pre><code>pip install ../../plugins/operators/ray_tune\n</code></pre> <p>then executing</p> <pre><code>ado get operators\n</code></pre> <p>should show an entry for <code>ray_tune</code> like below</p> <pre><code>Available operators by type:\n      OPERATOR     TYPE\n0  random_walk  explore\n1     ray_tune  explore\n</code></pre>"},{"location":"examples/search-custom-objective/#installing-the-custom-experiment","title":"Installing the custom experiment","text":"<p>The custom experiment is defined in a python package under <code>custom_actuator_function/</code>. To install it run:</p> <pre><code>pip install custom_experiment/\n</code></pre> <p>then</p> <pre><code>ado get actuators --details\n</code></pre> <p>will output something like</p> <pre><code>2          custom_experiments          CustomExperiments                        ml-multicloud-cost-v1.0       True\n3         molecule-embeddings                 Embeddings                   calculate-morgan-fingerprint       True\n4          molformer-toxicity         molformer-toxicity                               predict-toxicity       True\n5                     mordred         Mordred Descriptor                  mordred-descriptor-calculator       True\n6                       st4sd                      ST4SD                      toxicity-prediction-opera       True\n</code></pre> <p>You can see the custom experiment provided by the package, ml-multicloud-cost-v1.0 on the first line. Executing <code>ado describe experiment ml-multicloud-cost-v1.0</code></p> <p>outputs:</p> <pre><code>Identifier: custom_experiments.ml-multicloud-cost-v1.0\n\nRequired Inputs:\n  Constitutive Properties:\n      nodes\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [0, 1000]\n\n      cpu_family\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Values: [0, 1] Range: [0, 2]\n\n\n  Observed Properties:\n      op-benchmark_performance-wallClockRuntime\n\n\nOutputs: ml-multicloud-cost-v1.0-total_cost\n</code></pre> <p>From this you can see the <code>ml-multicloud-cost-v1.0</code> requires an observed property, i.e. a property measured by another experiment, as input. From the observed property identifier, the experiment is called <code>benchmark_performance</code> and the property is <code>wallClockRuntime</code>.</p>"},{"location":"examples/search-custom-objective/#create-a-discoveryspace-that-uses-the-custom-experiment","title":"Create a discoveryspace that uses the custom experiment","text":"<p>First create a <code>samplestore</code> with the <code>ml-multi-cloud</code> example data following these instructions. If you have already completed the talking a random walk example, reuse the <code>samplestore</code> you created there (use <code>ado get samplestores</code> if you cannot recall the identifier).</p> <p>To use the custom experiment you must add it in the <code>experiments</code> list of a <code>discoveryspace</code>. The <code>acutauatorIdentifer</code> will be <code>custom_experiments</code> and the <code>experimentIdentifier</code> will be the name of your experiment. For this case the relevant section looks like:</p> <pre><code>experiments:\n  - experimentIdentifier: \"benchmark_performance\"\n    actuatorIdentifier: \"replay\"\n  - experimentIdentifier: \"ml-multicloud-cost-v1.0\"\n    actuatorIdentifier: \"custom_experiments\"\n</code></pre> <p>The complete <code>discoveryspace</code> for this example is given in <code>ml_multicloud_space_with_custom.yaml</code> To create it execute:</p> <pre><code>ado create space -f ml_multicloud_space_with_custom.yaml --set \"sampleStoreIdentifier=$SAMPLE_STORE_IDENTIFIER\"\n</code></pre> <p>If an experiment takes the output of another experiment as input</p> <p>both experiments must be in the <code>discoveryspace</code>. In the above example if the entry <code>benchmark_performance</code> was omitted the <code>ado create space</code> command would fail with:</p> <p>SpaceInconsistencyError: MeasurementSpace does not contain an experiment measuring an observed property required by another experiment in the space</p> <p>Note the created <code>discoveryspace</code> resource identifier printed by this command for the next section. You can use this identifier in the following command to see a description of the space (replacing <code>$DISCOVERY_SPACE_IDENTIFIER</code>):</p> <pre><code>ado describe space $DISCOVERY_SPACE_IDENTIFIER\n</code></pre> <p>This will output:</p> <pre><code>Identifier: space-d5c150-0b762f\n\nEntity Space:\n\n  Number entities: 48\n\n  Categorical properties:\n           name     values\n    0  provider  [A, B, C]\n\n  Discrete properties:\n             name   range interval        values\n    0  cpu_family  [0, 2]     None        [0, 1]\n    1   vcpu_size  [0, 2]     None        [0, 1]\n    2       nodes  [2, 6]     None  [2, 3, 4, 5]\n\n\nMeasurement Space:\n                                       experiment  supported\n  0                 replay.benchmark_performance       True\n  1   custom_experiments.ml-multicloud-cost-v1.0       True\n\n  'replay.benchmark_performance'\n\n  Inputs:\n      parameter      type value parameterized\n  0  cpu_family  required  None            na\n  1   vcpu_size  required  None            na\n  2       nodes  required  None            na\n  3    provider  required  None            na\n\n  Outputs:\n       target property\n  0  wallClockRuntime\n  1            status  'custom_experiments.ml-multicloud-cost-v1.0'\n\n  Inputs:\n                                  parameter      type value parameterized\n  0                                   nodes  required  None            na\n  1                              cpu_family  required  None            na\n  2  benchmark_performance-wallClockRuntime  required  None            na\n\n  Outputs:\n     target property\n  0      total_cost\n\nSample store identifier: '0b762f'\n</code></pre>"},{"location":"examples/search-custom-objective/#exploring-the-discoveryspace","title":"Exploring the <code>discoveryspace</code>","text":"<p>To run a <code>randomwalk</code> operation on the new space, execute ((replacing <code>$DISCOVERY_SPACE_IDENTIFIER</code> with the identifier of the space you created):</p> <pre><code>ado create operation -f randomwalk_ml_multicloud_operation.yaml --set \"spaces[0]=$DISCOVERY_SPACE_IDENTIFIER\"\n</code></pre> <p>This produces an output similar to that described in the talking a random walk example and will exit printing the operation identifier. However, in this case there is additional information related to the dependent experiment.</p> <p>When it completes, execute the <code>ado show entities operation</code> command to see the results produced:</p> <pre><code>ado show entities operation $OPERATION_IDENTIFIER\n</code></pre> <p>You will see a table similar to the following - note the extra column for the new cost function:</p> <pre><code>               result_index                                   identifier                        benchmark_performance-wallClockRuntime benchmark_performance-status                           ml-multicloud-cost-v1.0-total_cost                                                                                        reason  valid\nrequest_index\n0                         0                               A_f0.0-c0.0-n3                         [221.5101969242096, 216.394127368927]                     [ok, ok]                     [1.8459183077017467, 1.8032843947410584]                                                                                                 True\n1                         0                               C_f0.0-c0.0-n5                        [150.9471504688263, 138.0605161190033]                     [ok, ok]                     [2.0964882009559207, 1.9175071683194902]                                                                                                 True\n2                         0                               A_f0.0-c0.0-n5                       [106.0709307193756, 130.30512285232544]                     [ok, ok]                      [1.473207371102439, 1.8097933729489646]                                                                                                 True\n3                         0                               C_f1.0-c1.0-n5                       [100.97977471351624, 92.17141437530518]                     [ok, ok]                      [2.8049937420421176, 2.560317065980699]                                                                                                 True\n4                         0                               C_f1.0-c0.0-n5                       [136.3071050643921, 135.47050046920776]                     [ok, ok]                       [3.786308474010892, 3.763069457477993]                                                                                                 True\n5                         0                               A_f0.0-c1.0-n2                                                    272.997822                           ok                                                     1.516655                                                                                                 True\n6                         0                               C_f1.0-c1.0-n4                                                    114.014369                           ok                                                     2.533653                                                                                                 True\n7                         0                               B_f1.0-c1.0-n2                                                    298.819305                           ok                                                     3.320214                                                                                                 True\n8                         0                               C_f0.0-c0.0-n4                                                    188.090878                           ok                                                     2.089899                                                                                                 True\n9                         0                               A_f1.0-c1.0-n2                                                    291.904456                           ok                                                     3.243383                                                                                                 True\n10                        0                               B_f0.0-c0.0-n3  [184.44801592826843, 153.51639366149902, 176.28814435005188]                 [ok, ok, ok]  [1.537066799402237, 1.2793032805124918, 1.4690678695837658]                                                                                                 True\n11                        0                               B_f0.0-c1.0-n2                        [166.74843192100525, 184.935049533844]                     [ok, ok]                     [0.9263801773389181, 1.0274169418546888]                                                                                                 True\n12                        0                               C_f0.0-c0.0-n2                                                    415.829285                           ok                                                     2.310163                                                                                                 True\n13                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.3                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n14                        0                               A_f0.0-c1.0-n4                                                    106.670121                           ok                                                     1.185224                                                                                                 True\n15                        0                               B_f1.0-c0.0-n3                       [273.7120273113251, 220.19828414916992]                     [ok, ok]                       [4.561867121855418, 3.669971402486165]                                                                                                 True\n16                        0                               B_f1.0-c0.0-n2                                                    346.070996                           ok                                                     3.845233                                                                                                 True\n17                        0                               B_f0.0-c0.0-n5   [113.88505148887634, 103.90595746040344, 112.7056987285614]                 [ok, ok, ok]  [1.5817368262343936, 1.443138298061159, 1.5653569267855751]                                                                                                 True\n18                        0                               C_f1.0-c1.0-n2                                                    363.285671                           ok                                                     4.036507                                                                                                 True\n19                        0                               C_f0.0-c1.0-n5                        [95.86326050758362, 85.67946743965149]                     [ok, ok]                      [1.331434173716439, 1.1899926033284929]                                                                                                 True\n20                        0                               C_f1.0-c1.0-n3                       [154.9813470840454, 168.34859228134155]                     [ok, ok]                      [2.583022451400757, 2.8058098713556925]                                                                                                 True\n21                        0                               B_f0.0-c0.0-n2                       [228.14362454414368, 225.1791422367096]                     [ok, ok]                      [1.267464580800798, 1.2509952346483866]                                                                                                 True\n22                        0  provider.B-cpu_family.0-vcpu_size.1-nodes.4                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n23                        0                               C_f0.0-c1.0-n4                                                    121.424925                           ok                                                     1.349166                                                                                                 True\n24                        0                               A_f0.0-c0.0-n2                                                    335.208518                           ok                                                      1.86227                                                                                                 True\n25                        0                               A_f1.0-c1.0-n4                                                    116.314171                           ok                                                     2.584759                                                                                                 True\n26                        0                               C_f0.0-c1.0-n2                                                    309.842324                           ok                                                     1.721346                                                                                                 True\n27                        0                               A_f1.0-c1.0-n3                      [155.02856159210205, 151.58562421798706]                     [ok, ok]                      [2.5838093598683676, 2.526427070299784]                                                                                                 True\n28                        0                               C_f1.0-c0.0-n2                                                    463.396539                           ok                                                      5.14885                                                                                                 True\n29                        0                               A_f0.0-c0.0-n4                                                    145.129484                           ok                                                      1.61255                                                                                                 True\n30                        0                               A_f1.0-c0.0-n3                       [206.74496150016785, 236.1715066432953]                     [ok, ok]                      [3.4457493583361307, 3.936191777388255]                                                                                                 True\n31                        0  provider.B-cpu_family.0-vcpu_size.1-nodes.3                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n32                        0                               C_f0.0-c0.0-n3                       [269.0906641483307, 240.07358503341675]                     [ok, ok]                       [2.242422201236089, 2.000613208611806]                                                                                                 True\n33                        0                               B_f0.0-c0.0-n4                       [113.87676978111269, 132.5415120124817]                     [ok, ok]                      [1.265297442012363, 1.4726834668053521]                                                                                                 True\n34                        0                               A_f1.0-c0.0-n4                                                    158.706395                           ok                                                     3.526809                                                                                                 True\n35                        0                               C_f0.0-c1.0-n3                        [168.9163637161255, 174.0335624217987]                     [ok, ok]                     [1.4076363643010457, 1.4502796868483225]                                                                                                 True\n36                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.5                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n37                        0                               B_f1.0-c0.0-n4                      [202.48239731788635, 193.55997109413147]                     [ok, ok]                       [4.499608829286363, 4.301332690980699]                                                                                                 True\n38                        0                               C_f1.0-c0.0-n4                                                    177.723598                           ok                                                     3.949413                                                                                                 True\n39                        0                               B_f1.0-c0.0-n5                      [168.79178500175476, 141.99024295806885]                     [ok, ok]                       [4.688660694493188, 3.944173415501912]                                                                                                 True\n40                        0  provider.B-cpu_family.0-vcpu_size.1-nodes.5                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n41                        0                               C_f1.0-c0.0-n3                       [244.33887457847595, 598.8834657669067]             [ok, Timed out.]                      [4.0723145763079325, 9.981391096115113]                                                                                                 True\n42                        0                               A_f1.0-c1.0-n5                        [96.8471610546112, 105.63729166984558]                     [ok, ok]                      [2.6901989181836448, 2.934369213051266]                                                                                                 True\n43                        0                               A_f0.0-c1.0-n3                      [170.15659737586975, 168.36590766906738]                     [ok, ok]                     [1.4179716447989146, 1.4030492305755615]                                                                                                 True\n44                        0  provider.B-cpu_family.1-vcpu_size.1-nodes.4                                                                                                                                                          Externally defined experiments cannot be applied to entities: replay.benchmark_performance.   False\n45                        0                               A_f1.0-c0.0-n2                                                     378.31657                           ok                                                     4.203517                                                                                                 True\n46                        0                               A_f0.0-c1.0-n5                        [86.23016095161438, 84.45346999168396]                     [ok, ok]                     [1.1976411243279776, 1.1729648609956105]                                                                                                 True\n47                        0                               A_f1.0-c0.0-n5                      [117.94136571884157, 135.91092538833618]                     [ok, ok]                       [3.276149047745599, 3.775303483009338]                                                                                                 True\n</code></pre>"},{"location":"examples/search-custom-objective/#explore-further","title":"Explore Further","text":"<ul> <li>perform an optimization instead of a random walk: See the   search a space with an optimizer example.</li> <li>modify the objective function: Try modifying the cost function and creating   a new space - be careful to change the name of the experiment!</li> <li>create a custom experiment: Explore   the documentation for writing your own custom experiment</li> <li>break the discoveryspace: See what happens if you try to create the   <code>discoveryspace</code> without the experiment that provides input to the cost   function.</li> <li>examine the requests: Run <code>ado show requests operation</code> to see what is   replayed (<code>benchmark_performance</code>) and what is calculated   (<code>ml_multicloud_cost-v1.0</code>)</li> </ul>"},{"location":"examples/search-custom-objective/#takeaways","title":"Takeaways","text":"<ul> <li>dependent experiments: <code>ado</code> allows you to define experiments which   consume the output of other experiments.</li> <li>There is no limit to the depth of the chain of dependent experiments</li> <li>Dependent experiments are executed when the required inputs are available.</li> <li>custom experiments: You can add your own python functions as experiments   using <code>ado</code>'s custom experiments feature.</li> <li>uniform usage pattern: How you use <code>ado</code> to define spaces or perform   operations does not change if you use custom or dependent experiments</li> </ul>"},{"location":"examples/search-custom-objective/#whats-next","title":"What's next","text":"<ul> <li> <p> Search using an optimizer</p> <p>Try the Search a space with an optimizer example to see how you can use RayTune in combination with custom experiments, via <code>ado</code>.</p> <p>Search a space with an optimizer </p> </li> <li> <p> Discovering important entity space dimensions</p> <p>Try the Identify the important dimensions of a space example to see how you can use <code>ado</code> to discover which entity space dimensions most influence a target metric.</p> <p>Identify the important dimensions of a space </p> </li> </ul>"},{"location":"getting-started/MAINTAINERS/","title":"MAINTAINERS","text":"<p>Following is the current list of maintainers on this project.</p> <p>The maintainers are listed in alphabetical order.</p> <ul> <li>Alessandro Pomponio (AlessandroPomponio)</li> <li>Christian Pinto (christian-pinto)</li> <li>Daniele Lotito (danielelotito)</li> <li>Michael Johnston (michael-johnston)</li> <li>Michele Gazzetti (mgazz)</li> <li>Srikumar Venugopal (srikumar003)</li> <li>Vassilis Vassiliadis (VassilisVassiliadis)</li> </ul>"},{"location":"getting-started/ado/","title":"The ado CLI","text":"<p>Note</p> <p>This page provides documentation for the <code>ado</code> CLI tool, which needs to be installed. If this is not the case, follow the instructions provided in Installation</p> <p>ado comes with a CLI utility that is designed to be familiar for users of <code>kubectl</code> and <code>oc</code>. It allows creating and retrieving resources, managing multiple backends, executing actuators, and more.</p> <p>This page provides documentation for every command that we support, presented in alphabetical order. Refer to the navigation pane on the left to go to the section you are more interested in.</p>"},{"location":"getting-started/ado/#cli-reference","title":"CLI reference","text":""},{"location":"getting-started/ado/#ado","title":"ado","text":"<p>ado supports a set of generic options that are passed down to all the other commands.</p> <pre><code>ado [--context | -c &lt;context-file.yaml&gt;] \\\n    [--log-level | -l &lt;value&gt;]\n</code></pre> <ul> <li><code>--context | -c</code> allows overriding the active context with one loaded from a   file. This feature should only be used when running on remote Ray clusters.</li> <li><code>--log-level | -l</code> allows configuring the level of logging to be used. This   does not affect child processes.</li> </ul>"},{"location":"getting-started/ado/#ado-context","title":"ado context","text":"<p>ado supports storing configuration and authentication details for multiple backends, which in ado terms are called contexts.</p> <p>The complete syntax of the <code>ado context</code> command is as follows:</p> <pre><code>ado context [CONTEXT_NAME]\n</code></pre>"},{"location":"getting-started/ado/#examples","title":"Examples","text":""},{"location":"getting-started/ado/#getting-the-current-context","title":"Getting the current context","text":"<p>In a similar way to <code>oc project</code>, users can see the name of the currently active context by running:</p> <pre><code>ado context\n</code></pre>"},{"location":"getting-started/ado/#listing-available-contexts","title":"Listing available contexts","text":"<p>In a similar way to <code>oc projects</code>, users can see the available contexts by running:</p> <pre><code>ado contexts\n</code></pre> <p>The default context will also be printed out.</p>"},{"location":"getting-started/ado/#switching-between-contexts","title":"Switching between contexts","text":"<p>To switch between the available contexts, provide the name of the target context to the <code>ado context</code> command. In this example we assume that the <code>my-context</code> context exists:</p> <pre><code>ado context my-context\n</code></pre>"},{"location":"getting-started/ado/#ado-create","title":"ado create","text":"<p>The ado CLI provides the create command to create resources given a YAML file with their configuration.</p> <p>The complete syntax of the <code>ado create</code> command is as follows:</p> <pre><code>ado create RESOURCE_TYPE [--file | -f &lt;FILE.yaml&gt;] \\\n                         [--set &lt;jsonpath=json-value&gt;] \\\n                         [--new-sample-store] [--dry-run]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types for <code>ado create</code>,   currently:</p> <ul> <li>actuator</li> <li>actuatorconfiguration</li> <li>context</li> <li>operation</li> <li>samplestore</li> <li>space</li> </ul> </li> <li> <p><code>--file</code> or <code>-f</code> is a path to the resource configuration file in YAML format.   It is mandatory in all scenarios, except when running   <code>ado create samplestore --new-sample-store</code>.</p> </li> <li><code>--new-sample-store</code> creates a new sample store. Only available when running   <code>ado create</code> on <code>space</code> and <code>samplestore</code>. If running   <code>ado create space --new-sample-store</code>, the optional <code>sampleStoreIdentifier</code>   contained in the <code>DiscoverySpaceConfiguration</code> will be disregarded.</li> <li><code>--set</code> allows overriding fields in the provided resource configuration. It   supports using JSONPath syntax. See the examples section for more information.</li> <li><code>--dry-run</code> is an optional flag to only validate the resource   configuration file provided and not actually creating the resource.</li> </ul>"},{"location":"getting-started/ado/#examples_1","title":"Examples","text":""},{"location":"getting-started/ado/#creating-a-discovery-space","title":"Creating a Discovery Space","text":"<p>In this example, we assume that the file <code>ds.yaml</code> exists and contains a valid Discovery Space definition.</p> <pre><code>ado create -f ds.yaml\n</code></pre>"},{"location":"getting-started/ado/#validating-a-sample-store-definition","title":"Validating a Sample Store definition","text":"<p>In this example, we assume that the file <code>sample-store.yml</code> exists, but we make no further assumptions on whether its content is a valid Sample Store definition or not.</p> <pre><code>ado create -f sample-store.yml --dry-run\n</code></pre>"},{"location":"getting-started/ado/#creating-a-new-sample-store-with-no-file","title":"Creating a new sample store with no file","text":"<pre><code>ado create samplestore --new-sample-store\n</code></pre>"},{"location":"getting-started/ado/#creating-a-space-with-a-new-sample-store","title":"Creating a space with a new sample store","text":"<p>Note that if the space definition <code>ds.yaml</code> contains an <code>sampleStoreIdentifier</code>, it will be disregarded, and a new one will be created.</p> <pre><code>ado create space -f ds.yaml --new-sample-store\n</code></pre>"},{"location":"getting-started/ado/#create-a-space-overriding-the-sample-store-identifier","title":"Create a space overriding the sample store identifier","text":"<pre><code>ado create space -f ds.yaml --set \"sampleStoreIdentifier=abcdef\"\n</code></pre>"},{"location":"getting-started/ado/#create-a-space-renaming-a-property-identifier-in-the-space","title":"Create a space renaming a property identifier in the space","text":"<pre><code>ado create space -f ds.yaml --set \"entitySpace[0].identifier=abcdef\"\n</code></pre>"},{"location":"getting-started/ado/#ado-delete","title":"ado delete","text":"<p>The ado CLI provides the delete command to delete resources given their unique identifier.</p> <p>The complete syntax of the <code>ado delete</code> command is as follows:</p> <pre><code>ado delete RESOURCE_TYPE RESOURCE_ID \\\n           [--force] \\\n           [--delete-local-db] [--no-delete-local-db]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is the type of resource you want to delete. Currently, the   only supported types are:</p> <ul> <li>actuatorconfiguration</li> <li>context</li> <li>datacontainer</li> <li>operation</li> <li>samplestore</li> <li>space</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource to delete.</p> </li> <li><code>--force</code> allows forcing deletion of resources in the following cases:<ul> <li>When attempting to delete operations while other operations are executing.</li> <li>When attempting to delete sample stores that still contain data.</li> </ul> </li> <li>When deleting a local context, users can specify the flags <code>--delete-local-db</code>   or <code>--no-delete-local-db</code> to explicitly delete or preserve a local DB when   deleting its related context. If neither of these flags are specified, the   user will be asked whether to delete the DB or not.</li> </ul>"},{"location":"getting-started/ado/#examples_2","title":"Examples","text":""},{"location":"getting-started/ado/#deleting-a-context","title":"Deleting a context","text":"<pre><code>ado delete context my-context\n</code></pre>"},{"location":"getting-started/ado/#deleting-a-local-context-and-preserving-the-local-db","title":"Deleting a local context and preserving the local db","text":"<pre><code>ado delete context my-local-context --no-delete-local-db\n</code></pre>"},{"location":"getting-started/ado/#deleting-a-space","title":"Deleting a space","text":"<pre><code>ado delete space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#ado-describe","title":"ado describe","text":"<p>ado provides the <code>describe</code> command to retrieve easy-to-read information about resources.</p> <p>The complete syntax of the <code>ado describe</code> command is as follows:</p> <pre><code>ado describe RESOURCE_TYPE [RESOURCE_ID] [--file | -f &lt;file.yaml&gt;]\\\n             [--actuator-id &lt;actuator&gt;]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is the type of resource you want to describe. Currently, the   supported resource types are:</p> <ul> <li>experiment</li> <li>datacontainer</li> <li>space</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource to describe.</p> </li> <li>The <code>--file</code> (or <code>-f</code>) flag is currently only available for spaces and   allows getting a description of the space, given a space configuration file.</li> <li><code>--actuator-id</code> (optional) can be used only when the resource type is   experiment and is used to indicate what actuator the experiment belongs to.</li> </ul>"},{"location":"getting-started/ado/#examples_3","title":"Examples","text":""},{"location":"getting-started/ado/#describing-a-discovery-space","title":"Describing a Discovery Space","text":"<pre><code>ado describe space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#ado-edit","title":"ado edit","text":"<p>ado automatically stores metadata in the backend for some of the resources you can create. The quickest way to make adjustments to these metadata is to use the <code>ado edit</code> command.</p> <p>The complete syntax of the <code>ado edit</code> command is as follows:</p> <pre><code>ado edit RESOURCE_TYPE RESOURCE_ID [--editor &lt;NAME&gt;]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is the type of resource you want to edit. Supported types are:</p> <ul> <li>actuatorconfiguration</li> <li>datacontainer</li> <li>operation</li> <li>samplestore</li> <li>space</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource to edit.</p> </li> <li> <p><code>--editor</code> is the name of the editor you want to use for editing metadata. It   must be one of the supported ones, which currently are:</p> <ul> <li><code>vim</code> (default)</li> <li><code>vi</code></li> <li><code>nano</code></li> </ul> </li> </ul> <p>Alternatively, you can also set the value for this flag by using the   environment variable <code>ADO_EDITOR</code>.</p>"},{"location":"getting-started/ado/#examples_4","title":"Examples","text":""},{"location":"getting-started/ado/#editing-an-operations-metadata","title":"Editing an operation's metadata","text":"<pre><code>ado edit operation randomwalk-0.5.0-123abc\n</code></pre>"},{"location":"getting-started/ado/#editing-a-spaces-metadata-using-a-different-editor","title":"Editing a space's metadata using a different editor","text":"<pre><code>ado edit space space-abc123-456def --editor nano\n</code></pre>"},{"location":"getting-started/ado/#editing-a-spaces-metadata-using-a-different-editor-set-by-environment-variable","title":"Editing a space's metadata using a different editor (set by environment variable)","text":"<pre><code>ADO_EDITOR=nano ado edit space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#ado-get","title":"ado get","text":"<p>ado allows getting resources in a similar way to <code>kubectl</code>. Users can choose to either get all resources of a given type or specify a resource identifier to limit the results to a single resource.</p> <p>The complete syntax of the <code>ado get</code> command is as follows:</p> <pre><code>ado get RESOURCE_TYPE [RESOURCE_ID] [--output | -o &lt;default | yaml | json | config | raw&gt;] \\\n                                    [--exclude-default | --no-exclude-default] \\\n                                    [--exclude-unset | --no-exclude-unset ] \\\n                                    [--exclude-none | --no-exclude-none ] \\\n                                    [--minimize] \\\n                                    [--query | -q &lt;path=candidate&gt;] \\\n                                    [--label | -l &lt;key=value&gt;] \\\n                                    [--details] [--show-deprecated] \\\n                                    [--matching-point &lt;point.yaml&gt;] \\\n                                    [--matching-space &lt;space.yaml] \\\n                                    [--matching-space-id &lt;space-id&gt;] \\\n                                    [--from-sample-store &lt;sample-store-id&gt;] \\\n                                    [--from-space &lt;space-id&gt;] [--from-operation &lt;operation-id&gt;]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is the type of resource you want to get. Currently, the only   supported types are:</p> <ul> <li>actuatorconfigurations</li> <li>actuators</li> <li>contexts</li> <li>datacontainers</li> <li>operations</li> <li>operators</li> <li>samplestores</li> <li>spaces</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the optional unique identifier of the resource to get.</p> </li> <li> <p><code>--output</code> or <code>-o</code> determine the type of output that will be displayed:</p> <ul> <li>The <code>default</code> format shows the identifier, the name, and the age of   the matching resources.</li> <li>The <code>yaml</code> format displays the full YAML document of the matching resources.</li> <li>The <code>json</code> format displays the full JSON document of the matching resources.</li> <li>The <code>config</code> format displays the <code>config</code> field of the matching resources.</li> <li>The <code>raw</code> format displays the raw resource as stored in the database,   performing no validation.</li> </ul> </li> <li> <p><code>--exclude-default</code> (set by default) allows excluding fields that use default   values from the output. Alternatively, the <code>--no-exclude-default</code> flag can be   used to show them.</p> </li> <li><code>--exclude-unset</code> (set by default) allows excluding from the output fields   whose values have not been set. Alternatively, the <code>--no-exclude-unset</code> flag   can be used to show them.</li> <li><code>--exclude-none</code> (set by default) allows excluding fields that have null   values from the output. Alternatively, the <code>--no-exclude-none</code> flag can be   used to show them.</li> <li><code>--exclude-field</code> allows the user to exclude fields from the output using   JSONPath expressions. Documentation for creating these expressions can be   found here:   https://github.com/h2non/jsonpath-ng?tab=readme-ov-file#jsonpath-syntax.   This flag is only supported when using the <code>yaml</code>, <code>json</code>, or <code>config</code> output   format.</li> <li><code>--minimize</code> attempts to minimize the output. This might entail applying   transformations on the model, changing it from the original. If set, it   implies <code>--exclude-default</code>, <code>--exclude-unset</code>, and <code>--exclude-none</code>. This   option is ignored when the output type is <code>default</code> or <code>raw</code>.</li> <li>By using (optionally multiple times) the <code>--query</code> (or <code>-q</code>) flag, users can   restrict the resources returned by requiring that a field in the resource is   equal to a provided value or that the content of a JSON document appear in the   resource. This flag can be specified multiple times (even in conjunction with   <code>-l</code> to further filter results). More information is provided in the   Using the field-level querying functionality   section.</li> <li>By using (optionally multiple times) the <code>--label</code> (or <code>-l</code>) flag, users can   restrict the resources returned by means of the labels set in the resource's   metadata. Labels must be specified in the <code>key=value</code> format. This flag can be   specified multiple times (even in conjunction with <code>-q</code> to further filter   results).</li> <li>When using the <code>--details</code> flag with the <code>default</code> output format, additional   columns with the description and the labels of the matching resources are   printed.</li> <li>The <code>--show-deprecated</code> flag is available only for   <code>ado get actuators --details</code> and allows displaying experiments that have   been deprecated. They are otherwise hidden by default.</li> <li>The <code>--matching-point</code> option allows the user to specify a file in the form   of:</li> </ul> <pre><code>entity: # A key-value dictionary of constitutive property identifiers and values\n  batch_size: 8\n  number_gpus: 4\nexperiments: # A list of experiments\n  - finetune-lora-fsdp-r-4-a-16-tm-default-v2.0.0\n</code></pre> <p>To find the spaces that match that point.</p> <ul> <li>The <code>--matching-space</code> option allows the user to specify a   <code>DiscoverySpaceConfiguration</code> file which will be used to find similar spaces   in the database. Spaces will match if:<ul> <li>They include exactly the same base experiments.</li> <li>Their entity space is in a hierarchical relationship with the input space.   The relationship will be output (i.e., a column will say whether the   matching space is a subspace, a superspace, or an exact match with the input   space).</li> </ul> </li> <li>The <code>--matching-space-id</code> option works in the same way as <code>--matching-space</code>   but allows the user to provide a space id instead of a   <code>DiscoverySpaceConfiguration</code>.</li> <li>The <code>--from-sample-store</code>, <code>--from-space</code>, <code>--from-operation</code> flags are   available only for <code>ado get measurementrequests</code> and allow specifying what   samplestore/space/operation the measurement request belongs to.</li> </ul>"},{"location":"getting-started/ado/#using-the-field-level-querying-functionality","title":"Using the field-level querying functionality","text":"<p>The field-querying functionality leverages MySQL's <code>JSON_CONTAINS</code> function. The user is encouraged to familiarise themselves with the documentation to understand what the capabilities provided are.</p> <p>Under the hood, users will be querying a JSON representation of the resources that are stored in the database. In a similar way to what is provided by the <code>jq</code> command line utility, querying a field means providing a dot-separated path leading to the target field (NOTE: the path is case sensitive) and the value the user is expecting to find in that field.</p> <p>As an example, if you want to query operations that use RayTune you can do it with:</p> <pre><code>ado get operations -q config.operation.module.moduleClass=RayTune\n</code></pre> <p>When dealing with array fields, instead, the user will have to query the array field and provide a JSON document that is to be contained in the array.</p> <ul> <li>NOTE: the document provided is required to be valid JSON, meaning that   keys and values must be in between quotes.</li> <li>NOTE: when querying an array field with a document, the results will   include all resources that AT LEAST include the provided document and not the   resources that ONLY have it. The -q and -l options can be used multiple times   to return only resources that match all the filters.</li> </ul> <p>As an example, to query all spaces that run the <code>finetune-lora-fsdp-r-4-a-16-tm-default-v2.0.0</code> experiment on the <code>NVIDIA-A100-SXM4-80GB</code> GPU and use the <code>mistral-7b-v0.1</code> model you can run:</p> <pre><code>ado get space -q 'config.entitySpace={\"identifier\": \"model_name\", \"propertyDomain\":{\"values\":[\"mistral-7b-v0.1\"]}}' \\\n              -q 'config.entitySpace={\"identifier\": \"gpu_model\", \"propertyDomain\":{\"values\":[\"NVIDIA-A100-SXM4-80GB\"]}}' \\\n              -q 'config.experiments={\"experiments\":{\"identifier\":\"finetune-lora-fsdp-r-4-a-16-tm-default-v2.0.0\"}}'\n</code></pre>"},{"location":"getting-started/ado/#examples_5","title":"Examples","text":""},{"location":"getting-started/ado/#getting-all-discovery-spaces","title":"Getting all Discovery Spaces","text":"<pre><code>ado get spaces\n</code></pre>"},{"location":"getting-started/ado/#getting-all-discovery-spaces-with-additional-details","title":"Getting all Discovery Spaces with additional details","text":"<pre><code>ado get spaces --details\n</code></pre>"},{"location":"getting-started/ado/#getting-all-discovery-spaces-that-include-granite-7b-base-in-the-property-domain","title":"Getting all Discovery Spaces that include granite-7b-base in the property domain","text":"<p>Info</p> <p>More information on field-level querying is provided in the Using the field-level querying functionality section</p> <pre><code>ado get space -q 'config.entitySpace={\"propertyDomain\":{\"values\":[\"granite-7b-base\"]}}'\n</code></pre>"},{"location":"getting-started/ado/#getting-all-discovery-spaces-with-certain-labels","title":"Getting all Discovery Spaces with certain labels","text":"<pre><code>ado get spaces -l key1=value1 -l key2=value2\n</code></pre>"},{"location":"getting-started/ado/#getting-all-discovery-spaces-matching-a-point","title":"Getting all discovery spaces matching a point","text":"<p>Assuming you have the following file saved as <code>point.yaml</code>:</p> <pre><code>entity: # A key-value dictionary of constitutive property identifiers and values\n  batch_size: 8\n  number_gpus: 4\nexperiments: # A list of experiments\n  - finetune-lora-fsdp-r-4-a-16-tm-default-v2.0.0\n</code></pre> <p>You can run:</p> <pre><code>ado get spaces --matching-point point.yaml\n</code></pre>"},{"location":"getting-started/ado/#getting-all-discoveryspaces-and-hiding-fields","title":"Getting all DiscoverySpaces and hiding fields","text":"<p>This example shows how to hide the <code>propertyDomain.variableType</code> and <code>propertyDomain.domainRange</code> fields from the Discovery Space's entity space:</p> <pre><code>ado get space space-df8077-7535f9 -o yaml \\\n  --exclude-field \"config.entitySpace[*].propertyDomain.variableType\" \\\n  --exclude-field \"config.entitySpace[*].propertyDomain.domainRange\"\n</code></pre>"},{"location":"getting-started/ado/#getting-an-actuator-configuration-and-hiding-the-status-for-the-created-event","title":"Getting an actuator configuration and hiding the status for the \"created\" event","text":"<pre><code>ado get actuatorconfiguration actuatorconfiguration-myactuator-123456 -o yaml \\\n  --exclude-field 'status[?(@.event=\"created\")]'\n</code></pre>"},{"location":"getting-started/ado/#getting-a-single-operation","title":"Getting a single Operation","text":"<pre><code>ado get operation randomwalk-0.5.0-123abc\n</code></pre>"},{"location":"getting-started/ado/#getting-the-yaml-of-a-single-operation","title":"Getting the YAML of a single Operation","text":"<pre><code>ado get operation randomwalk-0.5.0-123abc -o yaml\n</code></pre>"},{"location":"getting-started/ado/#displaying-all-current-experiments","title":"Displaying all current experiments","text":"<pre><code>ado get actuators --details\n</code></pre>"},{"location":"getting-started/ado/#displaying-all-experiments-for-the-st4sd-actuator","title":"Displaying all experiments for the st4sd actuator","text":"<pre><code>ado get actuator st4sd --details --show-deprecated\n</code></pre>"},{"location":"getting-started/ado/#getting-the-yaml-of-a-measurementrequest-from-an-operation","title":"Getting the yaml of a MeasurementRequest from an operation","text":"<pre><code>ado get measurementrequest measurement-request-123 --from-operation randomwalk-0.5.0-123abc -o yaml\n</code></pre>"},{"location":"getting-started/ado/#ado-show","title":"ado show","text":"<p>When interacting with resources, we might be interested in seeing some of their details, entities measured, or related resources. <code>ado show</code> provides this with the four following subcommands.</p>"},{"location":"getting-started/ado/#ado-show-details","title":"ado show details","text":"<p>show details supports displaying aggregate details about resources and related resources.</p> <p>The complete syntax of the <code>ado show details</code> command is as follows:</p> <pre><code>ado show details RESOURCE_TYPE RESOURCE_ID\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types:</p> <ul> <li>operations</li> <li>spaces</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource you want to see details   for.</p> </li> </ul>"},{"location":"getting-started/ado/#examples_6","title":"Examples","text":""},{"location":"getting-started/ado/#show-details-for-a-space","title":"Show details for a space","text":"<pre><code>ado show details space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#ado-show-entities","title":"ado show entities","text":"<p>show entities supports displaying entities that belong to a space or an operation.</p> <p>The complete syntax of the <code>ado show entities</code> command is as follows:</p> <pre><code>ado show entities RESOURCE_TYPE [RESOURCE_ID] [--file | -f &lt;file.yaml&gt;] \\\n                  [--property-format {observed | target}] \\\n                  [--output-format {console | csv | json}] \\\n                  [--property &lt;property-name&gt;] \\\n                  [--include {sampled | matching | missing | unsampled}] \\\n                  [--aggregate {mean | median | variance | std | min | max}]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types:</p> <ul> <li>operations</li> <li>spaces</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource you want to see   entities for.</p> </li> <li>The <code>--file</code> (or <code>-f</code>) flag is currently only available for spaces and   enables showing entities that match the space defined in the configuration   file. NOTE: using this flag forces <code>--include matching</code>.</li> <li> <p><code>--property-format</code> defines the naming format used for measured properties in   the output, one of:</p> <ul> <li><code>observed</code>: properties are named <code>$experimentid.$property_id</code>. There will be   one row per entity.</li> <li><code>target</code>: properties are named <code>$property_id</code>. There will be one row per   (entity, experiment) pair.</li> </ul> </li> <li> <p><code>--output-format</code> is the format in which to display the entity data. One of:</p> <ul> <li><code>console</code> (print to stdout)</li> <li><code>csv</code> (output as CSV file)</li> <li><code>json</code> (output as JSON file)</li> </ul> </li> <li> <p><code>--property</code> (can be specified multiple times) is used to filter what measured   properties need to be output.</p> </li> <li> <p><code>--include</code> (exclusive to spaces) determines what type of entities to   include. One of:</p> <ul> <li><code>sampled</code>: Entities that have been measured by explore operations on the   <code>discoveryspace</code></li> <li><code>unsampled</code>: Entities that have not been measured by an explore operation on   the <code>discoveryspace</code></li> <li><code>matching</code>: Entities in the <code>samplestore</code> the <code>discoveryspace</code> uses, that   match the <code>discoveryspace</code>'s description</li> <li><code>missing</code>: Entities in the <code>discoveryspace</code> that are not in the   <code>samplestore</code> the <code>discoveryspace</code> uses</li> </ul> </li> <li> <p><code>--aggregate</code> allows applying an aggregation to the result values in case   multiple are present. One of:</p> <ul> <li><code>mean</code></li> <li><code>median</code></li> <li><code>variance</code></li> <li><code>std</code></li> <li><code>min</code></li> <li><code>max</code></li> </ul> </li> </ul>"},{"location":"getting-started/ado/#examples_7","title":"Examples","text":""},{"location":"getting-started/ado/#show-matching-entities-in-a-space-with-target-format-and-output-them-as-csv","title":"Show matching entities in a Space with target format and output them as CSV","text":"<pre><code> ado show entities space space-abc123-456def --include matching \\\n                                             --property-format target \\\n                                             --output-format csv\n</code></pre>"},{"location":"getting-started/ado/#show-a-subset-of-the-properties-of-entities-that-are-part-of-an-operation-and-output-them-as-json","title":"Show a subset of the properties of entities that are part of an operation and output them as JSON","text":"<pre><code>ado show entities operation randomwalk-0.5.0-123abc --output-format json \\\n                                                    --property my-property-1 \\\n                                                    --property my-property-2\n</code></pre>"},{"location":"getting-started/ado/#ado-show-requests","title":"ado show requests","text":"<p>show requests supports displaying the <code>MeasurementRequest</code>s that were part of an operation.</p> <p>The complete syntax of the <code>ado show requests</code> command is as follows:</p> <pre><code>ado show requests operation RESOURCE_ID [--output-format | -o &lt;console | csv | json&gt;] \\\n                                        [--hide &lt;field&gt;]\n</code></pre> <ul> <li><code>--output-format</code> determines whether the output will be printed to console or   saved to a file.</li> <li><code>--hide</code> can be specified multiple times and allows hiding fields from the   output.</li> </ul>"},{"location":"getting-started/ado/#examples_8","title":"Examples","text":""},{"location":"getting-started/ado/#show-measurement-requests-for-an-operation-and-save-them-as-csv","title":"Show measurement requests for an operation and save them as csv","text":"<pre><code>ado show requests operation randomwalk-0.5.0-123abc -o csv\n</code></pre>"},{"location":"getting-started/ado/#show-measurement-requests-for-an-operation-and-hide-certain-fields","title":"Show measurement requests for an operation and hide certain fields","text":"<pre><code>ado show requests operation randomwalk-0.5.0-123abc --hide type --hide \"experiment id\"\n</code></pre>"},{"location":"getting-started/ado/#ado-show-results","title":"ado show results","text":"<p>show results supports displaying the <code>MeasurementResult</code>s that were part of an operation.</p> <p>The complete syntax of the <code>ado show results</code> command is as follows:</p> <pre><code>ado show results operation RESOURCE_ID [--output-format | -o &lt;console | csv | json&gt;] \\\n                                       [--hide &lt;field&gt;]\n</code></pre> <ul> <li><code>--output-format</code> determines whether the output will be printed to console or   saved to a file.</li> <li><code>--hide</code> can be specified multiple times and allows hiding fields from the   output.</li> </ul>"},{"location":"getting-started/ado/#examples_9","title":"Examples","text":""},{"location":"getting-started/ado/#show-measurement-results-for-an-operation","title":"Show measurement results for an operation","text":"<pre><code>ado show results operation randomwalk-0.5.0-123abc -o csv\n</code></pre>"},{"location":"getting-started/ado/#show-measurement-results-for-an-operation-and-hide-certain-fields","title":"Show measurement results for an operation and hide certain fields","text":"<pre><code>ado show results operation randomwalk-0.5.0-123abc --hide uid\n</code></pre>"},{"location":"getting-started/ado/#ado-show-related","title":"ado show related","text":"<p>show related supports displaying resources that are related to the one whose id is provided (e.g., operations run on a space).</p> <p>The complete syntax of the <code>ado show related</code> command is as follows:</p> <pre><code>ado show related RESOURCE_TYPE RESOURCE_ID\n</code></pre> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types:</p> <ul> <li>operation</li> <li>samplestore</li> <li>space</li> </ul> </li> <li> <p><code>RESOURCE_ID</code> is the unique identifier of the resource you want to see related   resources for.</p> </li> </ul>"},{"location":"getting-started/ado/#examples_10","title":"Examples","text":""},{"location":"getting-started/ado/#show-resources-related-to-a-discovery-space","title":"Show resources related to a discovery space","text":"<pre><code> ado show related space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#ado-show-summary","title":"ado show summary","text":"<p>show summary supports generating overviews about discovery spaces. The content can be provided in the following formats:</p> <ul> <li>Markdown table (for high level overviews).</li> <li>Markdown text (for an easy to read and more in-depth format)</li> <li>CSV</li> </ul> <p>The complete syntax of the <code>ado show summary</code> command is as follows:</p> <pre><code>ado show summary RESOURCE_TYPE [RESOURCE_IDS...] \\\n                 [--query | -q &lt;path=candidate&gt;] \\\n                 [--label | -l &lt;LABEL&gt; ] \\\n                 [--with-property | -p &lt;PROPERTY&gt; ] \\\n                 [--format | -o &lt;md | table | csv&gt;]\n</code></pre> <p>Where:</p> <ul> <li><code>RESOURCE_TYPE</code> is always space</li> <li><code>RESOURCE_IDS</code> are one or more space-separated space identifiers.</li> <li>By using (optionally multiple times) the <code>--query</code> (or <code>-q</code>) flag, users can   restrict the resources returned by requiring that a field in the resource is   equal to a provided value or that the content of a JSON document appear in the   resource. This flag can be specified multiple times (even in conjunction with   <code>-l</code> to further filter results). More information is provided in the   Using the field-level querying functionality   section.</li> <li>By using (optionally multiple times) the <code>--label</code> (or <code>-l</code>) flag, users can   restrict the resources returned by means of the labels set in the resource's   metadata. Labels must be specified in the <code>key=value</code> format. This flag can be   specified multiple times (even in conjunction with <code>-q</code> to further filter   results).</li> <li><code>--with-property | -p</code> can be used to display the values of a subset of the   constitutive properties. Cannot be used when the output format is <code>md</code>.</li> <li><code>--format | -o</code> allows choosing the output format in which the information   should be displayed. Can be one of either:<ul> <li><code>md</code> - for Markdown text.</li> <li><code>table</code> (default) - for Markdown tables.</li> <li><code>csv</code> - for a comma separated file.</li> </ul> </li> </ul>"},{"location":"getting-started/ado/#examples_11","title":"Examples","text":""},{"location":"getting-started/ado/#get-the-summary-of-a-space-as-a-markdown-table","title":"Get the summary of a space as a Markdown table","text":"<pre><code>ado show summary space space-abc123-456def\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-a-space-as-a-markdown-table-and-include-the-constitutive-property-my_property","title":"Get the summary of a space as a Markdown table and include the constitutive property MY_PROPERTY","text":"<pre><code>ado show summary space space-abc123-456def -p MY_PROPERTY\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-multiple-spaces-as-a-markdown-table-via-identifiers","title":"Get the summary of multiple spaces as a Markdown table via identifiers","text":"<pre><code>ado show summary space space-abc123-456def space-ghi789-123jkl\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-multiple-spaces-as-a-markdown-table-via-key-value-labels","title":"Get the summary of multiple spaces as a Markdown table via key-value labels","text":"<pre><code>ado show summary space -l issue=123\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-a-space-as-a-markdown-text","title":"Get the summary of a space as a Markdown text","text":"<pre><code>ado show summary space space-abc123-456def -o md\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-a-multiple-spaces-as-a-csv-file-via-key-value-labels","title":"Get the summary of a multiple spaces as a CSV file via key-value labels","text":"<pre><code>ado show summary space -l issue=123 -o csv\n</code></pre>"},{"location":"getting-started/ado/#get-the-summary-of-spaces-that-include-granite-7b-base-in-the-property-domain","title":"Get the summary of spaces that include granite-7b-base in the property domain","text":"<p>Info</p> <p>More information on field-level querying is provided in the Using the field-level querying functionality section</p> <pre><code>ado show summary space -q 'config.entitySpace={\"propertyDomain\":{\"values\":[\"granite-7b-base\"]}}'\n</code></pre>"},{"location":"getting-started/ado/#ado-template","title":"ado template","text":"<p>To help us in the creation of a resource configuration file, we typically start from a reference file. The <code>ado template</code> command allows you to create template files that you can edit to speed up the process.</p> <p>The complete syntax of the <code>ado template</code> command is as follows:</p> <pre><code>ado template RESOURCE_TYPE [--output | -o &lt;PATH&gt;] \\\n                           [--include-schema] \\\n                           [--operator-name &lt;NAME&gt;] \\\n                           [--operator-type &lt;TYPE&gt;] \\\n                           [--actuator-identifier &lt;NAME&gt;] \\\n                           [--from-experiment | -e &lt;experiment_id | actuator_id:experiment_id&gt;] \\\n                           [--local-context] \\\n                           [--no-parameters-only-schema]\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types:</p> <ul> <li>actuator</li> <li>actuatorconfiguration</li> <li>context</li> <li>operation</li> <li>run</li> <li>space</li> </ul> </li> <li> <p><code>--output</code> or <code>-o</code> can be used to point to a location where to save the   template. By default, the template will be saved in the current directory with   an autogenerated name.</p> </li> <li><code>--include-schema</code>, if set, will also produce the JSON Schema of the resource   the template was generated for.</li> <li><code>--operator-name</code> (exclusive for operations) allows generating an   operation template with a specific operator. If unset, a generic operation   will instead be output.</li> <li> <p><code>--operator-type</code> (exclusive for operations) is the type of operator to   generate a template for. Must be one of the supported operator types:</p> <ul> <li><code>characterize</code></li> <li><code>search</code></li> <li><code>compare</code></li> <li><code>modify</code></li> <li><code>study</code></li> <li><code>fuse</code></li> <li><code>learn</code></li> </ul> </li> <li> <p><code>--actuator-configuration</code> (exclusive for actuatorconfigurations) is the   identifier of the actuator to output. If unset, a generic actuator   configuration will be output.</p> </li> <li><code>--from-experiment</code> (exclusive for spaces) can either be the identifier of   the experiment you want to have in your space or, in case multiple actuators   implementing the same experiment identifier, the identifier of the actuator   and that of the experiment in the form <code>actuator_id:experiment_id</code>. If unset,   a generic space will be output.</li> <li><code>--local-context</code> (exclusive for contexts) creates a template using SQLite   instead of MySQL.</li> <li><code>--no-parameters-only-schema</code> (exclusive for operations) when used with   <code>--include-schema</code>, outputs a generic operation schema. By default (when not   specifying this flag), the schema will be operator-specific.</li> </ul>"},{"location":"getting-started/ado/#examples_12","title":"Examples","text":""},{"location":"getting-started/ado/#creating-a-template-for-a-context","title":"Creating a template for a context","text":"<pre><code>ado template context\n</code></pre>"},{"location":"getting-started/ado/#creating-a-template-for-a-space-that-uses-a-specific-experiment","title":"Creating a template for a space that uses a specific experiment","text":"<pre><code>ado template space --from-experiment finetune-gptq-lora-dp-r-4-a-16-tm-default-v1.1.0\n</code></pre>"},{"location":"getting-started/ado/#creating-a-template-for-a-space-that-uses-a-specific-experiment-from-a-specific-actuator","title":"Creating a template for a space that uses a specific experiment from a specific actuator","text":"<pre><code>ado template space --from-experiment SFTTrainer:finetune-gptq-lora-dp-r-4-a-16-tm-default-v1.1.0\n</code></pre>"},{"location":"getting-started/ado/#creating-a-template-for-a-discovery-space-with-the-schema","title":"Creating a template for a Discovery Space with the schema","text":"<pre><code>ado template space --include-schema\n</code></pre>"},{"location":"getting-started/ado/#creating-an-operation-template-for-the-rifferla-operator","title":"Creating an operation template for the Rifferla operator","text":"<pre><code>ado template operation --operator-name rifferla\n</code></pre>"},{"location":"getting-started/ado/#ado-upgrade","title":"ado upgrade","text":"<p>Tip</p> <p><code>ado</code> will detect automatically when resources need to be upgraded and will print the exact command to run as a warning. In all other cases, there is no need to run this command.</p> <p>Sometimes the models that are used in ado undergo changes that require updating stored representations of them in the metastore. When required, you can run this command to update all resources of a given kind in the database.</p> <pre><code>ado upgrade RESOURCE_TYPE\n</code></pre> <p>Where:</p> <ul> <li> <p><code>RESOURCE_TYPE</code> is one of the supported resource types:</p> <ul> <li>actuatorconfigurations</li> <li>datacontainers</li> <li>operations</li> <li>samplestores</li> <li>spaces</li> </ul> </li> </ul>"},{"location":"getting-started/ado/#examples_13","title":"Examples","text":""},{"location":"getting-started/ado/#upgrade-all-operation-resources","title":"Upgrade all operation resources","text":"<pre><code>ado upgrade operations\n</code></pre>"},{"location":"getting-started/ado/#ado-version","title":"ado version","text":"<p>When unsure about what ado version you are running, you can get this information with:</p> <pre><code>ado version\n</code></pre>"},{"location":"getting-started/ado/#whats-next","title":"What's next","text":"<ul> <li> <p> Let's get started!</p> <p>Jump into our examples</p> <p>Our how-tos </p> </li> <li> <p> Learn more about the built-in Operators</p> <p>Learn what <code>ado</code>'s built-in operators can offer you</p> <p>Follow the guide </p> </li> </ul>"},{"location":"getting-started/contributing/","title":"Contributing","text":""},{"location":"getting-started/contributing/#contributing-in-general","title":"Contributing In General","text":"<p>Our project welcomes external contributions. If you have an itch, please feel free to scratch it.</p> <p>To contribute code or documentation, please submit a pull request.</p> <p>A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker. Before embarking on a more ambitious contribution, please quickly get in touch with us.</p> <p>Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!</p>"},{"location":"getting-started/contributing/#proposing-new-features","title":"Proposing new features","text":"<p>If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.</p>"},{"location":"getting-started/contributing/#fixing-bugs","title":"Fixing bugs","text":"<p>If you would like to fix a bug, please raise an issue before sending a pull request so it can be tracked.</p>"},{"location":"getting-started/contributing/#merge-approval","title":"Merge approval","text":"<p>The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected.</p> <p>For a list of the maintainers, see the MAINTAINERS.md page.</p>"},{"location":"getting-started/contributing/#legal","title":"Legal","text":"<p>Each source file must include a license header for the MIT License. Using the SPDX format is the simplest approach. e.g.</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\n</code></pre> <p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO)</p> <ul> <li>that the Linux\u00ae Kernel community uses to manage code contributions.</li> </ul> <p>We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message.</p> <p>Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>You can include this automatically when you commit a change to your local git repository using the following command:</p> <pre><code>git commit -s\n</code></pre>"},{"location":"getting-started/contributing/#communication","title":"Communication","text":"<p>TBD</p>"},{"location":"getting-started/contributing/#setup","title":"Setup","text":"<p>TBD</p>"},{"location":"getting-started/contributing/#testing","title":"Testing","text":"<p>TBD</p>"},{"location":"getting-started/contributing/#coding-style-guidelines","title":"Coding style guidelines","text":"<p>TBD</p>"},{"location":"getting-started/contributing_readme/","title":"Contributing","text":""},{"location":"getting-started/contributing_readme/#contributing-in-general","title":"Contributing In General","text":"<p>Our project welcomes external contributions. If you have an itch, please feel free to scratch it.</p> <p>To contribute code or documentation, please submit a pull request.</p> <p>A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker. Before embarking on a more ambitious contribution, please quickly get in touch with us.</p> <p>Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!</p>"},{"location":"getting-started/contributing_readme/#proposing-new-features","title":"Proposing new features","text":"<p>If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.</p>"},{"location":"getting-started/contributing_readme/#fixing-bugs","title":"Fixing bugs","text":"<p>If you would like to fix a bug, please raise an issue before sending a pull request so it can be tracked.</p>"},{"location":"getting-started/contributing_readme/#merge-approval","title":"Merge approval","text":"<p>The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected.</p> <p>For a list of the maintainers, see the MAINTAINERS.md page.</p>"},{"location":"getting-started/contributing_readme/#legal","title":"Legal","text":"<p>Each source file must include a license header for the MIT License. Using the SPDX format is the simplest approach. e.g.</p> <pre><code># Copyright (c) IBM Corporation\n# SPDX-License-Identifier: MIT\n</code></pre> <p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO)</p> <ul> <li>that the Linux\u00ae Kernel community uses to manage code contributions.</li> </ul> <p>We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message.</p> <p>Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>You can include this automatically when you commit a change to your local git repository using the following command:</p> <pre><code>git commit -s\n</code></pre>"},{"location":"getting-started/contributing_readme/#communication","title":"Communication","text":"<p>TBD</p>"},{"location":"getting-started/contributing_readme/#setup","title":"Setup","text":"<p>TBD</p>"},{"location":"getting-started/contributing_readme/#testing","title":"Testing","text":"<p>TBD</p>"},{"location":"getting-started/contributing_readme/#coding-style-guidelines","title":"Coding style guidelines","text":"<p>TBD</p>"},{"location":"getting-started/demo/","title":"Demo","text":"<p>The following videos give a glimpse of using ado to benchmark fine-tuning performance across a range of fine-tuning workload configurations.</p>"},{"location":"getting-started/demo/#list-actuators-and-experiments","title":"List actuators and experiments","text":"<p>First we list the experiments given by the <code>SFTTrainer</code> actuator, which provides fine-tuning benchmarking capabilities. We can use <code>ado</code> to get the details of one of the experiments <code>finetune_full_benchmark-v1.0.0</code> and see what it requires as input and what it measures.</p>"},{"location":"getting-started/demo/#create-a-discoveryspace-to-explore-fine-tuning-performance","title":"Create a <code>discoveryspace</code> to explore fine-tuning performance","text":"<p>Next we create a <code>discoveryspace</code> that represents a fine-tuning benchmarking campaign. For quick start we use <code>ado</code>s <code>template</code> functionality to create a default configuration space for <code>lora</code> and <code>full</code> fine-tuning benchmark experiments.</p>"},{"location":"getting-started/demo/#explore-the-discoveryspace-with-a-randomwalk","title":"Explore the <code>discoveryspace</code> with a RandomWalk","text":"<p>This clip shows looking at the available operators and then creating a RandomWalk operation to explore the discovery space created above. The operation is configured to sample all 40 of the configurations, a.k.a. entities, in the <code>discoveryspace</code>. After the operation is finished we can look results at a summary of the operation and get the results as a CSV file.</p>"},{"location":"getting-started/demo/#examine-spaces-collaborators-have-created","title":"Examine spaces collaborators have created","text":"<p><code>ado</code> enables multiple distributed users can work on the same project. Here another user can query the <code>discoveryspaces</code> created by their colleagues, including in this case the one created above. Resources, like <code>discoveryspaces</code>, can be tagged with custom metadata. For example, in this clip the user requests a summary of all spaces tagged with <code>exp=ft</code>. They then apply a custom export operator to the data which in this case integrates new data with an external store in a rigorous and repeatable way.</p>"},{"location":"getting-started/developing/","title":"Developing ado","text":""},{"location":"getting-started/developing/#project-setup","title":"Project Setup","text":"<p>To start developing ado, you need to set up a Python environment. We use <code>uv</code> for project and dependency management.</p>"},{"location":"getting-started/developing/#installing-uv","title":"Installing uv","text":"<p>To install <code>uv</code>, refer to the official installation documentation and choose your preferred method.</p>"},{"location":"getting-started/developing/#creating-a-development-virtual-environment","title":"Creating a development virtual environment","text":"<p>Create a development virtual environment by executing the following commands in the top-level of the <code>ado</code> repository:</p> <pre><code>uv sync\nsource .venv/bin/activate\n</code></pre> <p>Note</p> <p>This installs <code>ado</code> in editable mode.</p> <p>Note</p> <p>In line with uv's defaults, the <code>uv sync</code> command creates a <code>.venv</code> in the top-level of the project's repository. Note that environments created by <code>uv sync</code> are intended only to be used when developing a specific project and should not be shared across projects.</p> <p>Caution</p> <p><code>uv sync</code> ensures a reproducible development environment is created by using a lock-file, <code>uv.lock</code>. Only packages in the lockfile are installed, and other packages found in the virtual environment will be deleted. See Making changes to dependencies for how to add packages to the lockfile.</p> <p>If you want to create your development virtual environment at an alternate location, $PATH, then</p> <pre><code>uv venv $PATH\nsource $PATH/bin/activate\nuv sync --active\n</code></pre> <p>Caution</p> <p>If you create a development in a different location you must direct <code>uv sync</code> explicitly to use it with <code>--active</code> If you do not it will default to using <code>.venv</code> in the project top-level directory.</p>"},{"location":"getting-started/developing/#code-style","title":"Code style","text":"<p>Note</p> <p>See the Automating checks with pre-commit section to automate this.</p> <p>This repository follows the <code>black</code> style for formatting.</p> <p>You can format your code by:</p> <ul> <li>Manually running <code>black &lt;the folder containing files to format&gt;</code></li> <li>Setting up PyCharm to use the <code>black</code> integration:   https://www.jetbrains.com/help/pycharm/reformat-and-rearrange-code.html#format-python-code-with-black</li> <li>Using the   \"Black formatter\" extension for VSCode   and setting it as the default formatter:   https://code.visualstudio.com/docs/python/formatting#_set-a-default-formatter</li> </ul>"},{"location":"getting-started/developing/#linting-with-ruff","title":"Linting with ruff","text":"<p>Note</p> <p>See the Automating checks with pre-commit section to automate this.</p> <p>This repository uses <code>ruff</code> to enforce linting rules. Install it using one of the methods described in the official <code>ruff</code> documentation. To run linting checks, execute:</p> <pre><code>ruff check --exclude website\n</code></pre>"},{"location":"getting-started/developing/#linting-markdown-with-markdownlint-cli2","title":"Linting markdown with markdownlint-cli2","text":"<p>Note</p> <p>See the Automating checks with pre-commit section to automate this.</p> <p>This repository uses <code>markdownlint-cli2</code> to enforce linting rules on markdown files. Install it using one of the methods described in the official documentation. To run linting checks, execute:</p> <pre><code>markdownlint-cli2 \"**/*.md\" \"#.venv\" --fix\n</code></pre>"},{"location":"getting-started/developing/#prettier-for-lines-too-long","title":"Prettier for lines too long","text":"<p>Note</p> <p>Prettier might undo some changes that markdownlint-cli2 has done. A common error is adding a line after the <code>markdownlint-disable-next-line</code> comments</p> <p>Note that line-too-long errors won't be automatically fixed. We recommend using <code>prettier</code> to autoformat markdown in that case. Installation instructions can be found on the official website</p> <p>Prettier can then be run with:</p> <pre><code>prettier -w \"**/*.md\"\n</code></pre>"},{"location":"getting-started/developing/#secret-scanning","title":"Secret scanning","text":"<p>Note</p> <p>See the Automating checks with pre-commit section to automate this.</p> <p>This repository uses IBM's detect-secrets to scan for secrets before the code is pushed to GitHub. Follow installation instructions in their repository: https://github.com/ibm/detect-secrets?tab=readme-ov-file#example-usage</p> <p>To update the secrets database manually, run:</p> <pre><code>detect-secrets scan --update .secrets.baseline\n</code></pre> <p>To audit detected secrets, use:</p> <pre><code>detect-secrets audit .secrets.baseline\n</code></pre>"},{"location":"getting-started/developing/#commit-style","title":"Commit style","text":"<p>We require commit messages to use the conventional commit style.</p> <p>Conventional Commit messages follow the following pattern (NOTE: the scope is optional):</p> <pre><code>type(scope): subject\n\nextended body\n</code></pre> <p>Where type is one of: build, chore, ci, docs, feat, fix, perf, refactor, revert, style, test.</p>"},{"location":"getting-started/developing/#copyright-and-license-headers","title":"Copyright and license headers","text":"<p>We require copyright and SPDX license headers to be added to the source code. This can be automated by using Hashicorp's Copywrite tool: https://github.com/hashicorp/copywrite</p> <p>Once installed, run</p> <pre><code>copywrite headers\n</code></pre>"},{"location":"getting-started/developing/#automating-checks-with-pre-commit","title":"Automating checks with pre-commit","text":"<p>To automate the checks for code style, linting, and security, you can utilize the provided pre-commit hooks.</p>"},{"location":"getting-started/developing/#installing-the-hooks","title":"Installing the hooks","text":"<p>Important</p> <p>Before installing the hooks, make sure you have the following prerequisites:</p> <ul> <li>A developer virtual environment created with uv</li> <li>A recent version of NodeJS</li> <li>On MacOS we suggest     installing it via brew     for ease of use</li> </ul> <pre><code>pre-commit install\n</code></pre> <p>This command will configure pre-commit to run automatically before each commit, highlighting any issues and preventing the commit if problems are found.</p>"},{"location":"getting-started/developing/#handling-pre-commit-failures","title":"Handling pre-commit failures","text":"<ol> <li>Black code formatting failures: try committing again, <code>black</code> might have    reformatted your code in-place.</li> </ol> <ul> <li>If black fails to format your code, your files have syntax errors.      Try manually running black.</li> </ul> <ol> <li>Ruff linter failures: run <code>ruff</code> as specified in    Linting with ruff and fix the code that is causing the    failures.</li> </ol> <ul> <li>In case of false positives, you might need to add <code>#noqa</code> annotations.</li> <li>If your local ruff installation does not detect any failure you may be      using an old version that needs updating.</li> </ul> <ol> <li>Detect secrets failures: include <code>.secrets.baseline</code> in your commit, it    was updated by the pre-commit hook.</li> </ol> <ul> <li>If secrets are detected, audit them as specified in      Secret scanning.</li> </ul> <ol> <li>Commit style failures: change your commit message to match conventional    commits. See Commit style for more in-depth information.</li> <li>Misspellings detected by codespell: fix the misspellings reported or    add an inline ignore comment.</li> <li>uv export failures: commit the updated <code>requirements.txt</code> file. It has    been updated following changes to the lock file.</li> <li>Markdown linter failures: <code>markdownlint-cli2</code> usually fixes most issues    automatically. If you review its error message and still don\u2019t see a clear    explanation or solution, try recommitting your changes and let the tool re-run.</li> </ol>"},{"location":"getting-started/developing/#making-changes-to-dependencies","title":"Making changes to dependencies","text":"<p>As mentioned in Project Setup, we use <code>uv</code> to manage dependencies. This means that all changes to dependencies must be done via <code>uv</code>, and not by manually editing <code>pyproject.toml</code>.</p> <p>The relevant documentation on <code>uv</code>'s website is available here , but at a glance:</p>"},{"location":"getting-started/developing/#adding-base-dependencies","title":"Adding base dependencies","text":"<p>If you are adding (or updating) base dependencies for <code>ado</code>, you should use the <code>uv add</code> command:</p> <p>Note</p> <p>You can optionally add specific version selectors. By default, <code>uv</code> will add <code>&gt;=CURRENT_VERSION</code>.</p> <pre><code>uv add pydantic\n</code></pre>"},{"location":"getting-started/developing/#adding-optional-dependencies","title":"Adding optional dependencies","text":"<p>Dependencies may be optional, making them available only when using extras, such as <code>ado-core[my-extra]</code>. To add these kind of dependencies, use the <code>uv add --optional</code> command:</p> <pre><code>uv add --optional validation pydantic\n</code></pre>"},{"location":"getting-started/developing/#adding-dependency-groups","title":"Adding dependency groups","text":"<p>Sometimes we might want to include dependencies that have a specific purpose, like testing the code, linting it, or building the documentation. This is a perfect use case for dependency groups, sets of dependencies that do not get published to indices like PyPI and are not installed with ado. A noteworthy dependency group is the <code>dev</code> group, which <code>uv</code> installs by default when syncing dependencies.</p> <p>Users are highly encouraged to read the documentation available both on uv's and Python's website:</p> <ul> <li>https://docs.astral.sh/uv/concepts/projects/dependencies/#development-dependencies</li> <li>https://docs.astral.sh/uv/concepts/projects/dependencies/#dependency-groups</li> <li>https://packaging.python.org/en/latest/specifications/dependency-groups</li> </ul> <p>With <code>uv</code> you can add dependencies to groups using <code>uv add --group NAME</code>:</p> <p>Note</p> <p>For the <code>dev</code> group there is the shorthand <code>--dev</code> that replaces <code>--group dev</code>.</p> <pre><code>uv add --group dev pytest\n</code></pre>"},{"location":"getting-started/install/","title":"Installing <code>ado</code>","text":"<p>ado can be installed in one of three ways:</p> <ol> <li>From PyPi</li> <li>From GitHub</li> <li>By cloning the GitHub repository locally</li> </ol> Warning <p>Before proceeding ensure you are using a supported Python version: run <code>python --version</code> in your terminal and check that you are on either Python 3.10, 3.11, or 3.12.</p> <p>It is also highly recommended to create a virtual environment for ado, to avoid dependency conflicts with other packages. You can do so with:</p> <pre><code>python -m venv ado-venv\n</code></pre> <p>And activate it with</p> <pre><code>source ado-venv/bin/activate\n</code></pre> From PyPiFrom GitHubCloning the repo locally <p>This method installs the <code>ado-core</code> package from PyPi</p> <pre><code>pip install ado-core\n</code></pre> <pre><code>pip install git+https://github.com/IBM/ado.git\n</code></pre> <pre><code>git clone https://github.com/IBM/ado.git\ncd ado\npip install .\n</code></pre>"},{"location":"getting-started/install/#installing-plugins","title":"Installing plugins","text":"<p>ado uses a plugin system to provide additional actuators and operators. We maintain a set of actuators and operators in the ado main repo. Some plugins may also have packages on PyPi. You can install these actuators as follows:</p> <p>Info</p> <p>Some plugins may have dependencies that may require credentials to access. Check the plugins's docs if you encounter issues installing a specific actuator.</p> From PyPiFrom GitHubCloning the repo <p>The following plugin packages are available: <code>ado-sfttrainer</code>, <code>ado-vllm-performance</code>, and <code>ado-ray-tune</code></p> <pre><code>pip install $PLUGIN_NAME\n</code></pre> <p>For actuators</p> <pre><code>pip install \"git+https://github.com/IBM/ado.git#subdirectory=plugins/actuators/$ACTUATOR_NAME\"\n</code></pre> <p>For operators</p> <pre><code>pip install \"git+https://github.com/IBM/ado.git#subdirectory=plugins/operators/$OPERATOR_NAME\"\n</code></pre> <p>If you've cloned the ado repository locally in the previous step, you can run from the top-level of the cloned repository</p> <pre><code>pip install plugins/actuators/$ACTUATOR_NAME\n</code></pre> <p>or</p> <pre><code>pip install plugins/operators/$OPERATOR_NAME\n</code></pre>"},{"location":"getting-started/install/#whats-next","title":"What's next","text":"<ul> <li> <p> Let's get started!</p> <p>Learn what you can do with <code>ado</code></p> <p>Follow the guide </p> </li> <li> <p> Collaborate with others</p> <p>Learn how to install the components that allow you to collaborate with others.</p> <p>Installing the Backend Services </p> </li> </ul>"},{"location":"getting-started/installing-backend-services/","title":"Installing backend services","text":""},{"location":"getting-started/installing-backend-services/#using-the-distributed-mysql-backend-for-ado","title":"Using the distributed MySQL backend for ado","text":"<p>This guide is intended for administrators who are responsible for deploying the distributed MySQL backend for ADO or provisioning new projects on it.</p>"},{"location":"getting-started/installing-backend-services/#overview","title":"Overview","text":"<p>We recommend using the Percona Operator for MySQL, which is built on Percona XtraDB Cluster, to provide a resilient and production-ready MySQL backend. This guide assumes that this setup is being used.</p>"},{"location":"getting-started/installing-backend-services/#deployment-instructions","title":"Deployment Instructions","text":""},{"location":"getting-started/installing-backend-services/#kubernetes","title":"Kubernetes","text":"<p>You can deploy the Percona Operator and create a Percona XtraDB Cluster using either of the following methods:</p> <ul> <li>Helm charts</li> <li>Kubernetes manifest files</li> </ul> <p>Click on the links to follow the official Percona documentation.</p>"},{"location":"getting-started/installing-backend-services/#openshift","title":"OpenShift","text":"<p>In OpenShift environments, the operator can be installed via OperatorHub using the Operator Lifecycle Manager (OLM).</p> <p>Refer to the official OpenShift-specific guide here:</p> <p>\ud83d\udc49 OpenShift Deployment Guide</p>"},{"location":"getting-started/installing-backend-services/#onboarding-projects","title":"Onboarding projects","text":"<p>Warning</p> <p>Before proceeding make sure you have followed the steps in Deployment Instructions.</p>"},{"location":"getting-started/installing-backend-services/#pre-requisites","title":"Pre-requisites","text":""},{"location":"getting-started/installing-backend-services/#software","title":"Software","text":"<p>To run the scripts in this guide you will need to have the following tools installed:</p> <ul> <li><code>kubectl</code>: https://kubernetes.io/docs/tasks/tools/#kubectl</li> <li><code>mysql</code> client version 8: https://formulae.brew.sh/formula/mysql-client@8.4</li> </ul>"},{"location":"getting-started/installing-backend-services/#pxc-related-variables","title":"PXC-related variables","text":"<p>Note</p> <p>We assume that your active namespace is the one in which you installed your Percona XtraDB Cluster.</p>"},{"location":"getting-started/installing-backend-services/#pxc-cluster-name","title":"PXC Cluster name","text":"<p>You will need to know the name of your <code>pxc</code> cluster:</p> <pre><code>kubectl get pxc -o jsonpath='{.items[].metadata.name}'\n</code></pre> <p>We will refer to its name as <code>$PXC_NAME</code>.</p>"},{"location":"getting-started/installing-backend-services/#pxc-cluster-root-credentials","title":"PXC Cluster root credentials","text":"<p>You will need a highly privileged account to onboard new projects, as you will need to create databases, users, and grant permissions. For this reason, we will use the default <code>root</code> account.</p> <p>You can retrieve its password with:</p> <pre><code>kubectl get secret $PXC_NAME-secrets --template='{{.data.root}}' | base64 -d\n</code></pre> <p>We will refer to this password as <code>$MYSQL_ADMIN_PASSWORD</code>.</p>"},{"location":"getting-started/installing-backend-services/#onboarding-new-projects","title":"Onboarding new projects","text":"<p>The simplest way to onboard a new project called <code>$PROJECT_NAME</code> is to use the <code>forward_mysql_and_onboard_new_project.sh</code>. This script creates a new project in the MySQL DB and outputs an ado context YAML that can be used to connect to it.</p> <p>For example:</p> <pre><code>./forward_mysql_and_onboard_new_project.sh --admin-user root \\\n                      --admin-pass $MYSQL_ADMIN_PASSWORD \\\n                      --pxc-name $PXC_NAME \\\n                      --project-name $PROJECT_NAME\n</code></pre> <p>Alternatively, if you are using a hosted MySQL instance somewhere (e.g., on the Cloud), you can use the other script: <code>onboard_new_project.sh</code>:</p> <pre><code>./onboard_new_project.sh --admin-user root \\\n                      --admin-pass $MYSQL_ADMIN_PASSWORD \\\n                      --mysql-endpoint $MYSQL_ENDPOINT \\\n                      --project-name $PROJECT_NAME\n</code></pre> <p>Once the project is created the context YAML can be shared with whoever needs access to the project.</p>"},{"location":"getting-started/installing-backend-services/#deploying-kuberay-and-creating-a-raycluster","title":"Deploying Kuberay and creating a RayCluster","text":"<p>This guide is intended for users who want to run operations on an autoscaling ray cluster deployed on kubernetes/OpenShift. Depending on cluster permissions users may need someone with administrator privileges to install KubeRay and/or create RayCluster objects.</p>"},{"location":"getting-started/installing-backend-services/#installing-kuberay","title":"Installing KubeRay","text":"<p>Warning</p> <p>KubeRay is included in OpenShift AI and OpenDataHub. Skip this step if they are already installed in your cluster.</p> <p>You can install the KubeRay Operator either via Helm or Kustomize by following the official documentation.</p>"},{"location":"getting-started/installing-backend-services/#deploying-a-raycluster","title":"Deploying a RayCluster","text":"<p>Warning</p> <p>The <code>ray</code> versions must be compatible. For a more in depth guide refer to the RayCluster configuration page.</p> <p>Note</p> <p>When running multi-node measurement make sure that all nodes in your multi-node setup have read and write access to your HuggingFace home directory. On Kubernetes with RayCluster, avoid S3-like filesystems as that is known to cause failures in transformers. Use a NFS or GPFS-backed PersistentVolumeClaim instead.</p>"},{"location":"getting-started/installing-backend-services/#best-practices-for-efficient-gpu-resource-utilization","title":"Best Practices for Efficient GPU Resource Utilization","text":"<p>To maximize the efficiency of your RayCluster and minimize GPU resource fragmentation, we recommend the following:</p> <ul> <li> <p>Enable Ray Autoscaler   This allows Ray to dynamically adjust the number of worker replicas based on   task demand.</p> </li> <li> <p>Use Multiple GPU Worker Variants   Define several GPU worker types with varying GPU counts. This flexibility   helps match task requirements more precisely and reduces idle GPU time.</p> </li> </ul>"},{"location":"getting-started/installing-backend-services/#recommended-worker-configuration-strategy","title":"Recommended Worker Configuration Strategy","text":"<p>Create GPU worker variants with increasing GPU counts, where each variant has double the GPUs of the previous one. Limit each variant to a maximum of 2 replicas, ensuring that their combined GPU usage does not exceed the capacity of a single replica of the next larger variant.</p>"},{"location":"getting-started/installing-backend-services/#example-kubernetes-cluster-with-4-nodes-8-gpus-each","title":"Example: Kubernetes Cluster with 4 Nodes (8 GPUs Each)","text":"<p>Recommended worker setup:</p> <ul> <li>2 replicas of a worker with 1 GPU</li> <li>2 replicas of a worker with 2 GPUs</li> <li>2 replicas of a worker with 4 GPUs</li> <li>4 replicas of a worker with 8 GPUs</li> </ul>  Example: The contents of the additionalWorkerGroups field of a RayCluster with 4 Nodes each with 8 NVIDIA-A100-SXM4-80GB GPUs, 64 CPU cores, and 1TB memory  <pre><code>one-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '1'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 1}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 8\n      nvidia.com/gpu: 1\n      memory: 100Gi\n    requests:\n      cpu: 8\n      nvidia.com/gpu: 1\n      memory: 100Gi\n  # volumes: ...\n  # volumeMounts: ....\n\ntwo-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '2'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 2}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 15\n      nvidia.com/gpu: 2\n      memory: 200Gi\n    requests:\n      cpu: 15\n      nvidia.com/gpu: 2\n      memory: 200Gi\n  # volumes: ...\n  # volumeMounts: ....\n\nfour-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '4'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 4}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 30\n      nvidia.com/gpu: 4\n      memory: 400Gi\n    requests:\n      cpu: 30\n      nvidia.com/gpu: 4\n      memory: 400Gi\n  # volumes: ...\n  # volumeMounts: ....\n\neight-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 4\n  rayStartParams:\n    block: 'true'\n    num-gpus: '8'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 8, \\\"full-worker\\\": 1}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n\n  resources:\n    limits:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 800Gi\n    requests:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 800Gi\n  # volumes: ...\n  # volumeMounts: ....\n</code></pre> <p>Note</p> <p>Notice that the only variant with a full-worker custom resource  is the one with 8 GPUs. Some actuators, like SFTTrainer, use this  custom resource for measurements that involve reserving an entire GPU node.</p> <p>We provide an example set of values for deploying a RayCluster via KubeRay. To deploy it, simply run:</p> <pre><code>helm upgrade --install ado-ray kuberay/ray-cluster --version 1.1.0 --values backend/kuberay/vanilla-ray.yaml\n</code></pre> <p>Feel free to customize it to suit your cluster, such as uncommenting GPU-enabled workers.</p>"},{"location":"getting-started/kuberay/","title":"Deploying Kuberay and creating a RayCluster","text":"<p>This guide is intended for users who want to run operations on an autoscaling ray cluster deployed on kubernetes/OpenShift. Depending on cluster permissions users may need someone with administrator privileges to install KubeRay and/or create RayCluster objects.</p>"},{"location":"getting-started/kuberay/#installing-kuberay","title":"Installing KubeRay","text":"<p>Warning</p> <p>KubeRay is included in OpenShift AI and OpenDataHub. Skip this step if they are already installed in your cluster.</p> <p>You can install the KubeRay Operator either via Helm or Kustomize by following the official documentation.</p>"},{"location":"getting-started/kuberay/#deploying-a-raycluster","title":"Deploying a RayCluster","text":"<p>Warning</p> <p>The <code>ray</code> versions must be compatible. For a more in depth guide refer to the RayCluster configuration page.</p> <p>Note</p> <p>When running multi-node measurement make sure that all nodes in your multi-node setup have read and write access to your HuggingFace home directory. On Kubernetes with RayCluster, avoid S3-like filesystems as that is known to cause failures in transformers. Use a NFS or GPFS-backed PersistentVolumeClaim instead.</p>"},{"location":"getting-started/kuberay/#best-practices-for-efficient-gpu-resource-utilization","title":"Best Practices for Efficient GPU Resource Utilization","text":"<p>To maximize the efficiency of your RayCluster and minimize GPU resource fragmentation, we recommend the following:</p> <ul> <li> <p>Enable Ray Autoscaler   This allows Ray to dynamically adjust the number of worker replicas based on   task demand.</p> </li> <li> <p>Use Multiple GPU Worker Variants   Define several GPU worker types with varying GPU counts. This flexibility   helps match task requirements more precisely and reduces idle GPU time.</p> </li> </ul>"},{"location":"getting-started/kuberay/#recommended-worker-configuration-strategy","title":"Recommended Worker Configuration Strategy","text":"<p>Create GPU worker variants with increasing GPU counts, where each variant has double the GPUs of the previous one. Limit each variant to a maximum of 2 replicas, ensuring that their combined GPU usage does not exceed the capacity of a single replica of the next larger variant.</p>"},{"location":"getting-started/kuberay/#example-kubernetes-cluster-with-4-nodes-8-gpus-each","title":"Example: Kubernetes Cluster with 4 Nodes (8 GPUs Each)","text":"<p>Recommended worker setup:</p> <ul> <li>2 replicas of a worker with 1 GPU</li> <li>2 replicas of a worker with 2 GPUs</li> <li>2 replicas of a worker with 4 GPUs</li> <li>4 replicas of a worker with 8 GPUs</li> </ul>  Example: The contents of the additionalWorkerGroups field of a RayCluster with 4 Nodes each with 8 NVIDIA-A100-SXM4-80GB GPUs, 64 CPU cores, and 1TB memory  <pre><code>one-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '1'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 1}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 8\n      nvidia.com/gpu: 1\n      memory: 100Gi\n    requests:\n      cpu: 8\n      nvidia.com/gpu: 1\n      memory: 100Gi\n  # volumes: ...\n  # volumeMounts: ....\n\ntwo-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '2'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 2}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 15\n      nvidia.com/gpu: 2\n      memory: 200Gi\n    requests:\n      cpu: 15\n      nvidia.com/gpu: 2\n      memory: 200Gi\n  # volumes: ...\n  # volumeMounts: ....\n\nfour-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 2\n  rayStartParams:\n    block: 'true'\n    num-gpus: '4'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 4}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n  resources:\n    limits:\n      cpu: 30\n      nvidia.com/gpu: 4\n      memory: 400Gi\n    requests:\n      cpu: 30\n      nvidia.com/gpu: 4\n      memory: 400Gi\n  # volumes: ...\n  # volumeMounts: ....\n\neight-A100-80G-gpu-WG:\n  replicas: 0\n  minReplicas: 0\n  maxReplicas: 4\n  rayStartParams:\n    block: 'true'\n    num-gpus: '8'\n    resources: '\"{\\\"NVIDIA-A100-SXM4-80GB\\\": 8, \\\"full-worker\\\": 1}\"'\n  containerEnv:\n    - name: OMP_NUM_THREADS\n      value: \"1\"\n    - name: OPENBLAS_NUM_THREADS\n      value: \"1\"\n  lifecycle:\n    preStop:\n      exec:\n        command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\n  # securityContext: ...\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: nvidia.com/gpu.product\n                operator: In\n                values:\n                  - NVIDIA-A100-SXM4-80GB\n\n  resources:\n    limits:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 800Gi\n    requests:\n      cpu: 60\n      nvidia.com/gpu: 8\n      memory: 800Gi\n  # volumes: ...\n  # volumeMounts: ....\n</code></pre> <p>Note</p> <p>Notice that the only variant with a full-worker custom resource  is the one with 8 GPUs. Some actuators, like SFTTrainer, use this  custom resource for measurements that involve reserving an entire GPU node.</p> <p>We provide an example set of values for deploying a RayCluster via KubeRay. To deploy it, simply run:</p> <pre><code>helm upgrade --install ado-ray kuberay/ray-cluster --version 1.1.0 --values backend/kuberay/vanilla-ray.yaml\n</code></pre> <p>Feel free to customize it to suit your cluster, such as uncommenting GPU-enabled workers.</p>"},{"location":"getting-started/remote_run/","title":"Running ado on remote ray clusters","text":"<p>Info</p> <p>It's recommended to read the documentation on working with operators and operation resources before trying to run <code>ado</code> remotely.</p> <p>The following sections explain how to run an <code>ado</code> operation on a remote ray cluster.</p>"},{"location":"getting-started/remote_run/#why-run-ado-on-a-remote-ray-cluster","title":"Why run <code>ado</code> on a remote ray cluster?","text":"<p>ado operations, normally started with ado create operation, can benefit from running on, or even require running on, a remote ray cluster.</p> <p>This is because certain operations require (or can use) more compute resource than is available on your laptop. This is usually because one or both of the following is true:</p> <ul> <li>the operation can leverage the fact that <code>ado</code> is built on <code>Ray</code> to run large   numbers of distributed tasks</li> <li>the operation requires access to (large amounts) of compute resources, like   GPUs.</li> </ul> <p>Often it is explore operations that require remote execution, due to the requirements of the actuators performing the measurements in the explore process. However, any <code>ado</code> operation can be run on a remote ray cluster.</p>"},{"location":"getting-started/remote_run/#running-ado-remotely-versus-using-an-actuator-that-can-run-remotely","title":"Running <code>ado</code> remotely versus using an actuator that can run remotely","text":"<p>Some actuators can spawn jobs on remote clusters, which may be remote ray clusters. This is different to running <code>ado</code> remotely as in this case the <code>ado</code> main process is still running on your laptop, and if you close your laptop the process will stop.</p> <p>When running ado remotely the <code>ado</code> main process runs on the remote cluster.</p>"},{"location":"getting-started/remote_run/#getting-ready","title":"Getting ready","text":"<p>First, create an empty directory to contain everything that needs to be uploaded to the remote ray cluster for the <code>ado</code> command to run. In the following we will refer to this directory as <code>$RAY_JOB_DATA_DIR</code>.</p>"},{"location":"getting-started/remote_run/#installing-ado-and-required-plugins-on-a-remote-ray-cluster-from-source","title":"Installing <code>ado</code> and required plugins on a remote ray cluster (from source)","text":"<p>If <code>ado</code>, or the plugins that the particular <code>ado</code> command, are not present on the remote ray cluster, then they can be installed as part of the ray job submission. The simplest approach is to do this from source. For this you need to have cloned the ado repository and cloned the plugin repositories(if they are not in the <code>ado</code> repository).</p> <p>Then there are two steps:</p> <ol> <li>Build python wheels for <code>ado</code> and any required plugins</li> <li>Tell ray to install the wheels as part of ray-job submission</li> </ol>"},{"location":"getting-started/remote_run/#building-the-python-wheels","title":"Building the python wheels","text":"<p>Info</p> <p>You only need to repeat this step if the source code changes between ray jobs and you want to include the changes.</p> Build <code>ado</code> wheelBuild the plugin wheels <p>In the top-level of the <code>ado</code> repository:</p> <pre><code>rm -rf dist/ build/\npython -m build -w\nmv dist/*.whl `$RAY_JOB_DATA_DIR\n</code></pre> <ul> <li>First command removes any previous build artifacts and wheels. This prevents   issues with old files being included in the new wheel</li> <li>Second command creates a <code>dist/</code> directory with the wheel. It will have a name   like <code>ado_core-$VERSION-py3-none.whl</code></li> <li>Last command copies the wheel to the directory you made</li> </ul> <p>In the top-level of the plugins repository e.g. in one of the orchestrator repositories' <code>plugins/actuators/$ACTUATOR</code> directories, execute:</p> <pre><code>rm -rf dist/ build/\npython -m build -w\nmv dist/*.whl `$RAY_JOB_DATA_DIR\n</code></pre> <ul> <li>First command removes any previous build artifacts and wheels. This prevents   issues with old files being included in the new wheel</li> <li>Second command creates a <code>dist/</code> directory with the wheel. It will have a name   like <code>$pluginname-$VERSION-py3-none.whl</code></li> <li>Last command copies the wheel to the directory you made</li> </ul>"},{"location":"getting-started/remote_run/#configuring-installation-of-the-wheels","title":"Configuring installation of the wheels","text":"<p>Once you have built the required wheels, change directory to <code>$RAY_JOB_DATA_DIR</code>.</p> <p>Now, create a <code>ray_runtime_env.yaml</code> file that will direct ray to install the wheels when the job is submitted. The section of this file relevant to wheel installation is under the <code>pip</code> field which is a list of packages for pip to install. In our case it is the wheels we just created e.g.</p> <pre><code>pip: # One line for each wheel to install, in this example there is two. Be sure to check spelling.\n  - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/$ADO_CORE.whl\n  - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/$ADO_PLUGIN1.whl\nenv_vars: # See below\n  ...\n</code></pre> <p>See ray runtime environment for more information on this file.</p> <p>Warning</p> <p>Do not delete or substitute the string ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR} when specifying the wheel names. It is required before every wheel you want to upload and if it is changed the wheel installation will fail.</p>"},{"location":"getting-started/remote_run/#submitting-the-ado-operation","title":"Submitting the <code>ado</code> operation","text":"<p>Once you have completed the previous section you need to create the <code>ado</code> input files in <code>$RAY_JOB_DATA_DIR</code> (or copy them there).</p> <p>This will be usually the following two YAML files:</p> <ul> <li>A YAML file describing the context to use for the   operation.</li> <li>A YAML file describing the operation to create.</li> </ul> <p>Here we will call the first <code>context.yaml</code> and the second <code>operation.yaml</code> although they can have any names.</p> <p>Then the execution command will look like:</p> <pre><code>ray job submit --no-wait --address http://localhost:8265  --working-dir . --runtime-env ray_runtime.yaml -v -- \\\n  ado -c context.yaml create operation -f operation.yaml\n</code></pre> <p>The following sections describe the different flags and values in this command line</p>"},{"location":"getting-started/remote_run/#specifying-the-remote-ray-cluster-to-submit-to-address","title":"Specifying the remote ray cluster to submit to: <code>--address</code>","text":"<p>To submit a job to remote ray cluster you need the address (URL) of its dashboard. If the ray cluster is running on kubernetes or OpenShift you will likely need to connect your laptop to this URL via a \"port-forward\".</p> <p>For example with OpenShift you can do this with the following <code>oc</code> command in a terminal other that the one you will submit the job from:</p> <pre><code>oc port-forward --namespace $NAMESPACE svc/$RAY_SERVICE_NAME 8265\n</code></pre> <p>You will need to find out the name of the $NAMESPACE and the $RAY_SERVICE_NAME from the administrator of the OpenShift cluster/the namespace. Once started the ray cluster address will be <code>http://localhost:8265</code></p> <p>The port-forward command keeps running unless you kill it, or it dies due to inactivity. Once it stops you will not be able to get to the ray cluster until it is restarted.</p> <p>Important</p> <p><code>ray job submit</code> communicates to the ray cluster using different protocols via the given URL. This means if only http is allowed to be sent to the URL <code>ray job submit</code> will not work. This is usually why you need a port-forward compared to, say, an OpenShift route.</p> <p>Note</p> <p>You can navigate to the dashboard of the remote ray cluster by pasting the URL into your browser. From there you can see running jobs, browse the logs of your job, see its workers etc. You may also be able to reach this dashboard by a different URL that doesn't require port-forward to access.</p>"},{"location":"getting-started/remote_run/#ray-runtime-environment-runtime-env","title":"ray runtime environment: <code>runtime-env</code>","text":"<p>The environment of the ray job is given in a YAML file. An example is:</p> <pre><code>pip: # One line for each wheel to install, in this example there is two. Be sure to check spelling.\n  - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/$ADO_CORE.whl\n  - ${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/$ADO_PLUGIN1.whl\nenv_vars: # These envars are recommend. Some plugins may require others. Check plugin docs.\n  PYTHONUNBUFFERED: \"x\" # Turns of buffering of the jobs logs. Useful if there is some error\n  OMP_NUM_THREADS: \"1\" # Restricts the number of threads started by the python process in the job. If this is not set it can cause the ray job to exceed OpenShift node thread limits.\n  OPENBLAS_NUM_THREADS: \"1\" # Same as above\n  RAY_AIR_NEW_PERSISTENCE_MODE: \"0\" # Required for using the ray_tune operator\n  #The following envars may be required or useful depending\n  HOME: \"/tmp\" # Optional: Use if python code used by operation assumes $HOME is writable which it may not be\n  LOGLEVEL: \"WARNING\" # Optional: Set this to get more/less debug logs from ado\n</code></pre> <p>For further details on what you can configure via this file see the ray documentation.</p>"},{"location":"getting-started/remote_run/#other-options","title":"Other options","text":""},{"location":"getting-started/remote_run/#-no-wait","title":"<code>--no-wait</code>","text":"<p>If specified <code>ray job submit</code> immediately disconnects from the remote job. Otherwise, it stays connected until the job finishes.</p> <p>If you want the job to keep running when you close your laptop, or be immune to the port-forward deactivating, use this option.</p>"},{"location":"getting-started/remote_run/#-working-dr","title":"<code>--working-dr</code>","text":"<p>Use this to specify the data to copy over with the ray job. Everything in this directory and subdirectories is copied over and the ray job started in it. Here this is the <code>$RAY_JOB_DATA_DIR</code>.</p>"},{"location":"getting-started/roadmap/","title":"Project Roadmap","text":""},{"location":"getting-started/roadmap/#overview","title":"\ud83d\udcc5 Overview","text":"<p>The ado roadmap outlines the planned direction and milestones for the next major versions of the project. This is a living document that will be updated regularly as the project evolves.</p>"},{"location":"getting-started/roadmap/#key-goals","title":"\ud83d\ude80 Key Goals","text":"<ul> <li>Production Ready: Ensure the project robustly executes all core operators,   actuators and examples.</li> <li>Performant and Human-Centred CLI: A CLI that is responsive and provides   helpful feedback on error</li> <li>Seamless Scaling: Make it easy to scale from single-person working on   their laptop, to a distributed team on executing on remote infrastructure.</li> <li>Community &amp; Ecosystem Development: Respond to user needs and empower   developers to extend <code>ado</code></li> </ul>"},{"location":"getting-started/roadmap/#milestones","title":"\ud83d\udcc6 Milestones","text":""},{"location":"getting-started/roadmap/#q3-2025","title":"Q3 2025","text":""},{"location":"getting-started/roadmap/#version-100-initial-release","title":"Version 1.0.0 (Initial Release)","text":"<ul> <li>Complete core feature set</li> <li>Production actuator for fine-tuning performance measurement</li> <li>Reference actuators for inference performance measurement and model evaluation</li> <li>Focus on bug fixing, stability, and documentation</li> <li>Initial performance optimizations</li> </ul>"},{"location":"getting-started/roadmap/#q4-2025","title":"Q4 2025","text":"<ul> <li>Property grouping and conditional properties</li> <li>Actuator and operator for building and applying predictive performance models</li> <li>REST API via <code>ado serve</code></li> </ul>"},{"location":"getting-started/roadmap/#q1-2026","title":"Q1 2026","text":"<ul> <li>Improved features for interacting (creating and running on) remote ray   clusters</li> <li><code>ado create operation --target=$REMOTE_CLUSTER</code> can spin up a RayCluster,     based on selected actuators needs, install code, start operation etc.</li> <li>Database performance improvements</li> <li>Agentic extensions - use natural language to create spaces and operations.</li> </ul>"},{"location":"getting-started/roadmap/#how-you-can-help","title":"\ud83d\udcac How You Can Help","text":"<ul> <li>Contribute: Submit pull requests for new features, bug fixes, or   documentation improvements.</li> <li>Open Issues: Report bugs, request features, or provide feedback.</li> <li>Spread the Word: Share the project with others who could benefit</li> </ul>"},{"location":"operators/creating-operators/","title":"Extending `ado` with new Operators","text":"<p>Info</p> <p>A complete example operator is provided here. This example operator is functional, and useful, out-of-the-box. It can be used as the basis to create new operators. It references this document to help tie details here to implementation.</p> <p>Developers can write their own operator plugins to add new operations that work on <code>discoveryspaces</code> to <code>ado</code>. Operator plugins are written in python and can live in their own repository.</p> <p>The main part of writing an operator plugin, from an integration standpoint, is writing a python function that implements a specific interface. <code>ado</code> will call this function to execute an operation with your operator. From this function you then call your operator logic (or in many cases it can just live in this function).</p> <p>This page gives an overview of how to get started creating your own operator. After reading this page the best resource is to check our example operator.</p>"},{"location":"operators/creating-operators/#knowledge-required","title":"Knowledge required","text":"<ul> <li>Knowledge of python</li> <li>Knowledge of pydantic is useful, but not   necessary</li> </ul>"},{"location":"operators/creating-operators/#ado-operator-functions","title":"<code>ado</code> operator functions","text":"<p>An operator function is a decorated python function with a specific signature. To execute your operator <code>ado</code> will call your function and expect it to return output in a given way. Below is an example of such a decorated function. The next sections describe the decorator, its parameters, and the structure of the operation function itself.</p> <pre><code>from orchestrator.modules.operators.collections import\n    characterize_operation  # Import the decorator from this module depending on the type of operation your operator performs\n\n\n@characterize_operation(\n    name=\"my_operator\",  # The name of your operator.\n    description=\"Example operator\",  # What this operator does\n    configuration_model=MyOperatorOptions,  # A pydantic model that describes your operators input parameters\n    configuration_model_default=MyOperatorOptions.default_parameters(),  # An example of your operators input parameters\n    version=\"1.0\",  # Version of the operator\n\n)\ndef detect_anomalous_series(\n        discoverySpace: DiscoverySpace,\n        operationInfo: typing.Optional[FunctionOperationInfo] = None,\n        **parameters,\n) -&gt; OperationOutput:\n    # Your operation logic - can also call other python modules etc.\n    ...\n    return operationOutput\n</code></pre>"},{"location":"operators/creating-operators/#operator-type","title":"Operator Type","text":"<p>The first thing you need to do is decided what type of operator you are creating. The choices are explore, characterize, learn, modify, fuse, export, or compare. You then import the decorator for this operator type from <code>orchestrator.modules.operators.collections</code> and use it to decorate your operator function.</p> <p>For example, if your operator compares <code>discoveryspaces</code> you would do</p> <pre><code>from orchestrator.modules.operators.collections import compare_operation\n\n@compare_operation(...)\ndef my_comparison_operation():\n</code></pre> <p>The decorator parameters are the same for all operator/operation types.</p>"},{"location":"operators/creating-operators/#operator-function-parameters","title":"Operator function parameters","text":"<p>All operator functions take one or more <code>discoveryspaces</code> along with a dictionary containing the inputs for the operation.</p> <p>If your operation type is <code>explore</code>, <code>characterize</code>, <code>learn</code> or <code>modify</code>, your function should have a parameter <code>discoverySpace</code> i.e.</p> <pre><code>def detect_anomalous_series(\n    discoverySpace: DiscoverySpace,\n    operationInfo: typing.Optional[FunctionOperationInfo] = None,\n    **parameters,\n) -&gt; OperationOutput:\n   ...\n</code></pre> <p>If it is <code>fuse</code> or <code>compare</code> your function should have a parameter <code>discoverySpaces</code> which is a list of <code>discoveryspaces</code> i.e.</p> <pre><code>def detect_anomalous_series(\n    discoverySpaces: list[DiscoverySpace],\n    operationInfo: typing.Optional[FunctionOperationInfo] = None,\n    **parameters,\n) -&gt; OperationOutput:\n   ...\n</code></pre> <p>Operator functions also take an optional third parameter, <code>operationInfo</code>, that holds information for <code>ado</code>. You do not have to interact with the parameter unless you are writing an explore operator.</p>"},{"location":"operators/creating-operators/#describing-your-operation-input-parameters","title":"Describing your operation input parameters","text":"<p>From the previous section, the <code>parameters</code> variable will contain the parameters values that should be used for a specific operation. However, how does <code>ado</code> know what the valid input parameters are for your operator so the contents of this variable will make sense?</p> <p>The answer is that the input parameters to your operator are described by a pydantic model that you give to the function decorator. Here's the example from the previous section with the relevant fields called out:</p> <pre><code>@characterize_operation(\n    name=\"my_operator\",\n    description=\"Example operator\",\n    configuration_model=MyOperatorOptions,  # &lt;- A pydantic model that describes your operators input parameters\n    configuration_model_default=MyOperatorOptions(), # &lt;- An example of your operators input parameters\n    version=\"1.0\",\n)\n</code></pre> <p>Here <code>MyOperatorOptions</code> is a pydantic model that describes your operators input parameters. The <code>parameters</code> dictionary that is passed to your operation function will be a dump of this model. So the typical first step in the function is to create the model for your inputs</p> <pre><code>inputs = MyOperatorOptions.model_validate(parameters)\n</code></pre>"},{"location":"operators/creating-operators/#providing-an-example-operation-configuration","title":"Providing an example operation configuration","text":"<p>The decorators <code>configuration_model_default</code> parameter takes a example of your operators parameters. If your operator's parameter model has defaults for all fields then the simplest approach is to use those as the value of <code>configuration_model_default</code>:</p> <pre><code>    configuration_model_default=MyOperatorOptions(), # &lt;- This will use the defaults specified for all fields of your operators parameters\n</code></pre>"},{"location":"operators/creating-operators/#how-your-operators-input-parameters-model-is-stored-and-output","title":"How your Operators input parameters model is stored and output","text":"<p>When outputting the default options via <code>ado template operator</code>, <code>model_dump_json()</code> is used with no options.</p> <p>When an <code>operation</code> is created using your Operator, the parameters are stored in the metastore using <code>model_dump_json()</code> with no options.</p>"},{"location":"operators/creating-operators/#operation-function-logic","title":"Operation function logic","text":"<p>We've covered how your operator will be called. However, where do you put your code?</p> <p>If you are not creating an explore operator you can implement as you like e.g. within the operator function or in a class or function in a separate module called from the operator function.</p> <p>If your operator type involves sampling and measuring entities e.g. it is an optimizer, your code has some additional packaging requirements which are discussed in explore operators.</p>"},{"location":"operators/creating-operators/#returning-data-from-your-operation","title":"Returning data from your operation","text":"<p>Note</p> <p>Any <code>ado</code> resources created will be stored in the context the operation was created in.</p> <p>The operator function must return data using the <code>orchestrators.core.operation.operation.OperationOutput</code> pydantic model.</p> <pre><code>class OperationOutput(pydantic.BaseModel):\n    metadata: typing.Dict = pydantic.Field(\n        default={},\n        description=\"Additional metadata about the operation. \",\n    )\n    resources: typing.List[orchestrator.core.resources.ADOResource] = pydantic.Field(\n        default=[],\n        description=\"Array of ADO resources generated by the operation\",\n    )\n    exitStatus: OperationResourceStatus = pydantic.Field(\n        description=\"Exit status of the operation. Default to success if not applied\",\n    )\n</code></pre> <p>The key fields to set are:</p> <ul> <li>resources: A list of <code>ado</code> resources your operation created.</li> <li>existStatus: Indicates if the operation worked or not</li> </ul> <p>Its expected that certain operation types return certain outputs:</p> <ul> <li>fuse, modify: Expected to return a new DiscoverySpaceResource and optionally a   SampleStoreResource</li> <li>compare: Expected to return a new DataContainerResource</li> <li>characterize: Expected to return a new DataContainerResource</li> </ul> <p>If you have non-ado resource data you want to return from your operation, for example pandas DataFrames, paths to files, text, lists etc. you can use <code>ado</code>s <code>datacontainer</code> resource.</p> <p>The following code snippet shows returning a dataframe, a dictionary with some key:value pairs, and an URL:</p> <pre><code>tabular_data = TabularData.from_dataframe(df)\nlocation = ResourceLocation.locationFromURL(someURL)\ndata_container = DataContainer(tabularData={\"main_dataframe\":tabular_data},\n                               data={\"important_dict\":results_dict},\n                               locationData={\"important_location\": location})\n\nreturn OperationOutput(resources=[DataContainerResource(config=data_container)])\n</code></pre>"},{"location":"operators/creating-operators/#how-to-update-your-operator-input-parameters","title":"How to update your operator input parameters","text":"<p>During development, there will be times when you might need to update the input parameter model for your operator, adding, removing or modifying fields. In these cases, it's important not to break backwards compatibility (where possible) while making sure that users are aware of the changes to the model and do not rely indefinitely on the model being auto upgraded.</p> <p>In ado, we recommend using Pydantic before validators coupled with the <code>ado upgrade</code> command. At a high level, you should:</p> <ol> <li>Use a before validator to create a temporary upgrade path for your model.</li> <li>Enable a warning in this validator using the provided support functions    (described below). This warning will inform users that an upgrade is needed.    The support function will automatically print the command to upgrade stored    model versions and remove the warning. It will also display a message    indicating that auto-upgrade functionality will be removed in a future    release.</li> <li>Remove the upgrade path in the specified future version.</li> </ol> <p>Let's see a practical example. Consider this class as the input parameter class in <code>my_operator</code> v1:</p> <pre><code>import pydantic\n\nclass MyOperatorOptions(pydantic.BaseModel):\n    my_parameter_name: int\n</code></pre> <p>And consider two cases:</p> <ul> <li>We want to deprecate a field.</li> <li>We want to apply changes to a field without deprecating it.</li> </ul>"},{"location":"operators/creating-operators/#deprecating-a-field-in-your-operator-input-parameters","title":"Deprecating a field in your operator input parameters","text":"<p>Let's imagine we want to change the name of the <code>my_parameter_name</code> field to be <code>my_improved_parameter_name</code>. The model for our operator v2 would then be:</p> <pre><code>import pydantic\n\nclass MyOperatorOptions(pydantic.BaseModel):\n    my_improved_parameter_name: int\n</code></pre> <p>To enable upgrading of the previous model versions when fields are being deprecated, we recommended using a Pydantic Before Model Validator. This allows the dictionary content of the model to be changed as appropriate before validation is applied. To ensure the users are aware of the change, we will also use the <code>warn_deprecated_operator_parameters_model_in_use</code> method in the validator:</p> <pre><code>import pydantic\n\nclass MyOperatorOptions(pydantic.BaseModel):\n    my_improved_parameter_name: int\n\n    @pydantic.model_validator(mode=\"before\")\n    @classmethod\n    def rename_my_parameter_name(cls, values: dict):\n        from orchestrator.modules.operators.base import (\n            warn_deprecated_operator_parameters_model_in_use,\n        )\n\n        old_key = \"my_parameter_name\"\n        new_key = \"my_improved_parameter_name\"\n        if old_key in values:\n\n            # Notify the user that the my_parameter_name\n            # field is deprecated\n            warn_deprecated_operator_parameters_model_in_use(\n                affected_operator=\"my_operator\",\n                deprecated_from_operator_version=\"v2\",\n                removed_from_operator_version=\"v3\",\n                deprecated_fields=old_key,\n                latest_format_documentation_url=\"https://example.com\",\n            )\n\n            # The user has set both the old\n            # and the new key - the new key\n            # takes precedence.\n            if new_key in values:\n                values.pop(old_key)\n            # Set the old value in the\n            # new field\n            else:\n                values[new_key] = values.pop(old_key)\n\n        return values\n</code></pre> <p>When a model with the old field will be loaded, the user will see the following warning:</p> <pre><code>WARN:   The parameters for the my_operator operator have been updated as of my_operator v2.\n        They are being temporarily auto-upgraded to the latest version.\n        This behavior will be removed with my_operator v3.\nHINT:   Run ado upgrade operations to upgrade the stored operations.\n        Update your operation YAML files to use the latest format: https://example.com\n</code></pre>"},{"location":"operators/creating-operators/#updating-a-field-in-your-operator-input-parameters-without-deprecating-it","title":"Updating a field in your operator input parameters without deprecating it","text":"<p>Let's imagine we want to change the type of the <code>my_parameter_name</code> field to be <code>str</code>. The model for our operator v2 would then be:</p> <pre><code>import pydantic\n\nclass MyOperatorOptions(pydantic.BaseModel):\n    my_parameter_name: str\n</code></pre> <p>To enable upgrading of the previous model versions when fields are not being deprecated, we recommended using a Pydantic Before Field Validator. This allows the specific field to be changed as appropriate before validation is applied. To ensure the users are aware of the change, we will also use the <code>warn_deprecated_operator_parameters_model_in_use</code> method in the validator:</p> <p>Note</p> <p>warning about deprecated fields, but we omit the <code>deprecated_fields</code> parameter.</p> <pre><code>import pydantic\n\nclass MyOperatorOptions(pydantic.BaseModel):\n    my_parameter_name: str\n\n    @pydantic.field_validator(\"my_parameter_name\", mode=\"before\")\n    @classmethod\n    def convert_my_parameter_name_to_string(cls, value: int | str):\n        from orchestrator.modules.operators.base import (\n            warn_deprecated_operator_parameters_model_in_use,\n        )\n\n        if isinstance(value, int):\n            # Notify the user that the parameters of my_operator\n            # have been updated\n            warn_deprecated_operator_parameters_model_in_use(\n                affected_operator=\"my_operator\",\n                deprecated_from_operator_version=\"v2\",\n                removed_from_operator_version=\"v3\",\n                latest_format_documentation_url=\"https://example.com\",\n            )\n            value = str(value)\n\n        return value\n</code></pre> <p>When a model using <code>int</code>s will be loaded, the user will see the following warning:</p> <pre><code>WARN:   The parameters for the my_operator operator have been updated as of my_operator v3.\n        They are being temporarily auto-upgraded to the latest version.\n        This behavior will be removed with my_operator v3.\nHINT:   Run ado upgrade operations to upgrade the stored operations.\n        Update your operation YAML files to use the latest format: https://example.com\n</code></pre>"},{"location":"operators/creating-operators/#nesting-operations","title":"Nesting Operations","text":"<p>Operators can use other operators. For example your operator can create operations using other operators and consume the results. You access other operators via the relevant collection in <code>orchestrator.modules.operators.collections</code>. For example to use the RandomWalk operator</p> <pre><code>from orchestrator.modules.operators.collections import explore\n\n@learn_operation(...)\ndef my_learning_operation(...):\n    ...\n    #Note: The name of the function called (here random_walk() ) is the operator name\n    random_walk_output = explore.random_walk(...Args...)\n    ...\n</code></pre> <p>The name used to call an operator function is the name of the</p> <p>operator. This is the name given to the decorator <code>name</code> parameter and is the name show by <code>ado get operators</code></p> <p>You access the data of the operation from the OperationOutput instance it returns. Any <code>ado</code> resources the nested operation creates will have been automatically added to the correct project by <code>ado</code>.</p>"},{"location":"operators/creating-operators/#creating-explore-operators","title":"Creating Explore Operators","text":"<p>Explore operators sample and measure entities. In <code>ado</code> all explore operation run as distributed ray jobs with:</p> <ul> <li>actuator ray actors for performing measurements</li> <li>discovery space manager actor for storing and notifying about measurement   results</li> </ul> <p>This means explore operators need to be implemented differently to the others, in particular</p> <ul> <li>The logic of your explore operator must be implemented as a ray actor (a   class)</li> <li>The explore operator functions must call this class i.e. you won't have any   operator logic in the function</li> </ul>"},{"location":"operators/creating-operators/#explore-operation-functions","title":"Explore operation functions","text":"<p>All explore operation functions follow this pattern:</p> <pre><code>@explore_operation(\n    name=\"ray_tune\",\n    description=RayTune.description(),\n    configuration_model=RayTuneConfiguration,\n    configuration_model_default=RayTuneConfiguration(),\n)\ndef ray_tune(\n        discoverySpace: DiscoverySpace,\n        operationInfo: FunctionOperationInfo = FunctionOperationInfo(),\n        **kwargs: typing.Dict,\n) -&gt; OperationOutput:\n    \"\"\"\n    Performs an optimization on a given discoverySpace\n\n    \"\"\"\n\n    from orchestrator.core.operation.config import OperatorModuleConf\n    from orchestrator.module.operator.orchestrate import explore_operation_function_wrapper\n\n\n    ## This describes where the class the implements your explore operation is\n    module = OperatorModuleConf(\n        moduleName=\"ado_ray_tune.operator\",  # The name of the package containing your explore actor\n        moduleClass=\"RayTune\",  # The name of your explore actor class\n        moduleType=orchestrator.modules.module.ModuleTypeEnum.OPERATION,\n    )\n\n    # validate parameters\n    RayTuneConfiguration.model_validate(kwargs)\n\n    # Tell ado to execute your class\n    output = explore_operation_function_wrapper(\n        discovery_space=discoverySpace,\n        module=module,\n        parameters=kwargs,\n        namespace=f\"namespace-{str(uuid.uuid4())[:8]}\",  #\n        operation_info=operationInfo,  # Important: This is where you must pass the operationInfo parameter to ado\n    )\n\n    return output\n</code></pre>"},{"location":"operators/creating-operators/#explore-operator-classes","title":"Explore operator classes","text":"<p>TBA</p>"},{"location":"operators/creating-operators/#operator-plugin-packages","title":"Operator plugin packages","text":"<p>Operator plugin packages follow a standard python structure</p> <pre><code>$YOUR_REPO_NAME\n\u2502\u00a0 \u2514\u2500\u2500 $YOUR_PLUGIN_PACKAGE        # Your plugin\n\u2502\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0     \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The key to making it an ado plugin is having a <code>[project.entry-points.\"ado.operators\"]</code> section in the <code>pyproject.toml</code> e.g.</p> <pre><code>[project]\nname = \"ado-ray-tune\" #Note this is the distribution name of the python package. Your ado operator(s) can have different ado identifier\nversion = \"0.1.0\"\ndependencies = [\n  #Dependencies\n]\n\n[project.entry-points.\"ado.operators\"]\nado-ray-tune = \"ado_ray_tune.operator_function\" # The key is the distribution name of your python package and the value is the python module in your package containing your decorated operator function\n</code></pre> <p>This references the python module (file) that contains your operator function</p> <p>Note</p> <p>You can define multiple operator functions in the referenced module.</p>"},{"location":"operators/explore_operators/","title":"Explore operators","text":"<p>A core task is sampling and measuring <code>entities</code> from a <code>discoveryspace</code> and this is the objective of explore <code>operators</code>. In fact, other than copying data from an external source, <code>operations</code>using explore <code>operators</code> are the only way new entities can be sampled and measurements performed.</p> <p>Because <code>operations</code> of the explore operators do more than just modify or analyse data, but result in measurements being executed and <code>entities</code> being placed in the <code>samplestore</code>, its worth diving into the in more detail.</p>"},{"location":"operators/explore_operators/#timeseries","title":"Timeseries","text":"<p>Every explore operation sample entities and perform measurements in some sequence, hence there is an associated timeseries. This timeseries is recorded for every <code>explore</code> operation. To see it via <code>ado</code> CLI use</p> <pre><code>ado show entities operation $OPERATION_IDENTIFIER\n</code></pre> <p>this will output a table of the entities in the order they were sampled during the operation.</p> <p>Programmatically you can access this information by modifying the follow snippet</p> <pre><code>import yaml\nfrom orchestrator.core.discoveryspace.space import DiscoverySpace\nfrom orchestrator.metastore.project import ProjectContext\n\nwith open(\"my_context.yaml\") as f:\n    c = ProjectContext.model_validate(yaml.safe_load(f))\n\nspace = DiscoverySpace.from_stored_configuration(project_context=c, space_identifier='space_abc123')\n# Get the timeseries of a property `someproperty` measured by `someexperiment from the space\n# for operation \"operation_abc123\". You can also omit the limit_to_properties parameter to retrieve\n# all the properties\nspace.complete_measurement_request_with_results_timeseries(operation_id=\"operation_abc123\",\n                                                           limit_to_properties=[\"someexperiment.someproperty\"])\n</code></pre> <p>Importantly the same entity can be visited by multiple different <code>operations</code>. Looking at the <code>entity</code> will not show which <code>operation</code> measured which value. However, this information is accessible via the timeseries.</p>"},{"location":"operators/explore_operators/#the-core-explore-loop","title":"The core explore loop","text":"<p>Each explore operation will perform the following steps in some way:</p> <ul> <li>Sample 1 or more entities from the <code>discoveryspace</code></li> <li>For each experiment in the <code>measurementspace</code>:</li> <li>If it has been already executed on this entity AND the <code>discoveryspace</code> only     permits one value per observed-property<ul> <li>replay the already measured value</li> </ul> </li> <li>Otherwise:<ul> <li>call the actuator to perform the measurement</li> </ul> </li> <li>Wait until all measurements have completed</li> <li>As each completes:<ul> <li>add the replayed/newly measured values to the sampling timeseries for this   operation (if the measurement did not fail)</li> <li>add the entity to the <code>samplestore</code> if it's not there</li> <li>update the entity in the <code>samplestore</code> with the new measured property   values (if the measurement did not fail)</li> </ul> </li> </ul>"},{"location":"operators/explore_operators/#replayed-measurements","title":"Replayed Measurements","text":"<p>A core goal of <code>ado</code> is transparent data-sharing. This is enabled via the common context provided by <code>samplestores</code> and the schema used to store <code>entities</code>.</p> <p>To leverage this data-sharing capability explore operation will, be default, not remeasure an entity they sample if it already has data for that measurement. For example, an explore operation samples an entity from a space that whose <code>measurementspace</code> includes an experiment called \"myexperiment-v1\". If it sees the entity has values for experiment <code>myexperiment-v1</code> it won't execute it again, instead it replays (aka \"memoizes\") it</p> <p>This means if a different user sampled and measured this entity with this experiment on a different space we transparently reuse their results, saving execution time.</p> <p>Some operators will allow turning this replay behaviour on and off.</p> <p>If the replay functionality is off then the entity will be remeasured with the experiment and it will have two values for each observed property of that experiment. Going back to our example above, if <code>myexperiment-v1</code> was executed again, and it measured properties <code>prop1</code> and <code>prop2</code> then the entity will two values for <code>myexperiment-v1.prop1</code> and <code>myexperiment-v1.prop2</code>, one from each time the experiment was applied to that entity.</p> <p>What if you switch it on but an entity has multiple measurements of the same experiment? In this case each existing measurement is replayed. In our example, this would mean if an entity has had <code>myexperiment-v1</code> applied twice, and then is sample again with replay on, two measurements will be replayed: the first and the second.</p>"},{"location":"operators/explore_operators/#failed-measurements","title":"Failed measurements","text":"<p>If a measurement of an entity fails in a way not expected by the <code>actuator</code> i.e. it could not measure any of its target properties, the <code>entity</code> will be added to the <code>samplestore</code> (if it was not present already); the <code>operation</code> will proceed; but this entity will have no measured values for this experiment.</p>"},{"location":"operators/explore_operators/#exploration-operation-metadata","title":"Exploration operation metadata","text":"<p>When an explore operation finishes the system (top-level) metadata field of the associated <code>operation</code> resource is updated with the following fields.</p> <pre><code>entities_submitted: #The number of entities sampled from the space\nexperiments_requested: #The number of experiments requested - should be (number of experiments in measurement space)*entitiesSampled\n</code></pre> <p>Example from a completed random walk <code>operation</code>:</p> <pre><code>config:\n  metadata:\n    description:\n      Both single and multi GPU runs of GPTQ-LoRA experiments for first group of\n      GPTQ-LoRA\n    labels:\n      group: \"1\"\n      group_type: gptq-lora\n      issue: \"904\"\n  operation:\n    module:\n      moduleClass: RandomWalk\n      moduleName: orchestrator.modules.operators.randomwalk\n      modulePath: .\n      moduleType: operation\n    parameters:\n      batchSize: 4\n      singleMeasurement: false\n      numberEntities: all\n      samplerConfig:\n        mode: sequential\n        samplerType: generator\n  spaces:\n    - space-8f1cfb-91ecfb\ncreated: \"2024-10-07T06:46:08.176924Z\"\nidentifier: randomwalk-0.6.4-1be83b\nkind: operation\nmetadata:\n  entities_submitted: 160\n  experiments_requested: 160\noperationType: search\noperatorIdentifier: randomwalk-0.6.4\nresult: null\nstatus: []\nversion: v1\n</code></pre>"},{"location":"operators/explore_operators/#viewing-the-operation-and-discoveryspace-state-as-an-operation-runs","title":"Viewing the <code>operation</code> and <code>discoveryspace</code> state as an operation runs","text":"<p>As an <code>operation</code> is running new measurements are being performed, entities added, and some fraction of the requested entities will have been sampled. Some <code>ado</code> commands reflect this changing state while other do not.</p> <p>Commands that reflect changing state during an <code>operation</code>:</p> <ul> <li><code>ado show entities space</code></li> <li><code>ado show entities operation</code></li> <li><code>ado show details space</code></li> </ul> <p>Commands that do not reflect changing state during an <code>operation</code>:</p> <ul> <li><code>ado get operation $OPERATION_IDENTIFIER</code></li> <li><code>ado show details operation $OPERATION_IDENTIFIER</code></li> </ul> <p>The <code>operation</code> resource itself will be updated with metadata when the operation finishes but not during it. <code>ado show details operation $OPERATION_IDENTIFIER</code> uses this metadata so hence it will be correct until the operation is finished.</p>"},{"location":"operators/optimisation-with-ray-tune/","title":"The Ray Tune Operator","text":""},{"location":"operators/optimisation-with-ray-tune/#overview","title":"Overview","text":""},{"location":"operators/optimisation-with-ray-tune/#what-does-the-ray_tune-operator-do","title":"What does the <code>ray_tune</code> operator do?","text":"<p>The <code>ray_tune</code> operator enables running optimization algorithms on a <code>discoveryspace</code>. It uses the RayTune framework, and most of the capabilities of RayTune can be access via the operator without the need to write python code.</p> <p><code>ray_tune</code> is an explore operator.</p>"},{"location":"operators/optimisation-with-ray-tune/#when-should-you-use-the-ray_tune-operator","title":"When should you use the <code>ray_tune</code> operator?","text":"<p>Use the <code>ray_tune</code> operator when you want to:</p> <ul> <li>find the maximum or minimum value of an observed property/target property   in a <code>discoveryspace</code></li> <li>efficiently sample a <code>discoveryspace</code> with respect to an observed   property/target property i.e. sample to understand the distribution of that   metric in the space</li> </ul> <p>The <code>ray_tune</code> operator supports memoization: if it samples the same entity twice, and that entity has already had the measurement space applied, it will replay the already measured values (by default).</p>"},{"location":"operators/optimisation-with-ray-tune/#differences-in-using-the-ray_tune-operator-and-raytune-directly","title":"Differences in using the <code>ray_tune</code> operator and RayTune directly","text":"<p>Using RayTune via the ado <code>ray_tune</code> operator brings the following advantages:</p> <ul> <li>Distributed storage and sharing of optimization runs and their results</li> <li>Automatic recording of provenance</li> <li>Transparent and distributed memoization</li> <li>Fully declarative interface, no need for programming</li> </ul> <p>However, there are a few drawbacks:</p> <ul> <li>Some customizations that requires programming are not available</li> <li>The current <code>ado</code> generic actuator model is not compatible with some RayTune   features which assume interaction with RayTrain.</li> <li>See early measurement stopping for more     details.</li> </ul>"},{"location":"operators/optimisation-with-ray-tune/#what-happens-if-i-apply-multiple-ray_tune-operations-to-a-space","title":"What happens if I apply multiple <code>ray_tune</code> operations to a space?","text":"<p>If you apply multiple <code>ray_tune</code> operations you just get multiple optimization runs of the different lengths and types you have requested. This is the same behaviour as applying RandomWalk multiple time to a space and is explained in more detail in the RandomWalk documentation</p>"},{"location":"operators/optimisation-with-ray-tune/#available-optimizers","title":"Available Optimizers","text":"<p>The optimizers available depend on the RayTune version used. At time of writing they are:</p> <ul> <li>ax</li> <li>hyperopt</li> <li>bayesopt</li> <li>bohb</li> <li>nevergrad</li> <li>optuna</li> <li>zoopt</li> <li>hebo</li> </ul> <p>In addition <code>ado</code> adds a additional optimizer called <code>lhu_sampler</code>.</p> <p>Important</p> <p>The above names are used to specify the optimizer to use in an operation.</p> <p>The names are defined by RayTune: check the raytune docs for the current list of optimizer names. The list is also defined by in the variable <code>ray.tune.search.SEARCH_ALG_IMPORT</code></p> <p>Warning</p> <p>RayTune also defines python classes for each optimizer. These class names are NOT the same as the \"optimizer names\" it defines and cannot be used with <code>ado</code></p>"},{"location":"operators/optimisation-with-ray-tune/#installing-an-optimizer","title":"Installing an optimizer","text":"<p>Each optimizer is its own python package and RayTune does not install them by default. <code>ado</code> installs <code>ax</code> but if you want to use any of the others you must install the corresponding python package. For example to use <code>nevergrad</code> run</p> <pre><code>pip install nevergrad\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#setting-the-parameters-of-a-ray_tune-operation","title":"Setting the parameters of a <code>ray_tune</code> operation","text":"<p>When configuring a <code>ray_tune</code> operation there are three groups of parameters to consider:</p> <ul> <li>Tuning Configuration: general optimization parameters</li> <li>This includes specific     Optimizer Parameters</li> <li>Runtime Configuration: parameters related to RayTune, for   example where its stores data</li> <li>This includes the Stopper Configuration that determines if an     optimization should stop</li> <li>Orchestrator Configuration: parameters related to   <code>ado</code></li> </ul> <p>For example, the default parameters and values for a <code>ray_tune</code> operation are:</p> <pre><code>orchestratorConfig:\n  failed_metric_value: None # This will be used for the value of \"metric' for any entities where it could not be measured (for any reason)\n  result_dump: none # If specified the best result found will be written to this file\n  single_measurement_per_property: true # If true memoization is used. If false already measured entities will be re-measured.\nruntimeConfig:\n  stop: None # A list of Stoppers or None. See below for stoppers\ntuneConfig:\n  metric: wallclock_time # The target property identifier to optimize w.r.t\n  mode: min # Whether to search for min or max of the target property\n  num_samples: 1 # The number of samples to draw\n  search_alg:\n    name: ax # The name of the optimization algorithm to use\n    params: {} # The parameters for the optimizer\n</code></pre> <p>The following sections describes each of these parameter sets in more detail. As you go through these sections it is worth referring to the comprehensive example that demonstrate putting all the pieces together and how they interact.</p> <p>Info</p> <p>You can get a default RayTune operation template and the schema of its parameters by running <code>ado template operation --operator-name ray_tune --include-schema</code> The information output by this command should always be preferred over the information presented here if there is an inconsistency.</p>"},{"location":"operators/optimisation-with-ray-tune/#trials-versus-samples","title":"Trials versus Samples","text":"<p>In RayTune <code>samples</code> means the points to measure, and <code>trials</code> are measurements of those points. They are related to <code>ado</code> concepts of <code>entities</code> (samples) and <code>measurements</code> (trials).</p>"},{"location":"operators/optimisation-with-ray-tune/#orchestrator-config","title":"Orchestrator Config","text":"<p>The <code>orchestratorConfig</code> section currently supports the following parameters, which are all optional:</p> <ul> <li><code>failed_metric_value</code> (default None)</li> <li>This will be used for the value of \"metric' for any entities where it could     not be measured (for any reason)</li> <li>`result_dump (default None)</li> <li>If specified the best result found will be written to this file</li> <li><code>single_measurement_per_property</code> (default true)</li> <li>If true     memoization     is used. If false already measured entities will be re-measured.</li> </ul>"},{"location":"operators/optimisation-with-ray-tune/#tune-config","title":"Tune Config","text":"<p>The <code>tuneConfig</code> section supports many of the parameters of the <code>ray.tune.TuneConfig</code> class.</p> <p>Supported parameters:</p> <ul> <li><code>metric</code> (required)</li> <li>The     target property identifier     to optimize.</li> <li><code>mode</code> (required)</li> <li><code>min</code> or <code>max</code>: Whether to search for min or max of the target property</li> <li><code>search_alg</code> (required)</li> <li>Note: This must be an optimizer name c.f. in     RayTune it would be an optimizer instance</li> <li><code>num_samples</code> (defaults to 1)</li> <li>Note: The exact interpretation of <code>num_samples</code> is optimizer dependent     e.g. some do not count \"warm-up\" samples as part of this.</li> <li><code>max_concurrent_trials</code></li> <li>Note: this can also be controlled via most optimizers parameters. If not     set, the default value depends on the optimizer</li> <li><code>time_budget_s</code>: How many second to run the optimizer for</li> </ul> <p>Unsupported parameters:</p> <ul> <li><code>scheduler</code> - Coming soon</li> <li><code>reuse_actors</code> - not relevant</li> <li><code>trial_name_creator</code> - not relevant</li> <li><code>trail_dirname_creator</code> - not relevant</li> </ul>"},{"location":"operators/optimisation-with-ray-tune/#optimizer-parameters-search_algparams","title":"Optimizer Parameters (search_alg.params)","text":"<p>The optimizer parameters are given as a dictionary to the <code>tuneConfig.search_alg.params</code> field.</p> <p>The parameters available for a given optimizer are detailed in the ray tune documentation for that optimizer. Almost all parameters that are listed in the RayTune docs for creating an instance of an optimizer can be specified in <code>tuneConfig.search_alg.params</code>. However, there are a few that should not be set. This is discussed in parameters to omit.</p> <p>Info</p> <p>The dictionary value you set for <code>tuneConfig.search_alg.params</code> will be used to initialise the optimizer in the standard python manner:</p> <pre><code>optimizerInstance = optimizerClass(**params)\n</code></pre> <p>Info</p> <p>Currently you cannot obtain a given optimizers parameters are via <code>ado</code>. To understand the configuration possibilities for an optimizer you must check the RayTune documentation.</p>"},{"location":"operators/optimisation-with-ray-tune/#parameters-to-omit","title":"Parameters to omit","text":"<p>All RayTune optimizers have the parameters <code>space</code>, <code>metric</code> and <code>mode</code> which should be omitted from the <code>tuneConfig.search_alg.params</code> dictionary.</p> <p>The <code>space</code> parameter will be filled by <code>ado</code> based on the <code>discoveryspace</code> the <code>ray_tune</code> operation is being applied to.</p> <p>The <code>metric</code> and <code>mode</code> parameters are provided via the <code>tuneConfig</code> fields (see above).</p>"},{"location":"operators/optimisation-with-ray-tune/#common-parameters","title":"Common parameters","text":"<p>All RayTune optimizers support a parameter <code>points_to_evaluate</code> which is a list of the initial entities to test. Each entity is described by a dictionary of \"constitutive property id/\"value\" pairs.</p> <p>Warning</p> <p>The constitutive property identifiers and values must be compatible with the <code>discoveryspace</code> the operation is being applied to. If they are not an XXXXX exception will be raised when the operation starts.</p> <p>For example:</p> <pre><code>- model_name: granite-3b\n  number_gpus: 4\n  tokens_per_sample: 2048\n  gpu_model: A100-SXM4-80GB\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#optimizer-specific-parameters","title":"Optimizer specific parameters","text":"<p>In addition to <code>points_to_evaluate</code> each optimizer has its own parameters. For example, some optimizers allow evaluating multiple points at same time, while others have a warm-up period of random sampling that can set. Check the RayTune documentation for each optimizer to understand the available options.</p> <p>Info</p> <p><code>ado</code> does not perform any validation of the optimizer parameters. Validation will be performed by RayTune when creating the optimizer class</p>"},{"location":"operators/optimisation-with-ray-tune/#nevergrad-parameters","title":"nevergrad parameters","text":"<p>The nevergrad search algorithm has a required parameter <code>optimizer</code> that programmatically is set to a nevergrad optimizer class or instance. In <code>ado</code> set this value to the string name of the optimizer. This valid strings are the keys of the nevergrad registry, which you can see with:</p> <pre><code>import nevergrad\nprint(list(nevergrad.optimizers.registry.keys()))\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#run-config","title":"Run Config","text":"<p>The <code>runConfig</code> section supports many of the parameters of the <code>ray.tune.RunConfig</code> class. All are optional although the above template shows <code>stop</code> as this is the most relevant</p> <p>Many of the run config parameters are related to storing the RayTune runs on disk. Since <code>ado</code> automatically stores the results and operation details in <code>samplestore</code> and <code>metastore</code> you likely do not need to set any of these values.</p> <p>Key Supported Parameters:</p> <ul> <li><code>stop</code> - see Stoppers</li> <li><code>storage_path</code>:</li> <li><code>ado</code> defaults this to \"/tmp/ray_results\" as this directory is writable in     the default <code>ado</code> image used in ray clusters.<ul> <li>If you change this path ensure it is writable</li> </ul> </li> </ul> <p>Other supported parameters:</p> <ul> <li><code>name</code></li> <li><code>storage_filesystem</code></li> <li><code>verbose</code></li> </ul> <p>Unsupported parameters:</p> <p>These parameters are mostly for use with RayTrain (using tune as train hyperparameter search). As such they are specific to the use case of model training.</p> <ul> <li><code>failure_config</code></li> <li><code>checkpoint_config</code> - for RayTrain, not relevant here</li> <li><code>sync_config</code></li> </ul>"},{"location":"operators/optimisation-with-ray-tune/#stoppers","title":"Stoppers","text":"<p>Stoppers define conditions for stopping an optimization. An example includes stopping when no new min/max have been found after N samples.</p> <p>An optimization run can specify any number of stoppers and the optimization will stop whenever the condition of any stopper in the list is met.</p> <p>You specify the stoppers to use as a list to the <code>runConfig.stop</code> field. Each stopper in the list is defined using the following yaml.</p> <pre><code>name: #The name of the stopper class\npositionalParams: #The positional params of the stopper: a list of strings/numbers\nkeywordParams: #The keyword params of the stopper: a dictionary of key/value pairs\n</code></pre> <p>Info</p> <p>Its recommend to use <code>keywordParams</code> even for positional parameters</p> <p>The following sections describe the available stoppers.</p>"},{"location":"operators/optimisation-with-ray-tune/#raytune-stoppers","title":"RayTune stoppers","text":"<p>RayTune provides a number of Stoppers. Some of these, MaximumIterationStopper, ExperimentPlateauStopper and TrialPlateauStopper, are for early measurement stopping when using RayTrain and have no effect when used via <code>ado</code>.</p> <p>Of the remaining stoppers TimeoutStopper is automatically used if you specify <code>tuneConfig.time_budget_s</code> so it does not have to specified independently. Similarly, CombinedStopper is automatically used if you specify more than one <code>stopper</code> in the list given to <code>runConfig.stop</code>.</p> <p>Finally, FunctionStopper, which allows passing a custom python function as a stopper, cannot currently be used with <code>ado</code>.</p>"},{"location":"operators/optimisation-with-ray-tune/#ado-stoppers","title":"<code>ado</code> stoppers","text":"<p><code>ado</code> provides four inbuilt stoppers:</p> <ul> <li>SimpleStopper: Stops if no improvement in the target metric after N steps</li> <li>GrowthStopper: Stops when the improvement in the target metric is less than a   threshold for N steps</li> <li>MaxSamplesStopper: Stops when a certain number of samples have been drawn. Is   less ambiguous than <code>tuneConfig.num_samples</code></li> <li>InformationGainStopper: Stops when samples are no longer providing significant   additional information on how the constitutive properties of the entity space   are related to the target property.</li> </ul> <p>Each of these are described in more detail, along with their parameters, here.</p>"},{"location":"operators/optimisation-with-ray-tune/#example","title":"Example","text":"<p>This example shows using <code>SimpleStopper</code> to stop an optimization when the metric has not improved after 10 trials. It will allow 5 trials to be performed before checking if it should stop the optimization, and will consider trials returning <code>nan</code> for the metric towards the 10 trial budget. It combines this with <code>MaxSamplesStopper</code> to stop when 50 samples have been drawn.</p> <pre><code>stop:\n  - name: SimpleStopper\n    keywordParams:\n      metric: wallclock_time\n      min_trials: 5\n      buffer_states: 10\n      count_nan: True\n  - name: MaxSamplesStopper\n    keywordParams:\n      max_samples: 50\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#early-measurement-stopping","title":"Early Measurement Stopping","text":"<p>Some RayTune stoppers directly support the case where each trial (measurement) is a RayTrain job. These stoppers can inspect the progress of an individual RayTrain jobs i.e. intermediate metric values, to determine if a trial should be stopped. This ability assumes particular behaviour of such a job e.g. the trial is generating a timeseries of the metric being optimized.</p> <p>Currently, <code>ado</code>s actuator model does not assume there are intermediate values of a metric being measured by an experiment, or provide a way to expose them. Instead, we leave these domain specific details to the actuator. For example, in <code>ado</code>s current model the actuator can implement and/or expose early-stopping if it is possible.</p>"},{"location":"operators/optimisation-with-ray-tune/#example-operation-yaml","title":"Example operation YAML","text":"<p>Here is an example ray_tune <code>operation</code> YAML for finding the workload configuration with the fastest throughput for fine-tuning performance using the sfttrainer actuator:</p> <ul> <li>using the Ax optimizer with its <code>parameter_constraint</code> optimizer specific   parameter</li> <li>the GrowthStopper to stop if no improvement found after 10   steps, where improvement means a configuration that is faster by more than 20   tokens per second</li> <li>the MaxSamplesStopper to stop once 50 configurations have   been searched</li> <li>a time budget of 2 hours</li> <li>specifying initial point to sample</li> </ul> <pre><code>orchestratorConfig:\n  failed_metric_value: None # This will be used for the value of \"metric' for any entities where it could not be measured (for any reason)\nruntimeConfig:\n  stop:\n    - name: GrowthStopper\n      keywordParams:\n        mode: max\n        metric: dataset_tokens_per_second\n        growth_threshold: 20 #if the change is less than 20 tokens per second consider the optimization not improving\n        grace_trials: 10\n    - name: MaxSamplesStopper\n      keywordParams:\n        max_samples: 50\ntuneConfig:\n  metric: dataset_tokens_per_second\n  mode: min\n  num_samples: 50 # The number of samples to draw. We also use max samples stopper in case Ax has a different interpretation of max samples\n  time_budget_s: 7200\n  search_alg:\n    name: ax # The name of the optimization algorithm to use\n    params:\n      points_to_evaluate:\n        - model_name: granite-3b\n          number_gpus: 4\n          model_max_length: 2048\n          gpu_model: A100-SXM4-80GB\n      parameter_constraints:\n        - \"batch_size &gt;= number_gpus\" # Don't sample points where batch_size &lt; number_gpus as these are invalid\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#ray_tune-operation-output","title":"<code>ray_tune</code> operation output","text":""},{"location":"operators/optimisation-with-ray-tune/#seeing-the-optimal-configuration-found","title":"Seeing the optimal configuration found","text":"<p>A successful <code>ray_tune</code> operation will create a <code>datacontainer</code> resource, containing information from RayTune on the best configuration found.</p> <p>To get the id of the <code>datacontainer</code> related to a ray_tune <code>operation</code> resource with id $OPERATION_IDENTIFIER use:</p> <pre><code>ado show related operation $OPERATION_IDENTIFIER\n</code></pre> <p>This will output something like:</p> <pre><code>datacontainer\n  - datacontainer-d6a6501b\ndiscoveryspace\n  - space-047b6a-f60613\n</code></pre> <p>To see the best point found (and in general the contents of the datacontainer) use the <code>describe</code> CLI command:</p> <pre><code>ado describe datacontainer $DATACONTAINER_ID\n</code></pre> <p>For a <code>datacontainer</code> created by a <code>ray_tune</code> operation, an example output is:</p> <pre><code>Identifier: datacontainer-d6a6501b\nBasic Data:\n\n  Label: best_result\n\n  {'config': {'x2': -1.1192905253425014,\n    'x1': 2.081208150586974,\n    'x0': 0.5621591414422049},\n   'metrics': {'function_value': 20.788056393697595,\n    'timestamp': 1756804287,\n    'checkpoint_dir_name': None,\n    'done': True,\n    'training_iteration': 1,\n    'trial_id': '7a7153ed',\n    'date': '2025-09-02_10-11-27',\n    'time_this_iter_s': 1.0576610565185547,\n    'time_total_s': 1.0576610565185547,\n    'pid': 52036,\n    'hostname': 'Michaels-MacBook-Pro-2.local',\n    'node_ip': '127.0.0.1',\n    'config': {'x2': -1.1192905253425014,\n     'x1': 2.081208150586974,\n     'x0': 0.5621591414422049},\n    'time_since_restore': 1.0576610565185547,\n    'iterations_since_restore': 1,\n    'experiment_tag': '40_x0=0.5622,x1=2.0812,x2=-1.1193'},\n   'error': None}\n</code></pre> <p>We can see here that the point found is <code>{'x2': -1.1192905253425014, 'x1': 2.081208150586974, 'x0': 0.5621591414422049}</code> where <code>function_value</code> was ~20.8.</p>"},{"location":"operators/optimisation-with-ray-tune/#optimization-path","title":"Optimization path","text":"<p>To see all the configurations (entities) visited during an optimization operation $OPERATION_IDENTIFIER run</p> <pre><code>ado show entities operation $OPERATION_IDENTIFIER\n</code></pre> <p>Note</p> <p>This command also works during an operation. It shows up to the most recent measured entity.</p>"},{"location":"operators/optimisation-with-ray-tune/#ado-additions-to-raytune","title":"ado additions to RayTune","text":"<p><code>ado</code> adds one optimizer and a selection of stoppers to those offered by RayTune</p>"},{"location":"operators/optimisation-with-ray-tune/#latin-hypercube-sampler","title":"Latin Hypercube Sampler","text":"<p>The <code>lhu_sampler</code> samples a <code>discoveryspace</code> using latin hypercube sampling. This is a method for \"almost\" randomly sampling a space while ensuring the samples are evenly spaced across the space. Using the <code>lhu_sampler</code> you can potentially get more information about the variance of a metric across a space with fewer samples than fully random sampling. It also reduces chances of not exploring dimensions in high-dimensional spaces when sampling budget is limited.</p> <p>The <code>lhu_sampler</code> pairs well with the InformationGainStopper</p> <p>Configuration the <code>lhu_sampler</code> follows the same pattern as those for other optimizer parameters. For the <code>lhu_sampler</code>, there is only one optional parameter, points_to_evaluate.</p> <pre><code>name: \"lhu_sampler\"\nparams:\n  points_to_evaluate:\n    - model_name: granite-3b\n      number_gpus: 4\n      tokens_per_sample: 2048\n      gpu_model: A100-SXM4-80GB\n    - model_name: granite-3b\n      number_gpus: 2\n      tokens_per_sample: 2048\n      gpu_model: A100-SXM4-80GB\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#maxsamplestopper","title":"MaxSampleStopper","text":"<p>Stops an optimization after N samples/trials.</p> <p>The following YAML describes the stoppers parameters. Parameters without values are required.</p> <pre><code>name: MaxSamplesStopper\nkeywordParams:\n  max_samples: 10 # Will stop the optimization when this number of samples have been measured. Required\n</code></pre>"},{"location":"operators/optimisation-with-ray-tune/#simplestopper","title":"SimpleStopper","text":"<p>Stops an optimization when the metric has not improved after N steps. Note, its excepted <code>mode</code> and <code>metric</code> should match the corresponding <code>tuneConfig</code> parameters but this is not checked.</p> <p>The following YAML describes the stoppers parameters. Parameters without values are required.</p> <pre><code>name: SimpleStopper\nkeywordParams:\n  mode: # `min` or `max`: Whether to search for min or max of the target property/metric. Required\n  metric: # The target property being optimized. Required.\n  min_trials: 5 # The number of trials to perform (samples to take) before applying any stopping criteria\n  buffer_states: 2 # The number of samples/optimization steps to wait before declaring no improvement.\n  stop_on_repeat: True # If True, the stopper will stop the optimization if it sees the same sample twice.\n  count_nan: True # If True, samples measuring 'nan' count towards the steps to wait before declaring no improvement.\n</code></pre> <p>Important</p> <p><code>buffer_states</code> does not reset if the metric is observed to improve in a step. That is, it is the total number of samples allowed that do not improve on the best found sample.</p>"},{"location":"operators/optimisation-with-ray-tune/#growthstopper","title":"GrowthStopper","text":"<p>Stops an optimization once it sees the metric improvement rate is below a threshold. This differs to SimpleStopper which as long as there is any improvement won't stop the optimization.</p> <ul> <li>if the metric value gets worse on a step, i.e. negative improvement, it is   considered to be below the threshold.</li> <li>Samples whose metric value is <code>nan</code> are always included</li> </ul> <p>The following YAML describes the stoppers parameters. Parameters without values are required.</p> <pre><code>name: GrowthStopper\nkeywordParams:\n  mode: # `min` or `max`: Whether to search for min or max of the target property/metric. Required\n  metric: # The target property being optimized. Required.\n  growth_threshold: 1.0 # If the difference in two samples is less than this threshold the optimization is considered to be not improving\n  grace_trials: 2 # The number of samples/optimization steps to wait before declaring the metric is not improving. Same as buffer_states for SimpleStopper.\n</code></pre> <p>Important</p> <p><code>grace_trials</code> does not reset if the metric is observed to \"grow\" in a step after it was observed to not \"grow\" That is, it is the total number of samples allowed where the improvement in the metric is less than threshold.</p>"},{"location":"operators/optimisation-with-ray-tune/#informationgainstopper","title":"InformationGainStopper","text":"<p>This stopper criteria is based on Mutual Information, which here is used to measures how related the constitutive properties (dimensions of the entity space) are to the metric being optimized. At a high-level this stopper stops when it observes the mutual information is converging.</p> <p>This stopper considers two ways that the mutual information can change:</p> <ol> <li>mutual information value: If the value is changing by less than a    threshold it is considered \"converging\"</li> <li>properties contribution to the mutual information: This can be measured    in two ways:</li> <li>Change in the ranking of which constitutive properties contribute the most       to the mutual information with the metric. If the ranking is not changing       the mutual information is considered to be converging.</li> <li>Change in the set of constitutive properties which contribute most to the       mutual information with metric. If the set of propertiers is not changing       the mutual information is considered to be converging.</li> </ol> <p>The stopper will only stop when it sees both the mutual information value and the properties that contribute to it converging.</p> <p>This stopper will perform at least <code>2x(number of constitutive properties in entity space)</code> samples before applying its stopping criteria.</p> <p>The following YAML describes the stoppers parameters. Parameters without values are required.</p> <pre><code>name: InformationGainStopper\nkeywordParams:\n  mi_diff_limit: # If the mutual information increase on addition of the latest sample is less than this value, it counts as \"converging\".\n  samples_below_limit: # # The number of samples/optimization steps to wait before declaring the mutual information is not increasing. Similar to buffer_states for SimpleStopper.\n  consider_pareto_front_convergence: # If True the stopper considers convergence of the set of important properties (2.2 above). If False it considers the ranking (2.1 above)\n</code></pre> <p>Important</p> <p>Both the mutual information value and the property ranking/set must stay unchanged for <code>samples_below_limit</code> for the stopping criteria to be reached</p>"},{"location":"operators/optimisation-with-ray-tune/#whats-next","title":"What's next","text":"<ul> <li> <p> Try Searching for the Best Configurations</p> <p>Try our example of using <code>ray_tune</code> to search for best configurations.</p> <p>Best configuration search example </p> </li> <li> <p> Try Latin Hyper-Cube Sampling</p> <p>Learn how to use Latin Hyber-Cube Sampling and the InformationGainStopper to discover important workload parameters.</p> <p>Latin Hyper-Cube sampler example </p> </li> </ul>"},{"location":"operators/random-walk/","title":"The Random Walk Operator","text":""},{"location":"operators/random-walk/#overview","title":"Overview","text":""},{"location":"operators/random-walk/#what-does-the-randomwalk-operator-do","title":"What does the RandomWalk operator do?","text":"<p>The RandomWalk operator provides different ways to randomly sample and measure points from a <code>discoveryspace</code>. Despite its name it can also perform deterministic sampling.</p> <p>RandomWalk is an <code>explore</code> type operator.</p>"},{"location":"operators/random-walk/#when-should-you-use-the-randomwalk-operator","title":"When should you use the RandomWalk operator?","text":"<p>Use the RandomWalk operator when you want to:</p> <ul> <li>get an unbiased idea of the distribution of property values across entities in   the <code>discoveryspace</code></li> <li>sample and measure all the entities in the space in random or sequential order   (finite size spaces)</li> <li>sample entities matching particular conditions (see below)</li> </ul> <p>The RandomWalk operator supports <code>memoization</code>: if it samples the same entity twice, and that entity has already had the measurement space applied, it will replay the already measured values (by default).</p>"},{"location":"operators/random-walk/#what-happens-if-i-apply-multiple-randomwalk-operations-to-a-space","title":"What happens if I apply multiple RandomWalk operations to a space?","text":"<p>If you apply multiple RandomWalk operations you just get multiple random walks of the different lengths and types you have requested.</p> <p>All explore operations are independent. This means each proceeds as configured - the only influence of previous operations is to enable <code>memoziation</code> if a subsequent operation visits the same point.</p> <p>To concretize this, consider two RandomWalk operations that sample deterministically (i.e. aren't actually random). The first is configured to sample 50 entities, the second 200 entities. At the start no entities have been sampled and measured from a given discovery space.</p> <p>After the first operation:</p> <ul> <li>50 entities will have been sampled and measured from the <code>discoveryspace</code>   (assuming no errors) and placed in its <code>samplestore</code></li> <li>The timeseries of this first operation is stored. It has 50 entities in it.</li> </ul> <p>After the second operation:</p> <ul> <li>200 entities will have been sampled and measured from the <code>discoveryspace</code>   (assuming no errors) and placed in its <code>samplestore</code></li> <li>150 will have been measured by this operation, with the first 50 being   replayed (as they were already measured during the first operation)</li> <li>The timeseries of this second operation is stored. It has 200 entities in it.</li> </ul>"},{"location":"operators/random-walk/#controlling-sampling-and-measurements-continuous-batching","title":"Controlling sampling and measurements: Continuous batching","text":"<p>When a RandomWalk operation encounters an unmeasured entity in the <code>discoveryspace</code> it applies the experiments defined by its <code>measurementspace</code> Depending on the experiments you may want to control how many concurrent experiments are being executed.</p> <p>RandomWalk uses the concept of continuous batching to set thr number of concurrent requested experiments and ensure that, as far as possible, there are always this number of experiments in flight.</p> <p>This approach maximises throughput compared to normal batch-wise submission. In the normal case the time to finish measuring batch of N entities is, at a minimum, the time taken for the longest experiment to complete. This means if one experiment is very long and the others short, there can be capacity in the system for (N-1) additional entities to be measured but it will not be used.</p> <p>The next section explains more about configuring continuous batching</p>"},{"location":"operators/random-walk/#configuring-a-randomwalk","title":"Configuring a RandomWalk","text":"<p>The parameters for a RandomWalk are (default values shown):</p> <pre><code>numberEntities: 1 # The maximum number of entities to sample. Can also be the string \"all\" - see \"Sampling all Entities\". \nbatchSize: 1  # The Number of entities in the initial batch. For more on this see \"Batch Size and Concurrent Experiments\" below\nsamplerConfig:\n  mode: random # How to sample - can be random, sequential, sequentialgrouped or randomgrouped. sequential requires the sampling supports sequencing entities\n  samplerType: selector # How to sample entities. Can be selector or generator. For more see Sampling Types and Modes below\n  grouping: [] # If the mode sequentialgrouped or randomgrouped this is a list of constitutive properties identifiers to group the entities by\nsingleMeasurement: true # If true memoization is used. If false already measured entities will be re-measured. For more see Multiple Measurement below\nmaxRetries: 0 # The number of times to retry a failed measurement on an entity. See Retrying Failed Measurements below.\nfilter:\n  filterMode:\n    nofilter # Sets filters on the entities in the space that should be sampled. Entities not matching the filter will not be sampled or measured.\n    # See \"Filtering Entities\" below for more details\n</code></pre> <p>An example <code>operation</code> YAML with a sequential selector:</p> <pre><code>metadata:\n  name: \"Example random walk operation\"\noperation:\n  module:\n    operatorName: random_walk\n    operatorType: explore\n  parameters:\n    batchSize: 1\n    samplerConfig:\n      mode: random\n      samplerType: selector\n    numberEntities: 1\n    singleMeasurement: true\n    filter:\n      filterMode: unmeasured\nspaces:\n  - your-spaces\n</code></pre> <p>Info</p> <p>You can get a default RandomWalk operation template and the schema of its parameters by running <code>ado template operation --operator-name random_walk --include-schema</code> The information output by this command should always be preferred over the information presented here if there is an inconsistency.</p>"},{"location":"operators/random-walk/#batch-size-and-concurrent-experiments","title":"Batch Size and Concurrent Experiments","text":"<p>When it comes to managing resources during an exploration, the key variable one wants to control is the number of concurrent experiments.</p> <p>For the RandomWalk operator this number is its <code>batchSize</code> parameter (the number of initial entities submitted) multiplied by the number of experiments in the <code>measurementspace</code> of the <code>discoveryspace</code> it is operating on. For example, if the <code>batchSize</code> is 2 and there are 2 experiments defined in the <code>measurementspace</code> there will be 4 (2*2) experiments requested. Each experiment will be measuring one entity. The continuous batching will endeavour to keep this many concurrent experiment requests during the operation.</p> <p>Info</p> <p>The RandomWalk operator only know how many experiments it has requested, not how many are actually executing. Hence continuous batching can only maintain that there are  N experiments requested at any time.</p>"},{"location":"operators/random-walk/#base-sampling-types-and-modes","title":"Base Sampling Types and Modes","text":"<p>The <code>samplerConfig</code> field controls how Entities are sampled during the operation. The base <code>samplerConfig</code> is shown in the examples above and has the following fields and defaults:</p> <pre><code> mode: random \n samplerType: selector \n grouping: [] \n</code></pre>"},{"location":"operators/random-walk/#sampling-types","title":"Sampling Types","text":"<p>There are two sampling types: <code>generator</code> and <code>selector</code>.</p> <p>The <code>generator</code> sampling type generates valid entities based on the <code>entityspace</code> definition. It currently only works with entities space that have a finite size i.e. all constitutive properties are DISCRETE or CATEGORICAL and are bounded.</p> <p>The <code>selector</code> sampling type draws existing matching entities from the <code>samplestore</code> of the <code>discoveryspace</code> i.e. it doesn't use the entity space.</p>"},{"location":"operators/random-walk/#sampler-modes","title":"Sampler Modes","text":"<p>Both sampling types support four modes, which can be categorised as flat or grouped :</p> <ul> <li>flat</li> <li>sequential</li> <li>random</li> <li>grouped</li> <li>sequentialgrouped</li> <li>randomgrouped.</li> </ul> <p>Info</p> <p>The flat modes sample entities directly. For the grouped modes, the sampling is done on 2 levels -  groups and then entities in the groups. The group level sampling can be either sequential or random, while group member level is always sequential</p> <p>The details of how each mode works can differ slightly depending on if a sampler is <code>generator</code> or <code>selector</code>.</p> <ul> <li><code>sequential</code></li> <li><code>generator</code>: The entities are generated by iterating over the constitutive     properties values with the first property being innermost and last outermost     (see below for more details).</li> <li><code>selector</code>: The entities are iterated in the order they are returned by the     <code>samplestore</code></li> <li><code>random</code></li> <li>For both samplers the entities are sampled in a random order.</li> <li><code>sequentialgrouped</code></li> <li>The entities are grouped by a user defined condition. Either all the     entities in space (<code>generator</code>) or the entities in the <code>samplestore</code>     (<code>selector</code>)<ul> <li>The groups are iterated in order</li> <li>The group members are iterated in order</li> </ul> </li> <li><code>randomgrouped</code></li> <li>The entities are grouped in the same way as <code>sequentialgrouped</code><ul> <li>The groups are iterated randomly</li> <li>The group members are iterated in order</li> </ul> </li> </ul> <p>In pseudocode, sequential mode iterates as follows if there are N constitutive properties</p> <pre><code>for x in propertyN.values:\n  for y in propertyN_1.values:\n     ...\n     for z in property1.values:\n         entity({'propertyN':x, 'propertyN_1':y, ..., 'property1':z})\n</code></pre>"},{"location":"operators/random-walk/#why-grouped-modes","title":"Why Grouped Modes?","text":"<p>The advantage of the group modes is that they can allow actuators to reuse their test environments, providing faster measurements. For example, consider an actuator that needs to create different test environments for different groups of entities. This creation may incur a significant overhead or there may be a limited on the number of simultaneous environments that can be created. In this case the measurements would be most efficient if all entities in a group are submitted to the actuator in sequence, as the actuator can create a test environment once and then reuse it for all group members. This is what grouping allows.</p> <p>Info</p> <p>See the docs of the specific actuator you are using to see if and how it can benefit from grouping.</p>"},{"location":"operators/random-walk/#enabling-grouping","title":"Enabling Grouping","text":"<p>To use the grouped modes (<code>randomgrouped</code>, <code>sequentialgrouped</code>) you need to supply a list of constitutive properties to group by using the <code>grouping</code> parameter. Here is an example configuration for using a <code>generator</code> sampler with <code>sequentialgrouped</code> mode:</p> <pre><code>metadata:\n  name: \"Example grouped random walk operation\"\noperation:\n  module:\n    operatorName: random_walk\n    operatorType: explore\n  parameters:\n    batchSize: 1\n    samplerConfig:\n      samplerType: generator\n      grouping:\n        - $CONSTITUTIVE_PROPERTY_ONE\n        - $CONSTITUTIVE_PROPERTY_TWO\n      mode: sequentialgrouped\n    numberEntities: 1\n\n    singleMeasurement: true\n    filter:\n      filterMode: unmeasured\nspaces:\n  - your-spaces\n</code></pre>"},{"location":"operators/random-walk/#custom-samplers","title":"Custom Samplers","text":"<p>It is also possible to specify that <code>random_walk</code> uses a custom sampler. This is a class that inherits from <code>orchestrator.core.discovery.samplers.BaseSampler</code>. This is useful for implementing more complex sampling schemes. For example, for developers who want to use random_walk to drive an exploration but have custom logic to execute before choosing each sample/entity.</p> <p>For custom samplers the <code>samplerConfig</code> field has the following structure:</p> <pre><code>module:\n  moduleClass: #The name of the custom sampler class\n  moduleName: #The name of the python module containing the sampler\nparameters: # A dictionary of key value pairs with the values for the custom samplers input parameters \n  ...\n</code></pre>"},{"location":"operators/random-walk/#implementing-a-custom-sampler","title":"Implementing a Custom Sampler","text":"<p>To implement a custom sampler create a sub-class of <code>orchestrator.core.discovery.samplers.BaseSampler</code> and implement all required methods</p> <p>The <code>BaseSampler</code> class does not specify any <code>__init__</code> parameters. If your custom class requires initialization parameters then</p> <ul> <li>define a pydantic model for them</li> <li>override the <code>parameters_model</code> class method to return this model</li> <li>add a non key-word parameter to your custom classes <code>__init__</code> that is this type.</li> </ul> <p>For example:</p> <pre><code># Class for the custom samplers parameters\nclass MySamplerParams(BaseModel): \n   ...\n\n# Subclass of BaseSampler implementing the custom sampling logic\nclass MySampler(BaseSampler):\n\n    @classmethod\n    def parameters_model(cls) -&gt; Optional[Type[BaseModel]]:\n        \n        # Return the custom samplers parameters model\n        return MySamplerParams\n    \n    # Add an init arg to take the parameters model\n    def __init__(self, parameters: MySamplerParams):\n         ...\n</code></pre>"},{"location":"operators/random-walk/#sampling-all-entities","title":"Sampling all Entities","text":"<p>If either of the following conditions are true you can specify a value of \"all\" for the <code>numberOfEntities</code> field in the random walk configuration:</p> <ul> <li>All dimensions in the <code>entityspace</code>s are discrete and bounded or categorical</li> <li>The sampling type is <code>selector</code> i.e. you are iterating over an existing set   number of entities in a <code>samplestore</code></li> </ul> <p>In the first case <code>all</code> will be converted to the size of the space. In the second case <code>all</code> will be converted to the number of matching entities in the <code>samplestore</code>.</p> <p>If both of these conditions is False the RandomWalk operator will raise a ValueError when the execution starts.</p> <p>Info</p> <p>Depending on the Filter settings a randomwalk operation may not sample \"all\" entities even if \"all\" is specified. This is because the filter may filter out some entities.</p> <p>Warning</p> <p>For <code>discoveryspaces</code> where one/both of the above conditions are True setting <code>numberOfEntities</code> greater than the corresponding size (size of space, or number of matching entities in <code>samplestore</code>) will raise a ValueError. This means you cannot set <code>numberOfEntities</code> to an arbitrarily large number to ensure sampling all of them - use <code>all</code> instead.</p>"},{"location":"operators/random-walk/#filtering-entities","title":"Filtering Entities","text":"<p>In some circumstance you may want to only sample a subset of Entities. Some examples include</p> <ul> <li>You want to skip replaying entities already measured as there are many and   just sample the unmeasured ones</li> <li>You might want to complete measurement of partially measured entities but not   want to start measuring unmeasured ones - for example for testing a   cost-function</li> <li>You might want to add additional measurements to all entities already fully   measured but not want to add measurements to any other measurements.</li> </ul> <p>The <code>filter</code> field provides this capability. It has one sub-field <code>filterMode</code> which can take the following values:</p> <ul> <li><code>noFilter</code>: The default value. No filtering is applied</li> <li><code>unmeasured</code>: Only Entities with no measurements by any of the experiments in   the <code>measurementspace</code> will be sampled</li> <li><code>partial</code>: Only Entities with measured by at least one and less than all the   experiments in the <code>measurementspace</code> will be sampled</li> <li><code>measured</code>: Only Entities fully measured by the experiments in the   <code>measurementspace</code> will be sampled</li> </ul>"},{"location":"operators/random-walk/#multiple-measurement","title":"Multiple Measurement","text":"<p>By setting <code>singleMeasurement:</code> to False the random walk operation will measure ALL entities it samples, even if they already have measurements.</p> <p>If entities have multiple measurements e.g. you turned this off and then turned it on again, then if an entity has multiple measurements each one will be replayed.</p> <p>Check replayed measurements for more details.</p>"},{"location":"operators/random-walk/#retrying-failed-measurements","title":"Retrying Failed Measurements","text":"<p>If the measurement of an entity by an experiment fails RandomWalk can retry it. The parameter controlling this is <code>maxRetries</code> which by default is 0 - no retries. If <code>maxRetries</code> is N then failing measurements will be retried up to <code>N</code> times.</p>"},{"location":"operators/random-walk/#experiment-request-index-v-number-of-experiments-requested","title":"Experiment request index v number of experiments requested","text":"<p>To understand a RandomWalk operations logs when maxRetries is greater than 0 its necessary to understand how it tracks the entity+experiment combinations it wants to measure versus the number of experiments it has requested to do these measurements.</p> <p>RandomWalk assigns an integer to each entity+experiment combination it wants to measure. This is called the request index - the Nth entity+experiment combination will have request index N-1.</p> <p>RandomWalk tracks retries based on request index. For example, it tracks that request index 5 has been retried 2 times.</p> <p>Info</p> <p>At the end of an RandomWalk operation, a summary of each request index that was retried is output. This includes how many times it was retried and what the final status was - either it performed maxRetries and still was FAILED  or one of the retries indicated SUCCESS</p> <p>Example summary output. Here <code>Request 8</code> means request index 8.</p> <p> <pre><code>Summary of 2 retried experiments\nRequest 0: Request d39947. Entity: fuchsia-raft (mock-sample-store-31799c). Experiment: mock.test-experiment. Retried: 2 times. Final status: Success\nRequest 8: Request f11853. Entity: flush-oasis (mock-sample-store-31799c). Experiment: mock.test-experiment. Retried: 1 times. Final status: Success\n</code></pre> </p> <p>Important</p> <p>When retrying is enabled, the highest request index is not equal to the number of experiments submitted. For example there may be 40 entity+experiment combinations tested, meaning greatest requestIndex is 39, but 50 experiments requested due to failures and retries.</p> <p>This also means that when retrying is enabled the experimentRequested metadata recorded with each RandomWalk operation is not equal to (number entities sampled) x (number of experiments in measurement space)</p>"},{"location":"operators/random-walk/#whats-next","title":"What's next","text":"<ul> <li> <p> Try our Random Walk Example</p> <p>Try using the RandomWalk operator with our example.</p> <p>Random Walk example </p> </li> <li> <p> Create new Operators</p> <p>Learn about extending ado with new Operators.</p> <p>Creating new Operators </p> </li> </ul>"},{"location":"operators/working-with-operators/","title":"Operators overview","text":"<p>An <code>operator</code> is a code module that provides a capability to perform an <code>operation</code> on a <code>discoveryspace</code>. For example the <code>RandomWalk</code> operator provides the capability to perform a random walk <code>operation</code> on a <code>discoveryspace</code>.</p> <p>The pages in this section (see left hand nav menu) give details about some of the operators available in <code>ado</code>: what they are for, what they do and how to use them.</p> <p>Info</p> <p>In addition, the examples section contains worked examples of using some of these operators.</p>"},{"location":"operators/working-with-operators/#operator-types","title":"<code>operator</code> types","text":"<p>Operators are grouped into the following types:</p> <ul> <li>explore: sample and measure entities from a <code>discoveryspace</code></li> <li>characterize: analyse a <code>discoveryspace</code></li> <li>modify: create a new <code>discoveryspace</code> by changing the entityspace or   measurementspace of an input <code>discoveryspace</code></li> <li>compare: compare one or more <code>discoveryspaces</code></li> <li>fuse: create a new <code>discoveryspace</code> from a set of input <code>discoveryspaces</code></li> </ul> <p>This page describes explore operators in more detail as they are the only operators that sample and measure entities.</p>"},{"location":"operators/working-with-operators/#listing-the-available-operators","title":"Listing the available operators","text":"<p>The following CLI command will list the available <code>operators</code></p> <pre><code>ado get operators\n</code></pre> <p>Example output:</p> <pre><code>                                       OPERATOR          TYPE\n1                       detect_anomalous_series  characterize\n0                                       profile  characterize\n2                                   random_walk       explore\n3                                      ray_tune       explore\n9                       export_to_llm_lakehouse        export\n8  integrate_and_export_to_llm_lakehouse_format        export\n4                                add_experiment        modify\n7              generate_representative_subspace        modify\n5                                learning_split        modify\n6                                      rifferla        modify\n</code></pre>"},{"location":"operators/working-with-operators/#using-operators","title":"Using operators","text":"<p>Using an operator involves the following steps:</p> <ol> <li>Find the input parameters for the operator:</li> </ol> <ul> <li><code>ado template operation --operator-name $OPERATOR_NAME</code></li> </ul> <ol> <li>Write an operation YAML for applying the operator.</li> </ol> <ul> <li>This involves setting specific values for its input parameters.</li> </ul> <ol> <li>Create the operation:</li> </ol> <ul> <li><code>ado create operation -f $YAML</code></li> </ul> <ol> <li>Retrieve the results of the operation:</li> </ol> <ul> <li><code>ado show related $OPERATION_IDENTIFIER</code></li> <li>in addition <code>ado show entities $OPERATION_IDENTIFIER</code> for explore operations</li> </ul> <p>These steps are covered in detail in operations.</p>"},{"location":"operators/working-with-operators/#whats-next","title":"What's next","text":"<ul> <li> <p> Try our examples</p> <p>Explore using some of these operators with our examples.</p> <p>Our examples </p> </li> <li> <p> Learn about Actuators</p> <p>Learn about extending ado with new Actuators.</p> <p>Creating new Actuators </p> </li> </ul>"},{"location":"resources/actuatorconfig/","title":"actuatorconfiguration","text":"<p>Some actuators expose parameters that can be configured. For these actuators an <code>actuatorconfiguration</code> resource define a particular set of values for these parameters. When creating an <code>operation</code> resource you can then optionally specify the <code>actuatorconfiguration</code> resources to retrieve to configure the actuators used in the operation.</p> <p>Info</p> <p>See enabling custom configuration of an actuator for how to enable this configuration capability for an actuator</p> <p>Some examples of configuration parameters are:</p> <ul> <li>location of storage e.g. pvc name</li> <li>A rest end-point</li> <li>what compute resources are available</li> </ul> <p>Often these parameters capture things that change depending on the system the actuator is running on. For example, one system has A100 GPUs but another doesn't, and the configuration allows the actuator to know before trying to submit an experiment that it can't be executed on the given system.</p> <p>Important</p> <p>Not all actuators can be configured</p> <p>Important</p> <p>By convention the value of actuator configuration parameters should not change the results of a given experiment on a given entity.</p>"},{"location":"resources/actuatorconfig/#creating-an-actuatorconfiguration","title":"creating an <code>actuatorconfiguration</code>","text":"<p>Similar to <code>operation</code> resources creating an <code>actuatorconfiguration</code> involves writing a YAML containing values for the configurable actuator parameters.</p>"},{"location":"resources/actuatorconfig/#getting-input-options-and-potential-values","title":"Getting input options and potential values","text":"<p>The available parameters can be found by checking the actuator documentation or a default configuration can be obtained using</p> <pre><code>ado template actuatorconfiguration --actuator-identifier $ACTUATORNAME\n</code></pre> <p>The output file will be called \"ACTUATORNAME_actuatorconfiguration_template_UID.yaml\"</p> <p>The schema of the configuration YAML, which includes documentation on each field, can additionally be output using</p> <pre><code>ado template actuatorconfiguration --actuator-identifier $ACTUATORNAME --include-schema\n</code></pre> <p>The schema will be output to a file called \"ACTUATORNAME_actuatorconfiguration_template_UID_schema.yaml\"</p>"},{"location":"resources/actuatorconfig/#creation-and-validation","title":"Creation and validation","text":"<p>Once the values you want are set and saved in a file called <code>$ACTUATOR_CONFIGURATION_FILE</code> then the <code>actuatorconfiguration</code> resource can be created with:</p> <pre><code>ado create actuatorconfiguration -f $ACTUATOR_CONFIGURATION_FILE\n</code></pre> <p>This will return an identifier with the format \"actuatorconfiguration-ACTUATORNAME-UID\" e.g.</p> <pre><code>&gt; Success! Created actuator configuration with identifier `actuatorconfiguration-robotic_lab-3edc9cd3`\n</code></pre> <p>On <code>create</code> the provided parameters are first passed to the actuator for validation. If it detects any errors you will get a warning indicating the issue and the resource will not be created.</p>"},{"location":"resources/actuatorconfig/#using-an-actuatorconfiguration","title":"Using an <code>actuatorconfiguration</code>","text":"<p><code>actuatorconfiguration</code>'s are used when creating <code>operation</code> resources. See specifying actuator parameters in the <code>operation</code> resource documentation for details.</p>"},{"location":"resources/actuatorconfig/#other-ado-commands-that-work-with-actuatorconfiguration","title":"Other ado commands that work with actuatorconfiguration","text":"<ul> <li><code>ado get actuatorconfigurations</code></li> <li>list stored <code>actuatorconfiguration</code>s or retrieve their representations</li> <li><code>ado show related actuatorconfiguration ID</code></li> <li>show operations using an <code>actuatorconfiguration</code></li> <li><code>ado edit actuatorconfiguration ID</code></li> <li>set the name, description, and labels for an <code>actuatorconfiguration</code></li> <li><code>ado delete actuatorconfiguration ID</code></li> <li>delete an <code>actuatorconfiguration</code></li> </ul>"},{"location":"resources/datacontainer/","title":"datacontainer","text":"<p>A <code>datacontainer</code> resource is one that contains data like tables, string and locations. It's main purpose is to store output of <code>operations</code> that aren't <code>samplestores</code> or <code>discoveryspaces</code>. For example, results of analysing the distribution of values in a space.</p>"},{"location":"resources/datacontainer/#creating-a-datacontainer","title":"creating a <code>datacontainer</code>","text":"<p>You currently can't create a <code>datacontainer</code> via the <code>ado</code> cli. They are only created as the result of applying certain operators.</p>"},{"location":"resources/datacontainer/#datacontainer-contents","title":"<code>datacontainer</code> contents","text":"<p>A <code>datacontainer</code> can contain the following types of data:</p> <ul> <li>lists, dicts, strings, numbers</li> <li>tabular data (DataFrames)</li> <li>location data (URLs)</li> </ul> <p>A <code>datacontainer</code> resource has upto three top-level fields: <code>data</code>, <code>locationData</code> and <code>tabularData</code> Each of these is a dictionary whose values are data objects and keys are the names of the data. The <code>tabularData</code> field contains items that are DataFrames. The <code>locationData</code> field contains items that are URLs. The <code>data</code> field contains items that are JSON serializable types: lists, dicts, string and numbers. Note, in the <code>data</code> field all data in containers must also be lists, dicts, strings or numbers.</p>"},{"location":"resources/datacontainer/#accessing-the-contents-of-a-datacontainer","title":"Accessing the contents of a <code>datacontainer</code>","text":""},{"location":"resources/datacontainer/#via-ado-cli","title":"via <code>ado</code> cli","text":"<p>The data in a <code>datacontainer</code> is stored directly in the resource description. Hence <code>ado get datacontainer $ID</code> will output it. However, depending on what is stored this may not be the best way to view it. Instead, you can try <code>ado describe datacontainer</code> which will format the contents e.g.</p> <pre><code>Identifier: datacontainer-532d8b6d\nBasic Data:\n\n  Label: person\n\n  {'age': 2, 'name': 'mj'}\n\n\n  Label: important_info\n\n  ['t1',\n   1,\n   't2']\n\nTabular Data:\n\n  Label: important_entities\n\n      nodes          config      status provider  vcpu_size  cpu_family  wallClockRuntime\n  0       5  A_f0.0-c1.0-n5          ok        A        1.0         0.0         84.453470\n  1       3  A_f1.0-c1.0-n3          ok        A        1.0         1.0        151.585624\n  2       3  A_f1.0-c1.0-n3          ok        A        1.0         1.0        155.028562\n  3       3  A_f1.0-c0.0-n3          ok        A        0.0         1.0        206.744962\n  4       4  A_f0.0-c0.0-n4          ok        A        0.0         0.0        145.129484\n  5       3  A_f0.0-c1.0-n3          ok        A        1.0         0.0        168.365908\n  6       5  A_f1.0-c1.0-n5          ok        A        1.0         1.0        105.637292\n  7       5  A_f1.0-c0.0-n5          ok        A        0.0         1.0        135.910925\n  8       4  A_f1.0-c1.0-n4          ok        A        1.0         1.0        116.314171\n  9       2  A_f1.0-c0.0-n2          ok        A        0.0         1.0        378.316570\n  10      5  A_f1.0-c0.0-n5          ok        A        0.0         1.0        117.941366\n  11      5  A_f0.0-c0.0-n5          ok        A        0.0         0.0        106.070931\n  12      4  A_f0.0-c1.0-n4          ok        A        1.0         0.0        106.670121\n  13      3  A_f0.0-c1.0-n3          ok        A        1.0         0.0        170.156597\n  14      2  A_f1.0-c1.0-n2          ok        A        1.0         1.0        291.904456\n  15      5  A_f0.0-c1.0-n5          ok        A        1.0         0.0         86.230161\n  16      2  A_f0.0-c0.0-n2          ok        A        0.0         0.0        335.208518\n  17      3  A_f0.0-c0.0-n3          ok        A        0.0         0.0        221.510197\n  18      4  A_f1.0-c0.0-n4          ok        A        0.0         1.0        158.706395\n  19      2  A_f0.0-c1.0-n2          ok        A        1.0         0.0        272.997822\n  20      5  A_f1.0-c1.0-n5          ok        A        1.0         1.0         96.847161\n  21      5  A_f0.0-c0.0-n5          ok        A        0.0         0.0        130.305123\n  22      3  A_f0.0-c0.0-n3          ok        A        0.0         0.0        216.394127\n  23      3  A_f1.0-c0.0-n3          ok        A        0.0         1.0        236.171507\n  24      3  B_f1.0-c0.0-n3          ok        B        0.0         1.0        220.198284\n  25      4  B_f1.0-c0.0-n4          ok        B        0.0         1.0        202.482397\n  26      5  B_f0.0-c0.0-n5          ok        B        0.0         0.0        103.905957\n  27      4  B_f1.0-c0.0-n4          ok        B        0.0         1.0        193.559971\n  28      2  B_f1.0-c1.0-n2          ok        B        1.0         1.0        298.819305\n  29      4  B_f0.0-c0.0-n4          ok        B        0.0         0.0        113.876770\n  30      3  B_f0.0-c0.0-n3          ok        B        0.0         0.0        153.516394\n  31      3  B_f0.0-c0.0-n3          ok        B        0.0         0.0        184.448016\n  32      5  B_f1.0-c0.0-n5          ok        B        0.0         1.0        141.990243\n  33      2  B_f1.0-c0.0-n2          ok        B        0.0         1.0        346.070996\n  34      5  B_f0.0-c0.0-n5          ok        B        0.0         0.0        112.705699\n  35      2  B_f0.0-c1.0-n2          ok        B        1.0         0.0        184.935050\n  36      4  B_f0.0-c0.0-n4          ok        B        0.0         0.0        132.541512\n  37      5  B_f1.0-c0.0-n5          ok        B        0.0         1.0        168.791785\n  38      2  B_f0.0-c0.0-n2          ok        B        0.0         0.0        225.179142\n  39      3  B_f0.0-c0.0-n3          ok        B        0.0         0.0        176.288144\n  40      2  B_f0.0-c0.0-n2          ok        B        0.0         0.0        228.143625\n  41      2  B_f0.0-c1.0-n2          ok        B        1.0         0.0        166.748432\n  42      5  B_f0.0-c0.0-n5          ok        B        0.0         0.0        113.885051\n  43      3  B_f1.0-c0.0-n3          ok        B        0.0         1.0        273.712027\n  44      2  C_f1.0-c1.0-n2          ok        C        1.0         1.0        363.285671\n  45      3  C_f1.0-c0.0-n3  Timed out.        C        0.0         1.0        598.883466\n  46      3  C_f1.0-c1.0-n3          ok        C        1.0         1.0        154.981347\n  47      5  C_f0.0-c0.0-n5          ok        C        0.0         0.0        138.060516\n  48      3  C_f0.0-c0.0-n3          ok        C        0.0         0.0        240.073585\n  49      3  C_f0.0-c1.0-n3          ok        C        1.0         0.0        168.916364\n  50      2  C_f0.0-c0.0-n2          ok        C        0.0         0.0        415.829285\n  51      3  C_f0.0-c1.0-n3          ok        C        1.0         0.0        174.033562\n  52      5  C_f0.0-c1.0-n5          ok        C        1.0         0.0         85.679467\n  53      4  C_f0.0-c0.0-n4          ok        C        0.0         0.0        188.090878\n  54      5  C_f1.0-c0.0-n5          ok        C        0.0         1.0        136.307105\n  55      4  C_f1.0-c0.0-n4          ok        C        0.0         1.0        177.723598\n  56      5  C_f1.0-c0.0-n5          ok        C        0.0         1.0        135.470500\n  57      4  C_f1.0-c1.0-n4          ok        C        1.0         1.0        114.014369\n  58      5  C_f0.0-c1.0-n5          ok        C        1.0         0.0         95.863261\n  59      4  C_f0.0-c1.0-n4          ok        C        1.0         0.0        121.424925\n  60      3  C_f1.0-c0.0-n3          ok        C        0.0         1.0        244.338875\n  61      3  C_f1.0-c1.0-n3          ok        C        1.0         1.0        168.348592\n  62      3  C_f0.0-c0.0-n3          ok        C        0.0         0.0        269.090664\n  63      5  C_f0.0-c0.0-n5          ok        C        0.0         0.0        150.947150\n  64      2  C_f1.0-c0.0-n2          ok        C        0.0         1.0        463.396539\n  65      5  C_f1.0-c1.0-n5          ok        C        1.0         1.0         92.171414\n  66      5  C_f1.0-c1.0-n5          ok        C        1.0         1.0        100.979775\n  67      2  C_f0.0-c1.0-n2          ok        C        1.0         0.0        309.842324\n\n\nLocation Data:\n\n  Label: entity_location\n\n  mysql+pymysql://admin:somepass@localhost:3306/sql_sample_store_aaa123\n</code></pre>"},{"location":"resources/datacontainer/#programmatically","title":"programmatically","text":"<p>For certain data, like large tables, it may be more convenient to access the data programmatically.</p> <p>If you do <code>ado get datacontainer $RESOURCEID -o yaml &gt; data.yaml</code>. Then the following snippet shows how to access the data in python</p> <pre><code>from orchestrator.core.datacontainer.resource import DataContainer\nimport yaml\n\nwith open('data.yaml') as f:\n    d = DataContainer.model_validate(yaml.safe_load(f))\n\n# for tabular data\nfor table in d.tabularData.values():\n    # Get the table as pandas dataframe\n    df = table.dataframe()\n    ...\n\nfor location in d.locationData.values():\n    # Each value in the locationData is a subclass of orchestrator.utilities.location.ResourceLocation\n    print(location.url().unicode_string())\n</code></pre>"},{"location":"resources/discovery-spaces/","title":"discoveryspace","text":"<p>Info</p> <p>If you are not familiar with the concept of a Discovery Space check here</p>"},{"location":"resources/discovery-spaces/#creating-a-discoveryspace-resource","title":"Creating a <code>discoveryspace</code> resource","text":""},{"location":"resources/discovery-spaces/#pre-requisites","title":"Pre-requisites","text":"<p>In order to create a <code>discoveryspace</code> you must provide a <code>samplestore</code> that the <code>discoveryspace</code> will use for storage. To see existing <code>samplestores</code> run:</p> <pre><code>ado get samplestores\n</code></pre> <p>Alternatively, if there is no existing store, when creating the space you can use the <code>---new-sample-store</code> flag. See the samplestores documentation for more details.</p>"},{"location":"resources/discovery-spaces/#discovery-space-configuration-yaml","title":"Discovery Space configuration YAML","text":"<p>Info</p> <p>You can execute <code>commandline ado template space --include-schema</code> to output example YAML and full schema information for <code>discoveryspace</code></p> <p>An example <code>discoveryspace</code> is given below. Note, the values in this YAML are for illustrative purposes and need to be changed to define a valid space.</p> <pre><code>sampleStoreIdentifier: source_abc123 # The id of the sample store to use\nentitySpace: #A list of constitutive properties\n  - identifier: my_property1 # The id of the first dimension/constitutive property of the space\n  - identifier: my_property2\nexperiments: # A list of experiments. The measurementspace of this discovery space\n  - acuatatorIdentifier: someactuator # The id of the actuator that contains the experiment\n    experimentIdentifier: experiment_one # The id of the experiment to execute\nmetadata:\n  description: \"This is an example discovery space\"\n  name: exampleSpace\n</code></pre> <p>The describing constitutive properties has more information on the options available for defining constitutive properties.</p> <p>Once you have your <code>discoveryspace</code> YAML in a file called <code>FILE.yaml</code> create it with</p> <pre><code>ado create space -f FILE.yaml\n</code></pre> <p>If there are errors or inconsistencies in the space definition the create command will output an error. A common reason for inconsistency is that the properties defined in the entity-space do not match the properties required for the experiments in the measurement space. The next section shows a way to handle this issue.</p>"},{"location":"resources/discovery-spaces/#generating-an-initial-yaml-from-an-experiment-list","title":"Generating an initial YAML from an Experiment list","text":"<p>Given a list of experiment ids, that you want to use for the <code>measurementspace</code>, you can create an initial compatible <code>discoveryspace</code> which you can then edit. See constitutive properties and domains for more.</p> <p>Assuming you are interested in the <code>finetune-gptq-lora-dp-r-4-a-16-tm-default-v1.1.0</code> experiment, you can create your space template using:</p> <pre><code>ado template space --from-experiment finetune-gptq-lora-dp-r-4-a-16-tm-default-v1.1.0\n</code></pre> <p>More in-depth documentation about this feature can be found in the section about <code>ado template</code></p>"},{"location":"resources/discovery-spaces/#differences-between-input-configuration-yaml-and-stored-configuration-yaml","title":"Differences between input configuration YAML and stored configuration YAML","text":"<p>After creating a <code>discoveryspace</code>, if you <code>ado get</code> its YAML you will notice that the information output is different from the one you provided in input. This is because the list of experiment references set in the YAML is expanded into the full experiment definitions and stored with the <code>discoveryspace</code>.</p>"},{"location":"resources/discovery-spaces/#discoveryspaces-and-shared-samplestores","title":"<code>discoveryspaces</code> and shared <code>samplestores</code>","text":"<p>Multiple <code>discoveryspace</code> resources can use the same <code>samplestore</code> resource. In this case you can think of the <code>discoveryspace</code> as a \"view\" on the <code>samplestore</code> contents, filtering just the <code>entities</code> that match its description.</p> <p>To be more rigorous, given a <code>discoveryspace</code> you can apply this filter in two ways:</p> <ol> <li>Filter <code>entities</code> that were placed in the <code>samplestore</code> via an operation the    <code>discoveryspace</code></li> <li>Filter <code>entities</code> in the <code>samplestore</code> that match the <code>discoveryspace</code></li> </ol> <p>To understand the difference in these two methods imagine two overlapping <code>discoveryspaces</code>, A and B, that use the same <code>samplestore</code>. If someone uses method one on <code>discoveryspace</code> A, they will only see the <code>entities</code> placed there by operations on <code>discoveryspace</code> A. However if someone uses method two on <code>discoveryspace</code> A, they will see <code>entities</code> placed there via operations on both <code>discoveryspace</code> A and space B.</p> <p>Shared samples stores also allow data to be reused across <code>discoveryspaces</code>, potentially accelerating exploration operations. See the shared sample store documentation for further details.</p>"},{"location":"resources/discovery-spaces/#accessing-entities","title":"Accessing Entities","text":"<p>A common task is to see a table of measured entities associated with a <code>discoveryspace</code></p>"},{"location":"resources/discovery-spaces/#ado-cli","title":"<code>ado</code> cli","text":"<p>The <code>show</code> command is used to show things related to a resource. In this case we want to show the entities related to a <code>discoveryspace</code> so we use:</p> <pre><code>ado show entities space\n</code></pre> <p>By default, this will output the entities as a table. There are various option flags that control this behaviour e.g. output to a CSV file.</p> <p>Following the above section there are two lists of entities this could show. The command above will use filter (1) - <code>entities</code> that were placed in the <code>samplestore</code> via an operation the <code>discoveryspace</code>.</p> <p>If you want to use filter (2) - <code>entities</code> in the <code>samplestore</code> that match the <code>discoveryspace</code> - use.</p> <pre><code>ado show entities space --include matching\n</code></pre> <p>Info</p> <p>Note: in both cases measurements on the entity will be filtered to be only those defined by the <code>measurementspace</code> of the <code>discoveryspace</code></p> <p>Two other options are</p> <ul> <li><code>--include unsampled</code> which lists <code>entities</code> defined by the <code>discoveryspace</code>   but not yet sampled by any operation (as long as the space is finite).</li> <li><code>--include missing</code> which lists <code>entities</code> defined by the <code>discoveryspace</code> but   not in the <code>samplestore</code></li> </ul>"},{"location":"resources/discovery-spaces/#programmatically","title":"Programmatically","text":"<p>Assuming you have your context in a file \"my_context.yaml\"</p> <pre><code>import yaml\nfrom orchestrator.metastore.project import ProjectContext\nfrom orchestrator.core.discoveryspace.space import DiscoverySpace\n\nwith open(\"my_context.yaml\") as f:\n    c = ProjectContext.model_validate(yaml.safe_load(f))\n\nspace = DiscoverySpace.from_stored_configuration(project_context=c, space_identifier='space_abc123')\n# Get the sampled and measured entities. Returns a pandas DataFrame\ntable = space.measuredEntitiesTable()\n# Get the matching. Returns a pandas DataFrame\ntable = space.matchingEntitiesTable()\n</code></pre>"},{"location":"resources/discovery-spaces/#target-v-observed-property-formats","title":"Target v observed property formats","text":"<p>There are two formats the entities can be output controlled by the <code>--property-format</code> option to <code>show entities</code></p> <p>The observed format outputs one row per entity. The columns are constitutive property names and the observed property names i.e. they include both the experiment id and target property id. This ensures that with one row per entity there are no clashing column names.</p> <p>The target format outputs one row per entity+experiment combination: so if there are two experiments in the Measurement Space then there will be two rows per entity. In this format the columns are constitutive property names and target property names.</p> <p>Info</p> <p>With <code>property-format=target</code> if the measurement space contains multiple experiments measuring different target properties, this will result in many empty fields in the table. This is because the column for a given target of one experiment will not have values in the rows corresponding to other experiments.</p>"},{"location":"resources/discovery-spaces/#defining-the-domains-of-constitutive-properties-in-the-entityspace","title":"Defining the domains of constitutive properties in the entityspace","text":"<p>The YAML for the constitutive properties in the <code>entityspace</code> has the following structure</p> <pre><code>identifier: model_name # The name of the property\npropertyDomain: # The domain describes the values the property can take\n  variableType:# The type of the variable: CATEGORICAL_VARIABLE_TYPE, DISCRETE_VARIABLE_TYPE, CONTINUOUS_VARIABLE_TYPE or UNKNOWN_VARIABLE_TYPE\n    # The type defines what values the next fields can take\n  values: # If the variable is CATEGORICAL_VARIABLE_TYPE this is a list of the categories\n    -  # If the variable is DISCRETE_VARIABLE_TYPE this can be a list of discrete float or integer values it can take\n  domainRange:# If the variables is DISCRETE_VARIABLE_TYPE or CONTINUOUS_VARIABLE_TYPE this is the min inclusive, max exclusive range it can take\n    # If the variable is DISCRETE_VARIABLE_TYPE and values are given this must be compatible with the values\n  interval:# If the variable is DISCRETE_VARIABLE_TYPE this is the interval between the values.\n    # If given domainRange is required and values cannot be given\n</code></pre> <p>As long as all constitutive properties are not \"UNKNOWN_VARAIBLE_TYPE\" there is sufficient information to sample new entities from the <code>entityspace</code> description.</p>"},{"location":"resources/discovery-spaces/#ensuring-the-entityspace-and-measurementspace-are-compatible","title":"Ensuring the <code>entityspace</code> and <code>measurementspace</code> are compatible","text":"<p>This section elaborates on Generating an initial YAML from an Experiment list.</p> <p>Experiments take entities as inputs and those entities must have values for various properties in order for the experiments to be able to process them. This means the domains of the properties in the <code>entityspace</code> must be compatible with the experiments - if not entities could be sampled that experiments in the <code>measurementspace</code> cannot measure.</p> <p>For example, to see the input requirements of the experiment <code>finetune_full_benchmark-v1.0.0</code> you can run:</p> <pre><code>ado describe experiment finetune-full-fsdp-v1.6.0 --actuator-id SFTTrainer\n</code></pre> <p>you will get output like</p> <pre><code>Identifier: SFTTrainer.finetune_full_benchmark-v1.0.0\n\nMeasures the performance of full-finetuning a model for a given (GPU model, number GPUS, batch_size, model_max_length, number nodes) combination.\n\nRequired Inputs:\n  Constitutive Properties:\n      model_name: The huggingface name or path to the model\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['allam-1-13b', 'granite-13b-v2', 'granite-20b-v2', 'granite-3-8b', 'granite-3.1-2b', 'granite-3.1-3b-a800m-instruct', 'granite-3.1-8b-instruct', 'granite-34b-code-base', 'granite-3b-1.5', 'granite-3b-code-base-128k', 'granite-7b-base', 'granite-8b-code-base', 'granite-8b-code-base-128k', 'granite-8b-code-instruct', 'granite-8b-japanese', 'granite-vision-3.2-2b', 'hf-tiny-model-private/tiny-random-BloomForCausalLM', 'llama-13b', 'llama-7b', 'llama2-70b', 'llama3-70b', 'llama3-8b', 'llama3.1-405b', 'llama3.1-70b', 'llama3.1-8b', 'mistral-123b-v2', 'mistral-7b-v0.1', 'mixtral-8x7b-instruct-v0.1', 'smollm2-135m']\n\n\n      model_max_length: The maximum context size. Dataset entries with more tokens they are truncated. Entries with fewer are padded\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 131073]\n\n      batch_size: The total batch size to use\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 4097]\n\n      number_gpus: The total number of GPUs to use\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [0, 33]\n\n\nOptional Inputs and Default Values:\n  max_steps: The number of optimization steps to perform. Set to -1 to respect num_train_epochs instead\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [-1, 10001]\n\n  Default value: -1\n\n  num_train_epochs: How many epochs to run. Ignored if max_steps is greater than 0\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1.0, 10001.0]\n\n  Default value: 1.0\n\n  stop_after_seconds: If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [-1.0, 1000001.0]\n\n  Default value: -1.0\n\n  dataset_id: The identifier of the dataset to use for training\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['news-chars-1024-entries-1024', 'news-chars-1024-entries-256', 'news-chars-1024-entries-4096', 'news-chars-2048-entries-1024', 'news-chars-2048-entries-256', 'news-chars-2048-entries-4096', 'news-chars-512-entries-1024', 'news-chars-512-entries-256', 'news-chars-512-entries-4096', 'news-tokens-128kplus-entries-320', 'news-tokens-128kplus-entries-4096', 'news-tokens-16384plus-entries-4096', 'vision-384x384-16384plus-entries-4096', 'vision-384x768-16384plus-entries-4096']\n\n\n  Default value: news-tokens-16384plus-entries-4096\n\n  gradient_checkpointing: If True, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [True, False]\n\n  Default value: 1\n\n  torch_dtype: The torch datatype to use\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['bfloat16', 'float16', 'float32']\n\n  Default value: bfloat16\n\n  gradient_accumulation_steps: Number of update steps to accumulate before performing a backward/update pass.\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 33]\n\n  Default value: 4\n\n  r: The LORA rank\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 33]\n\n  Default value: 4\n\n  lora_alpha: LORA Alpha scales the learning weights\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 33]\n\n  Default value: 16\n\n  fast_moe: Configures the amount of expert parallel sharding. number_gpus must be divisible by it\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [0, 33]\n\n  Default value: 0\n\n  fast_kernels: Switches on fast kernels, the value is a list with strings of boolean values for [fast_loss, fast_rms_layernorm, fast_rope_embeddings]\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: [None, ['True', 'True', 'True']]\n\n  Default value: None\n\n  optim: The optimizer to use.\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'adamw_torch_4bit', 'ademamix', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'ademamix_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_ademamix_32bit', 'paged_ademamix_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise', 'lomo', 'adalomo', 'grokadamw', 'schedule_free_adamw', 'schedule_free_sgd']\n\n\n  Default value: adamw_torch\n\n  bf16: Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [False, True]\n\n  Default value: 0\n\n  gradient_checkpointing_use_reentrant: Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function.\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [False, True]\n\n  Default value: 0\n\n  dataset_text_field: Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['output', 'messages']\n\n  Default value: output\n\n  dataset_image_field: For running vision language model tuning pass the column name of the image data in the dataset.\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: [None, 'images']\n\n  Default value: None\n\n  remove_unused_columns: Remove columns not required by the model when using an nlp.Dataset.\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [True, False]\n\n  Default value: 1\n\n  dataset_kwargs_skip_prepare_dataset: When True, configures trl to skip preparing the dataset.\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [True, False]\n\n  Default value: 0\n\n  flash_attn: Use Flash attention v2 from transformers\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [True, False]\n\n  Default value: 1\n\n  gpu_model: The GPU model to use\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: [None, 'NVIDIA-A100-SXM4-80GB', 'NVIDIA-A100-80GB-PCIe', 'Tesla-T4', 'L40S', 'Tesla-V100-PCIE-16GB', 'NVIDIA-H100-PCIe', 'NVIDIA-H100-80GB-HBM3']\n\n\n  Default value: None\n\n  distributed_backend: Which pytorch backend to use when training with multiple GPU devices\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['DDP', 'FSDP', 'None']\n\n  Default value: FSDP\n\n  number_nodes: If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1 Range: [1, 9]\n\n  Default value: 1\n\n  fms_hf_tuning_version: The version of fms-hf-tuning to use - controls which wrapper to use as well as python dependencies\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['2.0.1', '2.1.0', '2.1.1', '2.1.2', '2.2.1', '2.3.1', '2.4.0', '2.5.0', '2.6.0', '2.7.1', '2.8.2']\n\n\n  Default value: 2.1.2\n\n  enable_roce: This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [False, True]\n\n  Default value: 0\n\n  fsdp_sharding_strategy: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['FULL_SHARD', 'SHARD_GRAD_OP', 'NO_SHARD', 'HYBRID_SHARD', 'HYBRID_SHARD_ZERO2']\n\n\n  Default value: FULL_SHARD\n\n  fsdp_state_dict_type: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: ['FULL_STATE_DICT', 'LOCAL_STATE_DICT', 'SHARDED_STATE_DICT']\n\n\n  Default value: FULL_STATE_DICT\n\n  fsdp_use_orig_params: If True, allows non-uniform `requires_grad` during init, which means support for interspersed frozen and trainable parameters. (useful only when `use_fsdp` flag is passed).\n  Domain:\n    Type: BINARY_VARIABLE_TYPE Values: [False, True]\n\n  Default value: 1\n\n  accelerate_config_mixed_precision: Whether or not to use mixed precision training. Choose from 'no', 'fp16', 'bf16' or 'fp8'. 'fp8' requires the installation of transformers-engine.\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['no', 'fp16', 'bf16', 'fp8']\n\n  Default value: no\n\n  accelerate_config_fsdp_transformer_layer_cls_to_wrap: List of transformer layer class names (case-sensitive) to wrap, e.g, BertLayer, GraniteDecoderLayer, GPTJBlock, T5Block ... (useful only when using FSDP)\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE\n    Values: [None, 'GraniteDecoderLayer', 'LlamaDecoderLayer', 'MistralDecoderLayer', 'GPTJBlock', 'T5Block']\n\n\n  Default value: None\n\n\nOutputs:\n  finetune_full_benchmark-v1.0.0-gpu_compute_utilization_min\n  finetune_full_benchmark-v1.0.0-gpu_compute_utilization_avg\n  finetune_full_benchmark-v1.0.0-gpu_compute_utilization_max\n  finetune_full_benchmark-v1.0.0-gpu_memory_utilization_min\n  finetune_full_benchmark-v1.0.0-gpu_memory_utilization_avg\n  finetune_full_benchmark-v1.0.0-gpu_memory_utilization_max\n  finetune_full_benchmark-v1.0.0-gpu_memory_utilization_peak\n  finetune_full_benchmark-v1.0.0-gpu_power_watts_min\n  finetune_full_benchmark-v1.0.0-gpu_power_watts_avg\n  finetune_full_benchmark-v1.0.0-gpu_power_watts_max\n  finetune_full_benchmark-v1.0.0-gpu_power_percent_min\n  finetune_full_benchmark-v1.0.0-gpu_power_percent_avg\n  finetune_full_benchmark-v1.0.0-gpu_power_percent_max\n  finetune_full_benchmark-v1.0.0-cpu_compute_utilization\n  finetune_full_benchmark-v1.0.0-cpu_memory_utilization\n  finetune_full_benchmark-v1.0.0-train_runtime\n  finetune_full_benchmark-v1.0.0-train_samples_per_second\n  finetune_full_benchmark-v1.0.0-train_steps_per_second\n  finetune_full_benchmark-v1.0.0-train_tokens_per_second\n  finetune_full_benchmark-v1.0.0-train_tokens_per_gpu_per_second\n  finetune_full_benchmark-v1.0.0-dataset_tokens_per_second\n  finetune_full_benchmark-v1.0.0-dataset_tokens_per_second_per_gpu\n  finetune_full_benchmark-v1.0.0-is_valid\n</code></pre> <p>You can see the required inputs under the section <code>Required Inputs</code> and the optional inputs under <code>Optional Inputs and Default Values</code>. The next section explains how to use optional properties.</p> <p>Note</p> <p>The experiment gives the full domains it supports for each required and optional constitutive property. However, when constructing the <code>entityspace</code> you usually only want to use a sub-domain.</p>"},{"location":"resources/discovery-spaces/#parameterizing-experiments","title":"Parameterizing Experiments","text":"<p>If an experiment has optional properties you can define equivalent properties in the entity space. If you don't, the default value for the property will be used.</p> <p>In addition, you can define your own custom parameterization of the experiment. For example, take the following experiment:</p> <pre><code>Identifier: robotic_lab.peptide_mineralization\n\nMeasures adsorption of peptide lanthanide combinations\n\nRequired Inputs:\n  Constitutive Properties:\n      peptide_identifier\n      Domain:\n        Type: CATEGORICAL_VARIABLE_TYPE\n        Values: ['test_peptide', 'test_peptide_new']\n\n\n      peptide_concentration\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE\n        Values: [0.1, 0.4, 0.6, 0.8]\n        Range: [0.1, 1.8]\n\n\n      lanthanide_concentration\n      Domain:\n        Type: DISCRETE_VARIABLE_TYPE\n        Values: [0.1, 0.4, 0.6, 0.8]\n        Range: [0.1, 1.8]\n\n\n\nOptional Inputs and Default Values:\n  temperature\n  Domain:\n    Type: CONTINUOUS_VARIABLE_TYPE Range: [0, 100]\n\n  Default value: 23.0\n\n  replicas\n  Domain:\n    Type: DISCRETE_VARIABLE_TYPE Interval: 1.0 Range: [1, 4]\n\n  Default value: 1.0\n\n  robot_identifier\n  Domain:\n    Type: CATEGORICAL_VARIABLE_TYPE Values: ['harry', 'hermione']\n\n  Default value: hermione\n\n\nOutputs:\n  peptide_mineralization-adsorption_timeseries\n  peptide_mineralization-adsorption_plateau_value\n</code></pre> <p>It has three optional properties: <code>temperature</code>, <code>robot_identifier</code> and <code>replicas</code>.</p>"},{"location":"resources/discovery-spaces/#example-customizing-an-experiment","title":"Example: Customizing an experiment","text":"<p>The default temperature is <code>23</code> degrees C, however imagine you want to run this experiment at <code>30</code> degrees C. You can define a <code>discoveryspace</code> like:</p> <pre><code>sampleStoreIdentifier: c04713\nentitySpace:\n  - identifier: peptide_identifier\n    propertyDomain:\n      values: [\"test_peptide\"]\n  - identifier: peptide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\n  - identifier: lanthanide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\nexperiments:\n  - actuatorIdentifier: robotic_lab\n    experimentIdentifier: peptide_mineralization\n    parameterization:\n      - value: 30\n        property:\n          identifier: \"temperature\"\nmetadata:\n  description: Space for exploring the absorption properties of test_peptide\n</code></pre>"},{"location":"resources/discovery-spaces/#example-multiple-customizations-of-the-same-experiment","title":"Example: Multiple customizations of the same experiment","text":"<p>You can add the multiple custom parameterizations of the same experiment e.g. one experiment that runs at 30 degrees C and another at 25 degrees.</p> <pre><code>sampleStoreIdentifier: c04713 # PUT REAL ID HERE\nentitySpace:\n  - identifier: peptide_identifier\n    propertyDomain:\n      values: [\"test_peptide\"]\n  - identifier: peptide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\n  - identifier: lanthanide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\nexperiments:\n  - actuatorIdentifier: robotic_lab\n    experimentIdentifier: peptide_mineralization\n    parameterization:\n      - value: 30\n        property:\n          identifier: \"temperature\"\n  - actuatorIdentifier: robotic_lab\n    experimentIdentifier: peptide_mineralization\n    parameterization:\n      - value: 25\n        property:\n          identifier: \"temperature\"\nmetadata:\n  description: Space for exploring the absorption properties of test_peptide\n</code></pre>"},{"location":"resources/discovery-spaces/#example-using-an-optional-property-in-the-entityspace","title":"Example: Using an optional property in the <code>entityspace</code>","text":"<p>Finally, if you want to scan a range of temperatures in your discovery space, the best would be to move this parameter into the <code>entityspace</code>:</p> <pre><code>sampleStoreIdentifier: c04713 # PUT REAL ID HERE\nentitySpace:\n  - identifier: peptide_identifier\n    propertyDomain:\n      values: [\"test_peptide\"]\n  - identifier: peptide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\n  - identifier: lanthanide_concentration\n    propertyDomain:\n      values: [0.1, 0.4, 0.6, 0.8]\n  - identifier: temperature\n    propertyDomain:\n      domainRange: [20, 30]\n      interval: 1\nexperiments:\n  - actuatorIdentifier: robotic_lab\n    experimentIdentifier: peptide_mineralization\nmetadata:\n  description: Space for exploring the absorption properties of test_peptide\n</code></pre> <p>Here entities will be generated with a temperatures property that ranges from 20 to 30 degrees. When the experiment is run on the entity it will retrieve the value of the temperature from it rather than the Experiment.</p> <p>Our toy example actuator contains the above examples. You can use it to experiment and explore custom parameterization.</p>"},{"location":"resources/metastore/","title":"The metastore","text":"<p><code>ado</code> uses a SQL database to store resource definitions and SQLSampleStores. When you execute <code>ado</code> commands like <code>get</code> or <code>describe</code> they are interacting with this metastore.</p> <p>By hosting a metastore on a dedicated server <code>ado</code> can be used by multiple distributed users.</p> <p>Info</p> <p>The <code>ado</code> CLI can create local metastore instances. Shared metastores require separately provisioning the database server.</p>"},{"location":"resources/metastore/#contexts-and-projects","title":"Contexts and Projects","text":"<p>An instance of the metastore can host one or more <code>projects</code>. To access a <code>project</code> you create a <code>context</code> which contains location information, and optionally access credentials, for the <code>project</code>.</p>"},{"location":"resources/metastore/#contexts-for-local-projects","title":"Contexts for local projects","text":"<p>Local projects are stored in local metastores. Local metastores use SQLite. A local metastore can hold a single project. Hence, there is one database per local metastore instance that contains the resources associated with this project.</p> <p>A context for a local metastore looks like:</p> <pre><code>project: local-test\nmetadataStore:\n  path: $HOME/Library/Application Support/ado/databases/local-test.db\n  sslVerify: false\n</code></pre>"},{"location":"resources/metastore/#contexts-for-remote-projects","title":"Contexts for remote projects","text":"<p>Remote projects are stored in remote metastores. Remote metastore uses MySQL. A remote metastore can host multiple projects. Each project is associated with an access-controlled databases that contains the projects resources.</p> <p>Info</p> <p>Everyone with access to the same remote project can see and interact with all the resources in it</p> <p>A context for a remote metastore looks like:</p> <pre><code>project: ft-prod\nmetadataStore:\n  host: 192.168.0.1\n  password: XXXXXXXXXXX\n  port: 32001\n  sslVerify: false\n</code></pre>"},{"location":"resources/metastore/#creating-a-context","title":"Creating a context","text":"<p>To create a local or remote context in <code>ado</code>, create a file with the corresponding YAML definition (see above) and run:</p> <pre><code>ado create context -f $YAML_FILE\n</code></pre> <p>If the context refers to a local project (a local context), a SQLite database created for the project if it doesn't exist. If the context refers to a remote project (a remote context), the MySQL database for the project must have been created separately.</p>"},{"location":"resources/metastore/#listing-available-contexts","title":"Listing available contexts","text":"<p>To see a list of contexts do</p> <pre><code>ado get contexts\n</code></pre> <p>This will output something like</p> <pre><code>                  CONTEXT DEFAULT\n0              finetuning\n1              ap-testing\n2       developer-testing\n3             mascots2024\n4      caikit-testharness\n5    materials-evaluation\n6                 ft-prod       *\n7            unit-testing\n8       your-project-name\n9  resource-store-testing\n</code></pre> <p>Note, the name of the context is the name of the associated project.</p>"},{"location":"resources/metastore/#the-active-context","title":"The active context","text":"<p>To use a context you activate it with:</p> <pre><code>ado context $CONTEXTNAME\n</code></pre> <p>and it becomes the \"Active Context\". All <code>ado</code> commands that interact with the metastore, like <code>get</code>, <code>show</code>, will be directed to the project associated with the active context.</p> <p>Example:</p> <pre><code>$ ado context materials-evaluation\nSuccess! Now using context materials-evaluation\n</code></pre> <p>To remind yourself what the active context is run</p> <pre><code>ado context\n</code></pre> <p>The active context is also denoted by a \"*\" in the output of <code>ado get contexts</code> (see output above).</p> <p>Note</p> <p>Although <code>context</code> appears like resource in <code>ado</code> e.g. you can <code>get</code> contexts, the definition is not stored in the metastore, so it is purely local.</p>"},{"location":"resources/metastore/#deleting-contexts","title":"Deleting contexts","text":"<p>You can delete a context using</p> <pre><code>ado delete context $CONTEXT_NAME\n</code></pre> <p>For remote contexts the delete operation only deletes the context yaml. The underlying MySQL database remains and must be deleted separately.</p> <p>For local contexts, the delete operation prompts if you want to delete the underlying SQLite database, and thus the project. If you opt to delete the project, the data cannot be retrieved. In this case, if you recreate the context a new local database will be created.</p> <p>If you just delete the context, the underlying SQLite database, and hence the project data, remains. In this case, if you recreate the context it will use the existing database.</p>"},{"location":"resources/operation/","title":"operation","text":""},{"location":"resources/operation/#operator-and-operation","title":"<code>operator</code> and <code>operation</code>","text":"<p>An <code>operator</code> is a code module that provides a capability to perform an <code>operation</code> on a <code>discoveryspace</code>. For example the <code>RandomWalk</code> operator provides the capability to perform a random walk <code>operation</code> on a <code>discoveryspace</code>.</p> <p>The <code>operator</code> defines the inputs arguments you can set for its <code>operations</code>.</p> <p>The operators section contains more details about the available <code>operators</code>, what they do and how to use them. This page covers how you create and work with <code>operations</code> using a given <code>operator</code> in general.</p>"},{"location":"resources/operation/#creating-an-operation-resource","title":"Creating an <code>operation</code> resource","text":""},{"location":"resources/operation/#pre-requisites","title":"Pre-requisites","text":"<p>The only pre-requisite to creating an <code>operation</code> is that there is a suitable <code>discoveryspace</code> for it to work on.</p>"},{"location":"resources/operation/#getting-input-options-and-potential-values","title":"Getting input options and potential values","text":"<p>You can get a default input options for an operation along with a description of its fields by executing</p> <pre><code>ado template operation --operator-name $OPERATOR_NAME --include-schema\n</code></pre> <p>In addition, check the operator section for an entry for the particular $OPERATOR_NAME for more detail.</p> <p>Info</p> <p>The <code>ado template operation</code> command will always output the latest arguments schema for the operator. So use this if the online documentation does not seem to work as it may have fallen behind.</p>"},{"location":"resources/operation/#the-operation-configuration-yaml","title":"The <code>operation</code> configuration YAML","text":"<p>The configuration for all <code>operations</code> has three top-level fields</p> <pre><code>spaces: # This list of spaces to operate on\n  - space_abc123\noperation:\n  module: # Describes the operator to use - more below\n    ...\n  parameters: # The input arguments for this operation\nactuatorConfigurationIdentifiers: # Optional\n  - actuatorconfiguration-name-123\n</code></pre> <p>Each <code>operator</code> has different input arguments. As a result the fields under <code>operation.parameters</code> of every operation will be different. To get an initial set of parameter values, use one of the following methods</p> <ul> <li>Follow the previous section and use <code>ado template operation</code></li> <li>Check if there is an entry for the <code>operator</code> in the   operator section of the   documentation website.</li> <li>Check the examples section to see if there is an   example of using the given <code>operator</code></li> </ul> <p>For illustrative purposes here is a configuration for a random walk operation</p> <pre><code>spaces: ###The spaces to operate on\n  - 'space-630588-bfebfe'\noperation: #The operators\n  module: # The operator will be random_walk\n    operatorName: random_walk\n    operationType: search\n  parameters: # The parameters for this RandomWalk operation\n    numberEntities: 60\n    batchSize: 1\n    mode: 'sequential'\n    samplerType: 'generator'\nactuatorConfigurationIdentifiers:\n  - actuatorconfiguration-st4sd-3cb3bb82\n</code></pre>"},{"location":"resources/operation/#specifying-the-operator","title":"Specifying the <code>operator</code>","text":"<p>The command <code>ado get operators</code> will list the names and types of the known operators e.g.</p> <pre><code>      OPERATOR     TYPE\n0  random_walk  explore\n1     ray_tune  explore\n2     rifferla   modify\n</code></pre> <p>You use this information to specify the operator to use in the operation YAML, for example:</p> <pre><code>module:\n  operatorName: rifferla # The name of the operator\n  operationType: modify #The type of the operation/operator\n</code></pre> <p>Warning</p> <p>To use operators listed with type explore by <code>ado get operators</code>, currently you must set operationType to search e.g.</p> <pre><code>module: \n operatorName: random_walk \n operationType: search # note: search not explore\n</code></pre>"},{"location":"resources/operation/#passing-actuator-parameters","title":"Passing actuator parameters","text":"<p>If you need to provide parameters to the actuators defined on the space, you can do it via the <code>actuatorConfigurationIdentifiers</code> field.</p> <p>For example, the below shows an operation specifying an actuator configuration for the <code>st4sd</code> actuator.</p> <pre><code>spaces: ###The spaces to operate on\n  - \"space-630588-bfebfe\"\noperation: #The operators\n  ... # The operation config\nactuatorConfigurationIdentifiers:\n  - actuatorconfiguration-st4sd-3cb3bb82\n</code></pre> <p>The actuator configurations provided must belong to one of the actuators defined on the space(s) operated on and only one actuator configuration can be provided for each actuator. Both these conditions are checked on creating the <code>operation</code> resource and an appropriate error message is output if they are not.</p>"},{"location":"resources/operation/#starting-the-operation","title":"Starting the operation","text":"<p>Executing</p> <pre><code>ado create operation -f OPERATION.YAML\n</code></pre> <p>will create the <code>operation</code> resource in the active context and start performing whatever that operation does. The operation executes synchronously with this command i.e. it will not return until the operation is complete.</p> <p>Before creating, and hence starting, the <code>operation</code>, the <code>parameters</code> will be validated with the <code>operator</code>. If they are not valid e.g. a required argument is missing, the <code>create</code> operation will fail with a relevant error.</p> <p>Info</p> <p>Although the <code>create</code> command does not exit until the operation is finished distributed users querying the same context will be able to see the created operation, from the moment it is created.</p>"},{"location":"resources/operation/#operation-resource-specific-fields","title":"<code>operation</code> resource specific fields","text":"<p><code>operation</code> resources have two additional top-level fields in addition to common ones described in resources These are (with example values):</p> <pre><code>operationType: characterize # The type of the operation: characterize, modify etc.\noperatorIdentifier: profile-1.1 # The identifier of the operator including its version\n</code></pre>"},{"location":"resources/operation/#getting-operation-output","title":"Getting <code>operation</code> output","text":"<p>An operation can create any type of ado resource. To see the resources created by an operation with identifier $OPERATION_IDENTIFIER use:</p> <pre><code>ado show related operation $OPERATION_IDENTIFIER\n</code></pre> <p>Output that is text, tables or locations will be contained in a <code>datacontainer</code> resource. It can be output with:</p> <pre><code>ado describe datacontainer $DATACONTAINER_IDENTIFIER\n</code></pre> <p>See datacontainers for more details.</p>"},{"location":"resources/operation/#operation-status-update-events","title":"<code>operation</code> status update events","text":"<p>In addition to the status update events common to all resources <code>operations</code> define two more events: <code>started</code> and <code>finished</code>.</p> <p>The finished event also has a custom field <code>exit_state</code> which described how the operation finished. It can be <code>success</code>, <code>fail</code> or <code>error</code>.</p> <p>Here is an example of the status field of an operation resource after it has completed</p> <pre><code>created: \"2024-12-19T10:54:42.015388Z\"\nidentifier: raytune-0.7.5.dev10+g731d1e21.d20241218-ax-018647\nkind: operation\nmetadata:\n  entities_submitted: 11\n  experiments_requested: 11\noperationType: search\noperatorIdentifier: raytune-0.7.5.dev10+g731d1e21.d20241218\nstatus:\n  - event: created\n    recorded_at: \"2025-09-02T09:09:02.187149Z\"\n  - event: added\n    recorded_at: \"2025-09-02T09:09:15.187747Z\"\n  - event: started\n    recorded_at: \"2025-09-02T09:09:15.190337Z\"\n  - event: updated\n    recorded_at: \"2025-09-02T09:09:15.190383Z\"\n  - event: finished\n    exit_state: success\n    message: Ray Tune operation completed successfully\n    recorded_at: \"2025-09-02T09:11:28.047676Z\"\n  - event: updated\n    recorded_at: \"2025-09-02T09:11:29.096685Z\"\nversion: v1\n</code></pre> <p>Note</p> <p>The first updated status reflects an update to the stored operation resource which added the <code>started</code> event. The second updated status reflects an update to the stored operation resource which added the <code>finished</code> event.</p>"},{"location":"resources/operation/#deleting-operations","title":"Deleting operations","text":"<p>Info</p> <p>Please note that standard deletion constraints apply alongside the considerations discussed in this section.</p> <p>Warning</p> <p>Deleting an operation that uses the replay actuator will also remove the original measurements from the database, making it impossible for it to be run again on the same sample store.</p> <p>Deleting an operation is a destructive action that also removes associated data. The deletion process follows this logic:</p> <ul> <li>Requests associated with the operation are deleted if their results are not   referenced by other operations</li> <li>Results are deleted only if they are no longer referenced by any remaining   requests.</li> </ul> <p>This means that if a measurement result has been reused (e.g., replayed) in another operation, it will not be deleted.</p>"},{"location":"resources/operation/#restrictions","title":"Restrictions","text":"<p>By default, operations cannot be deleted while other operations are still running. This safeguard prevents potential consistency issues, such as deleting results that are actively being replayed in a running operation.</p> <p>To override this restriction, you can use the --force option, which allows deletion even when other operations are in progress.</p>"},{"location":"resources/resources/","title":"Resources","text":"<p>Info</p> <p>We recommend to first familiarise yourself with the core-concepts before reading about resources.</p> <p><code>ado</code> manages resources related to discovery, for example descriptions of spaces to explore, exploration and analysis operations, and actuator configurations. It allows you to create resources, which it stores in a database (the metastore) along with their relationships to other resources. You can then describe, list or delete those resources.</p> <p>The resources are (use side panel to fine out more about each type):</p> <ul> <li>samplestore: A bucket used to store entities</li> <li>discoveryspace: Describes a set of entities along with the experiment   protocols that should be applied to them</li> <li>operation: An instance of applying an operator to a DiscoverySpace. For   example, running an optimization</li> <li>datacontainer: A collection of string, tabular or location data. Used to   store arbitrary output from <code>operation</code>'s.</li> <li>actuatorconfigurations: A configuration for an actuator.</li> </ul> <p>Note</p> <p>Some resources take other resources as input, for example <code>operations</code> take <code>discoveryspaces</code> as input.</p>"},{"location":"resources/resources/#naming-conventions-concepts-versus-resources","title":"Naming Conventions: Concepts versus Resources","text":"<p><code>ado</code> resources are directly related to <code>ado</code> concepts and usually have the same name. To differentiate a concept and the associated resource in the documentation we adopt the following conventions.</p> <p>When we refer to concepts, upper case nouns like \"Sample Store\", \"Actuator\" are used. However, for the corresponding resources lower case is used, with no spaces, so <code>samplestore</code> and <code>actuator</code>.</p> <p>We also apply the same approach to <code>entities</code>, although these are not properly resources. See below for more.</p>"},{"location":"resources/resources/#actuators-operators-and-contexts","title":"<code>actuators</code>, <code>operators</code> and <code>contexts</code>","text":"<p>Many <code>ado</code> commands work with <code>actuators</code>, <code>operators</code> and <code>contexts</code> as if they were resources. However, they are not true resources and are not stored in the metastore.</p> <ul> <li>For more on <code>actuators</code> see   working with actuators.</li> <li>For more on <code>operators</code> see   working with operators.</li> <li>For more on <code>contexts</code> see the   metastore docs</li> </ul>"},{"location":"resources/resources/#common-cli-commands-for-interacting-with-resources","title":"Common CLI commands for interacting with resources","text":"<p>Here is a list of common <code>ado</code> CLI commands for interacting with resources. See the ado CLI guide for more details</p> <ul> <li><code>ado get [resource type]</code></li> <li>Lists all resource of the requested type</li> <li><code>ado get [resource type] [$identifier] -o yaml</code></li> <li>Outputs the YAML of resource <code>$identifier</code></li> <li><code>ado create [resource type] -f [YAMLFILE]</code></li> <li>Creates the resource of the given type from the definition in \"YAMLFILE\"</li> <li><code>ado delete [resource type] [$identifier]</code></li> <li>Deletes the resource of the given type with the provided identifier from the     database. See the deleting resources section for more     information and considerations to keep in mind.</li> <li><code>ado describe [resource type] [$identifier]</code></li> <li>Outputs a human-readable description of resource <code>$identifier</code></li> <li><code>ado show related [resource type] [$identifier]</code></li> <li>List ids of resources related to resource <code>$identifier</code></li> <li><code>ado show details [resource type] [$identifier]</code></li> <li>Outputs some details on the resource. Usually these are quantities that have     to be computed.</li> <li><code>ado template [resource type] --include-schema</code></li> <li>Outputs a default YAML for the given resource along with a schema file     explaining the fields.`</li> </ul>"},{"location":"resources/resources/#deleting-resources","title":"Deleting resources","text":"<p>Info</p> <p>Refer to the following documentation for detailed information on specific use cases:</p> <pre><code>- [Deleting sample stores](sample-stores.md#deleting-sample-stores)\n- [Deleting operations](operation.md#deleting-operations)\n</code></pre> <p>In ado you can delete resources, but there is an important constraint: a resource cannot be deleted if it has dependent (child) resources.</p> <p>If you attempt to delete a resource that still has children, you will encounter an error similar to the following:</p> <pre><code>ERROR:  Cannot delete discoveryspace space-3bcf27-108c4c as it has children resources:\n\n                                          IDENTIFIER       TYPE\n0  raytune-0.9.2.dev11+gff71d082.d20250520-ax-a44c7b  operation\n\nHINT:   You must delete each of them them first.\n</code></pre> <p>To proceed, ensure that all child resources are deleted (using the <code>ado delete</code> command on them) before attempting to remove the parent resource.</p>"},{"location":"resources/resources/#common-features-of-resources","title":"Common features of resources","text":"<p>All resources have a YAML or JSON representation which is what is stored in the metastore. The schema of this YAML has a common structure.</p> <pre><code>config: ... # The configuration of the resource - different for each different type\ncreated: \"2024-10-03T12:42:35.786484Z\" # Creation date\nidentifier: space-8f1cfb-91ecfb # Resource identifier\nkind: discoveryspace # Resource kind\nmetadata: {} # A field for system metadata. User metadata will be in config.metadata\nstatus: [] # A status field\nversion: v2 # The version of this resource\n</code></pre>"},{"location":"resources/resources/#resource-status","title":"Resource status","text":"<p>The status field of a resource contains an ordered sequence of status updates to it. The most recent update is last. Each status update is associated with an event that occurred to the resource, and this event is captured in the <code>event</code> field. A status update will also have a timestamp, which is when the event was recorded (usually right after it occurred). It can also contain additional event dependent fields.</p> <p>All resources have status updates recorded for the following events:</p> <ul> <li>created: When the resource is created</li> <li>added: When the resource is added to the metastore</li> <li>updated: Whenever the resource is updated in the metastore</li> </ul> <p>Here is an example:</p> <pre><code>created: \"2024-12-19T10:47:03.931824Z\"\nidentifier: 04535d\nkind: samplestore\nmetadata: {}\nstatus:\n  - event: created\n    recorded_at: \"2024-12-19T10:47:03.931840Z\"\n  - event: added\n    recorded_at: \"2024-12-19T10:47:05.720459Z\"\nversion: v2\n</code></pre>"},{"location":"resources/resources/#programmatic-view-of-resources","title":"Programmatic view of resources","text":"<p>Programmatically each resource type is represented by a Python pydantic model class. All these classes inherit their basic structure from the root resource class <code>ADOResource</code>.</p> <p>You can load and validate any <code>ado</code> resource YAML with the following code snippet: replace <code>discoveryspace</code> with the name of the resource as shown in above list</p> <pre><code>import yaml\nfrom orchestrator.core import kindmap\n\nwith open(\"resource.yaml\") as f:\n    resource = kindmap['discoveryspace'].model_validate(yaml.safe_load(f))\n</code></pre>"},{"location":"resources/resources/#where-are-the-entities","title":"Where are the entities?","text":"<p>Note that <code>entities</code> are not a resource <code>ado</code> manages. Instead, you work at the level of sets of <code>entities</code> i.e. a <code>discoveryspace</code>.</p> <ul> <li>A <code>discoveryspace</code> defines a set of <code>entities</code></li> <li>Applying certain <code>operations</code> to a <code>discoveryspace</code> results in <code>entities</code>   being sampled from the space and measurements being applied to them</li> <li>The sampled entities and measurement results are stored in a <code>samplestore</code></li> </ul> <p>You can think of <code>discoveryspaces</code>, <code>samplestores</code> and <code>operations</code> as being (different) \"containers\" of Entities. <code>ado</code> also provides commands to <code>show</code> the Entities that are in those containers.</p>"},{"location":"resources/resources/#whats-next","title":"What's next","text":"<ul> <li> <p> Try our examples</p> <p>Explore working with resources via our examples.</p> <p>Our examples </p> </li> <li> <p> Learn about the ado CLI tool</p> <p>Learn more about the ado CLI tool for interacting with resources.</p> <p>ado CLI </p> </li> </ul>"},{"location":"resources/sample-stores/","title":"samplestore","text":"<p>A <code>samplestore</code> resource is a database containing <code>entities</code> along with result of experiments that have been applied to them.</p>"},{"location":"resources/sample-stores/#samplestores-and-discoveryspaces","title":"<code>samplestore</code>s and <code>discoveryspace</code>s","text":"<p>When you create a discovery space you associate a <code>samplestore</code> with it. This is where the <code>discoveryspace</code> will read and write data i.e. entities and the results of experiments on them. You primarily access the entities in a <code>samplestore</code> via a <code>discoveryspace</code> that is attached to it.</p> <p>You can think of a <code>discoveryspace</code> as a view or filter on a sample store - when you access data in a <code>samplestore</code> through a discovery space you only see data that matches the <code>discoveryspace</code>.</p> <p>Tip</p> <ul> <li>Multiple <code>discoveryspace</code>s can use the same <code>samplestore</code></li> <li>There is no restriction or condition on the <code>discoveryspace</code>s sharing a   <code>samplestore</code> i.e. they can be very similar or completely different.</li> </ul>"},{"location":"resources/sample-stores/#samplestores-and-data-sharing","title":"<code>samplestore</code>s and data-sharing","text":"<p>When multiple <code>discoveryspace</code>s use the same <code>samplestore</code> this enables transparent data-sharing between the different <code>discoveryspace</code>s. When and how data is shared is covered in detail in shared sample stores.</p> <p>To see the <code>discoveryspaces</code> using a given <code>samplestore</code> run</p> <pre><code>ado show related samplestore $SAMPLE_STORE_IDENTIFIER\n</code></pre> <p>The greater the similarity between two <code>discoveryspace</code>s, the greater</p> <p>the chance they can share data. So it is usually beneficial to ensure that such <code>discoveryspace</code>s use the same <code>samplestore</code>.</p> <p>Warning</p> <p>If you use two different <code>samplestore</code>s for similar <code>discoveryspace</code>s there is no way to share results between them.</p>"},{"location":"resources/sample-stores/#active-and-passive-sample-stores","title":"active and passive Sample Stores","text":"<p><code>ado</code> distinguishes two types of Sample Stores: active Sample Stores which allow read and write; and passive Sample Stores that only have read capabilities (for example a CSV file containing measurement data).</p> <p>All <code>samplestore</code> resources created with <code>ado</code> will be `active. However, they can copy data in from passive Sample Stores.</p>"},{"location":"resources/sample-stores/#the-primary-sample-store-type-sqlsamplestore","title":"The primary Sample Store type: SQLSampleStore","text":"<p>The primary Sample Store used in <code>ado</code>, and represented by <code>samplestore</code> resources, is SQLSampleStore. SQLSampleStore represents storage in SQL tables. When you create a <code>samplestore</code> resource that uses SQLSampleStore the storage is allocated automatically in the SQL db associated with the active context</p>"},{"location":"resources/sample-stores/#creating-a-samplestore","title":"Creating a samplestore","text":"<p>Running <code>ado create samplestore --new-sample-store</code> will create an empty SQLSampleStore in the current context.</p>"},{"location":"resources/sample-stores/#copying-data-into-a-samplestore","title":"Copying data into a samplestore","text":"<p>You can specify data to be copied into a new <code>samplestore</code> resource on creation. The data comes from other Sample Stores. The general structure of the YAML when copying from other sample stores is:</p> <pre><code>specification:\n  module:\n    moduleClass: SQLSampleStore\n    moduleName: orchestrator.core.samplestore.sql\ncopyFrom: # An array of Sample Stores data will be copied from\n  - identifier: # Optional, the id of the Sample Store if not given in the storageLocation\n    module: # The type of this Sample Store\n      moduleClass: ... # The module class for this sample store\n      moduleName: ... # The name of the module containing the class\n    parameters: # Sample Store parameters\n    storageLocation: # The location of this Sample Store\n</code></pre> <p>The Sample Store types section details how to fill the above fields for the different available Sample Store. Here is an example of copying data from a CSV file using <code>CSVSampleStore</code>:</p> <pre><code>specification:\n  module:\n    moduleName: orchestrator.core.samplestore.sql\n    moduleClass: SQLSampleStore\ncopyFrom:\n  - module:\n      moduleClass: CSVSampleStore\n    storageLocation:\n      path: \"examples/ml-multi-cloud/ml_export.csv\"\n    parameters:\n      generatorIdentifier: \"multi-cloud-ml\"\n      identifierColumn: \"config\"\n      experiments:\n        - experimentIdentifier: \"benchmark_performance\"\n          propertyMap:\n            wallClockRuntime: \"wallClockRuntime\"\n</code></pre>"},{"location":"resources/sample-stores/#accessing-the-entities-in-a-sample-store","title":"Accessing the entities in a sample store","text":"<p>You access the entities in a <code>samplestore</code> via a discovery space attached to it.</p> <p>For an existing <code>discoveryspace</code> to retrieve all entities that match it run</p> <pre><code>ado show entities space $SPACEID --include matching\n</code></pre> <p>You can also define a <code>discoveryspace</code> in a YAML and run:</p> <pre><code>ado show entities space --file $FILE\n</code></pre> <p>This allows you to see what entities match a space without having to create it.</p>"},{"location":"resources/sample-stores/#sample-store-types","title":"Sample Store types","text":""},{"location":"resources/sample-stores/#sqlsamplestore","title":"SQLSampleStore","text":"<p>This is an active Sample Store that stores entity data in SQL tables. In <code>ado</code> a SQLSampleStore is always associated with a particular project.</p> <p>When you want to copy from another SQLSampleStore you need the identifier and the metastore URL to the the project it is in</p> <pre><code>copyFrom:\n  - identifier: source_abc123\n    module:\n      moduleClass: SQLSampleStore\n      moduleName: orchestrator.core.samplestore.sql\n    storageLocation:\n      host: localhost\n      port: 30002\n      database: my_project. # The database field is the name of the project containing the samplestore\n      user: my_project # The user field is the name of the project containing the samplestore\n      password: XXXXXXX\n</code></pre>"},{"location":"resources/sample-stores/#csvsamplestore","title":"CSVSampleStore","text":"<p>This is a passive Sample Store that can be used to extract entities from a CSV file. Its assumed each row is an entity and the columns are constitutive properties or observed properties</p> <pre><code>copyFrom:\n  - module:\n      moduleClass: CSVSampleStore\n      moduleName: orchestrator.core.samplestore.csv\n    storageLocation:\n      path: 'examples/ml-multi-cloud/ml_export.csv'. # The path to the CSV file\n    parameters:\n      generatorIdentifier: 'multi-cloud-ml' # A string that will be stored with the extracted entities as their generatorIdentifier\n      identifierColumn: 'config'. # The column in the CSV file that contains the entity id\n      constitutivePropertyColumns:\n        -  # A list of columns which contain constitutive properties\n      experiments: # A list of dictionaries that map CSV columns to experiments and target properties. Each dictionary is an experiment\n        - experimentIdentifier: 'benchmark_performance' # The experiment name you want the following properties to be associated with\n          propertyMap: # List of target property name:column id pairs\n            wallClockRuntime: 'wallClockRuntime' # The key is the target property name, the value is the column containing the values for that property\n</code></pre> <p>Note, since CSV have arbitrary data in general there is no way <code>ado</code> can know how a particular value was generated or how to generate new such values. However, the measurements in a CSV can be mapped to the \"experiment+property\" model that <code>ado</code> uses, if you want to copy them.</p> <p>You do not have to copy all the columns in a CSV or have any experiments.</p>"},{"location":"resources/sample-stores/#deleting-sample-stores","title":"Deleting sample stores","text":"<p>Info</p> <p>Please note that   standard deletion constraints apply alongside   the considerations discussed in this section.</p> <p>Deleting a sample store is a high-impact operation and should be performed with caution. When a sample store is deleted:</p> <ul> <li>All stored entities will be deleted.</li> <li>All leftover measurement results stored in it will be permanently deleted.   These will be measurements that were copied into the <code>samplestore</code> i.e., not   generated through <code>ado</code> operations. All results from <code>ado</code> operations would   have already been subject to   standard deletion constraints.</li> <li>The corresponding database tables will be dropped.</li> </ul> <p>This is especially critical when the sample store was populated externally, such as via a <code>CSVSampleStore</code>. In such cases, deletion may result in the loss of externally sourced data that cannot be recovered unless it has been backed up beforehand.</p> <p>To prevent this from unintentionally happening, <code>ado</code> will check if the sample store contains stored results, and exit if this is the case. A warning such as the following will be output:</p> <p>ERROR: Cannot delete sample store 995ff6 because there are 68 measurement results present in the sample store.</p> <p>HINT: You can force the deletion by adding the --force flag.</p> <p>If this is expected, re-running the same command while adding the <code>--force</code> flag according to the hint will perform the deletion.</p>"}]}