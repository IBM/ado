<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Documentation website for https://github.com/ibm/ado"><meta name=author content="The ado authors"><link href=https://ibm.github.io/ado/actuators/sft-trainer/ rel=canonical><link href=../replay/ rel=prev><link href=../../operators/working-with-operators/ rel=next><link rel=icon href=../../favicon.ico><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.18"><title>SFTTrainer - measure fine-tuning performance - ado Documentation</title><link rel=stylesheet href=../../assets/stylesheets/main.7e37652d.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&display=swap"><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#overview class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="ado Documentation" class="md-header__button md-logo" aria-label="ado Documentation" data-md-component=logo> <img src=../../logo-ibm.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ado Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> SFTTrainer - measure fine-tuning performance </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=black data-md-color-accent=blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=blue aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ibm/ado title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> ibm/ado </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../examples/examples/ class=md-tabs__link> Examples </a> </li> <li class=md-tabs__item> <a href=../../core-concepts/concepts/ class=md-tabs__link> Core Concepts </a> </li> <li class=md-tabs__item> <a href=../../resources/resources/ class=md-tabs__link> Resources </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../working-with-actuators/ class=md-tabs__link> Actuators </a> </li> <li class=md-tabs__item> <a href=../../operators/working-with-operators/ class=md-tabs__link> Operators </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="ado Documentation" class="md-nav__button md-logo" aria-label="ado Documentation" data-md-component=logo> <img src=../../logo-ibm.png alt=logo> </a> ado Documentation </label> <div class=md-nav__source> <a href=https://github.com/ibm/ado title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> ibm/ado </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../.. class="md-nav__link "> <span class=md-ellipsis> Getting Started </span> <span class="md-status md-status--published"></span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../getting-started/demo/ class=md-nav__link> <span class=md-ellipsis> Demo </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/install/ class=md-nav__link> <span class=md-ellipsis> Installation </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/ado/ class=md-nav__link> <span class=md-ellipsis> The ado CLI </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/installing-backend-services/ class=md-nav__link> <span class=md-ellipsis> Installing backend services </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/remote_run/ class=md-nav__link> <span class=md-ellipsis> Running ado on remote ray clusters </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/developing/ class=md-nav__link> <span class=md-ellipsis> Developing ado </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/roadmap/ class=md-nav__link> <span class=md-ellipsis> Roadmap </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/contributing/ class=md-nav__link> <span class=md-ellipsis> Contributing </span> <span class="md-status md-status--published"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../examples/examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> <span class="md-status md-status--draft"></span> </a> </li> <li class=md-nav__item> <a href=../../examples/random-walk/ class=md-nav__link> <span class=md-ellipsis> Taking a random walk </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../examples/best-configuration-search/ class=md-nav__link> <span class=md-ellipsis> Search a space with an optimizer </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../examples/search-custom-objective/ class=md-nav__link> <span class=md-ellipsis> Search based on a custom objective function </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../examples/lhu/ class=md-nav__link> <span class=md-ellipsis> Identify the important dimensions of a space </span> <span class="md-status md-status--draft"></span> </a> </li> <li class=md-nav__item> <a href=../../examples/finetune-locally/ class=md-nav__link> <span class=md-ellipsis> Measure throughput of fine-tuning locally </span> </a> </li> <li class=md-nav__item> <a href=../../examples/finetune-remotely/ class=md-nav__link> <span class=md-ellipsis> Measure throughput of fine-tuning on a remote RayCluster </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Core Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Core Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../core-concepts/concepts/ class=md-nav__link> <span class=md-ellipsis> Concepts </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../core-concepts/entity-spaces/ class=md-nav__link> <span class=md-ellipsis> Entities and Entity Spaces </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../core-concepts/actuators/ class=md-nav__link> <span class=md-ellipsis> Actuators, Experiments & Measurement Spaces </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../core-concepts/discovery-spaces/ class=md-nav__link> <span class=md-ellipsis> Discovery Spaces </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../core-concepts/data-sharing/ class=md-nav__link> <span class=md-ellipsis> Shared Sample Stores </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../resources/resources/ class=md-nav__link> <span class=md-ellipsis> Resources </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/metastore/ class=md-nav__link> <span class=md-ellipsis> The metastore </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/sample-stores/ class=md-nav__link> <span class=md-ellipsis> samplestore </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/discovery-spaces/ class=md-nav__link> <span class=md-ellipsis> discoveryspace </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/operation/ class=md-nav__link> <span class=md-ellipsis> operation </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/actuatorconfig/ class=md-nav__link> <span class=md-ellipsis> actuatorconfiguration </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../resources/datacontainer/ class=md-nav__link> <span class=md-ellipsis> datacontainer </span> <span class="md-status md-status--published"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Actuators </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Actuators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../working-with-actuators/ class=md-nav__link> <span class=md-ellipsis> Actuators overview </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../creating-actuator-classes/ class=md-nav__link> <span class=md-ellipsis> Extending `ado` with new actuators </span> <span class="md-status md-status--published #Status can be draft, reviewed or published."></span> </a> </li> <li class=md-nav__item> <a href=../creating-custom-experiments/ class=md-nav__link> <span class=md-ellipsis> Adding custom experiments </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../replay/ class=md-nav__link> <span class=md-ellipsis> Using externally obtained data </span> <span class="md-status md-status--draft"></span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> SFTTrainer - measure fine-tuning performance </span> <span class="md-status md-status--draft"></span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> SFTTrainer - measure fine-tuning performance </span> <span class="md-status md-status--draft"></span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#available-experiments class=md-nav__link> <span class=md-ellipsis> Available experiments </span> </a> <nav class=md-nav aria-label="Available experiments"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#finetune_full_benchmark-v100 class=md-nav__link> <span class=md-ellipsis> finetune_full_benchmark-v1.0.0 </span> </a> <nav class=md-nav aria-label=finetune_full_benchmark-v1.0.0> <ul class=md-nav__list> <li class=md-nav__item> <a href=#full-finetuning-requirements class=md-nav__link> <span class=md-ellipsis> Full Finetuning Requirements </span> </a> </li> <li class=md-nav__item> <a href=#full-finetuning-entity-space class=md-nav__link> <span class=md-ellipsis> Full Finetuning Entity space </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#finetune_full_stability-v100 class=md-nav__link> <span class=md-ellipsis> finetune_full_stability-v1.0.0 </span> </a> <nav class=md-nav aria-label=finetune_full_stability-v1.0.0> <ul class=md-nav__list> <li class=md-nav__item> <a href=#full-finetuning-stability-requirements class=md-nav__link> <span class=md-ellipsis> Full Finetuning (Stability) Requirements </span> </a> </li> <li class=md-nav__item> <a href=#full-finetuning-stability-entity-space class=md-nav__link> <span class=md-ellipsis> Full Finetuning (Stability) Entity space </span> </a> </li> <li class=md-nav__item> <a href=#full-finetuning-stability-measured-properties class=md-nav__link> <span class=md-ellipsis> Full Finetuning (Stability) Measured properties </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#finetune_lora_benchmark-v100 class=md-nav__link> <span class=md-ellipsis> finetune_lora_benchmark-v1.0.0 </span> </a> <nav class=md-nav aria-label=finetune_lora_benchmark-v1.0.0> <ul class=md-nav__list> <li class=md-nav__item> <a href=#lora-requirements class=md-nav__link> <span class=md-ellipsis> LoRA Requirements </span> </a> </li> <li class=md-nav__item> <a href=#lora-entity-space class=md-nav__link> <span class=md-ellipsis> LoRA Entity space </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#finetune_pt_benchmark-v100 class=md-nav__link> <span class=md-ellipsis> finetune_pt_benchmark-v1.0.0 </span> </a> <nav class=md-nav aria-label=finetune_pt_benchmark-v1.0.0> <ul class=md-nav__list> <li class=md-nav__item> <a href=#prompt-tuning-requirements class=md-nav__link> <span class=md-ellipsis> Prompt Tuning Requirements </span> </a> </li> <li class=md-nav__item> <a href=#prompt-tuning-entity-space class=md-nav__link> <span class=md-ellipsis> Prompt Tuning Entity space </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#finetune_gtpq-lora_benchmark-v100 class=md-nav__link> <span class=md-ellipsis> finetune_gtpq-lora_benchmark-v1.0.0 </span> </a> <nav class=md-nav aria-label=finetune_gtpq-lora_benchmark-v1.0.0> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gptq-lora-requirements class=md-nav__link> <span class=md-ellipsis> GPTQ LoRA Requirements </span> </a> </li> <li class=md-nav__item> <a href=#gptq-lora-entity-space class=md-nav__link> <span class=md-ellipsis> GPTQ LoRA Entity space </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#actuator-parameters class=md-nav__link> <span class=md-ellipsis> Actuator Parameters </span> </a> </li> <li class=md-nav__item> <a href=#example-actuator-configuration-yaml class=md-nav__link> <span class=md-ellipsis> Example Actuator Configuration YAML </span> </a> </li> <li class=md-nav__item> <a href=#configuration-fields class=md-nav__link> <span class=md-ellipsis> Configuration Fields </span> </a> <nav class=md-nav aria-label="Configuration Fields"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#match_exact_dependencies-bool-default-true class=md-nav__link> <span class=md-ellipsis> match_exact_dependencies (bool, default: true) </span> </a> </li> <li class=md-nav__item> <a href=#output_dir-str-default-output class=md-nav__link> <span class=md-ellipsis> output_dir (str, default: "output") </span> </a> </li> <li class=md-nav__item> <a href=#data_directory-str-default-datafms-hf-tuningartificial-dataset class=md-nav__link> <span class=md-ellipsis> data_directory (str, default: "/data/fms-hf-tuning/artificial-dataset/") </span> </a> </li> <li class=md-nav__item> <a href=#aim_db-str-default-none class=md-nav__link> <span class=md-ellipsis> aim_db (str, default: None) </span> </a> </li> <li class=md-nav__item> <a href=#aim_dashboard_url-str-or-null-optional class=md-nav__link> <span class=md-ellipsis> aim_dashboard_url (str or null, optional) </span> </a> </li> <li class=md-nav__item> <a href=#hf_home-str-default-hf-models-pvchuggingface_home class=md-nav__link> <span class=md-ellipsis> hf_home (str, default: "/hf-models-pvc/huggingface_home") </span> </a> </li> <li class=md-nav__item> <a href=#model_map-dict-optional class=md-nav__link> <span class=md-ellipsis> model_map (dict, optional) </span> </a> </li> <li class=md-nav__item> <a href=#num_tokens_cache_directory-str-or-null-default-cache class=md-nav__link> <span class=md-ellipsis> num_tokens_cache_directory (str or null, default: "cache") </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#measured-properties class=md-nav__link> <span class=md-ellipsis> Measured Properties </span> </a> <nav class=md-nav aria-label="Measured Properties"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#validation class=md-nav__link> <span class=md-ellipsis> Validation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configure-your-raycluster class=md-nav__link> <span class=md-ellipsis> Configure your RayCluster </span> </a> <nav class=md-nav aria-label="Configure your RayCluster"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#annotating-gpu-workers-with-custom-resources class=md-nav__link> <span class=md-ellipsis> Annotating GPU workers with custom resources </span> </a> <nav class=md-nav aria-label="Annotating GPU workers with custom resources"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#custom-resource-types class=md-nav__link> <span class=md-ellipsis> Custom Resource Types </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#creating-the-datasets class=md-nav__link> <span class=md-ellipsis> Creating the datasets </span> </a> <nav class=md-nav aria-label="Creating the datasets"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dataset-for-text-to-text-tasks class=md-nav__link> <span class=md-ellipsis> Dataset for text-to-text tasks </span> </a> </li> <li class=md-nav__item> <a href=#dataset-for-image-to-text-tasks class=md-nav__link> <span class=md-ellipsis> Dataset for image-to-text tasks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-weights class=md-nav__link> <span class=md-ellipsis> Model Weights </span> </a> </li> <li class=md-nav__item> <a href=#configure-your-raycluster-for-rdma-over-converged-ethernet-roce class=md-nav__link> <span class=md-ellipsis> Configure your RayCluster for RDMA over Converged Ethernet (RoCE) </span> </a> <nav class=md-nav aria-label="Configure your RayCluster for RDMA over Converged Ethernet (RoCE)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#install-the-required-libraries-and-drivers class=md-nav__link> <span class=md-ellipsis> Install the required libraries and drivers </span> </a> </li> <li class=md-nav__item> <a href=#collect-all-necessary-information class=md-nav__link> <span class=md-ellipsis> Collect all necessary information </span> </a> <nav class=md-nav aria-label="Collect all necessary information"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#identify-roce-capable-network-devices class=md-nav__link> <span class=md-ellipsis> Identify RoCE-Capable Network Devices </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#putting-it-all-together class=md-nav__link> <span class=md-ellipsis> Putting it all together </span> </a> <nav class=md-nav aria-label="Putting it all together"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#summary-of-steps class=md-nav__link> <span class=md-ellipsis> Summary of Steps </span> </a> </li> <li class=md-nav__item> <a href=#verify-youre-using-roce class=md-nav__link> <span class=md-ellipsis> Verify you're using RoCE: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next steps: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Operators </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../operators/working-with-operators/ class=md-nav__link> <span class=md-ellipsis> Operators overview </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../operators/explore_operators/ class=md-nav__link> <span class=md-ellipsis> Explore operators </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../operators/creating-operators/ class=md-nav__link> <span class=md-ellipsis> Extending `ado` with new Operators </span> <span class="md-status md-status--draft"></span> </a> </li> <li class=md-nav__item> <a href=../../operators/random-walk/ class=md-nav__link> <span class=md-ellipsis> The Random Walk Operator </span> <span class="md-status md-status--published"></span> </a> </li> <li class=md-nav__item> <a href=../../operators/optimisation-with-ray-tune/ class=md-nav__link> <span class=md-ellipsis> The Ray Tune Operator </span> <span class="md-status md-status--draft"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/ibm/ado/edit/main/website/docs/actuators/sft-trainer.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.06 9 .94.94L5.92 19H5v-.92zm3.6-6c-.25 0-.51.1-.7.29l-1.83 1.83 3.75 3.75 1.83-1.83c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.2-.2-.45-.29-.71-.29m-3.6 3.19L3 17.25V21h3.75L17.81 9.94z"/></svg> </a> <h1>SFTTrainer - measure fine-tuning performance</h1> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">&para;</a></h2> <p>The <code>SFTTrainer</code> actuator provides a flexible and scalable interface for running supervised fine-tuning (SFT) experiments on large language and vision-language models. It supports a variety of fine-tuning strategies including full fine-tuning, LoRA, QPTQ-LoRA, and prompt-tuning across both text-to-text and image-to-text datasets.</p> <p>Designed for high-performance and distributed environments, <code>SFTTrainer</code> supports:</p> <ul> <li><strong>Single-GPU</strong>, <strong>multi-GPU</strong>, and <strong>multi-node</strong> training</li> <li><strong>Distributed Data Parallel (DDP)</strong> and <strong>Fully Sharded Data Parallel (FSDP)</strong> strategies</li> <li><strong>RDMA over Converged Ethernet (RoCE)</strong> for optimized multi-node communication</li> <li><strong>Ray-based task scheduling</strong>, enabling execution on both Kubernetes clusters and bare-metal infrastructure</li> </ul> <p>Under the hood, this actuator wraps the <a href=https://github.com/foundation-model-stack/fms-hf-tuning>fms-hf-tuning</a> library, which itself builds on the <a href=https://huggingface.co/docs/trl/sft_trainer><code>SFTTrainer</code> API from Hugging Face Transformers</a>. This layered design allows users to leverage the robustness of the Hugging Face ecosystem while benefiting from ado’s orchestration and reproducibility features.</p> <h2 id=available-experiments>Available experiments<a class=headerlink href=#available-experiments title="Permanent link">&para;</a></h2> <p>The <code>SFTTrainer</code> actuator includes a set of experiments that evaluate different fine-tuning strategies under controlled conditions. These experiments use artificial datasets to ensure reproducibility and comparability across runs. A full list of available experiments and their configurations is available in the <a href=https://github.com/ibm/ado/tree/main/plugins/actuators/sfttrainer/ado_actuators/sfttrainer>README.MD file</a> of the Actuator.</p> <p>The most frequently used experiments are:</p> <h3 id=finetune_full_benchmark-v100>finetune_full_benchmark-v1.0.0<a class=headerlink href=#finetune_full_benchmark-v100 title="Permanent link">&para;</a></h3> <p>Performs full fine-tuning of all model parameters. This experiment is ideal for evaluating end-to-end training performance and resource utilization on large models.</p> <details class=note> <summary>Experiment documentation</summary> <p>An experiment instance:</p> <ul> <li>performs full fine tuning</li> <li>You may notice that even large-memory GPUs like the 80GB variant of the NVIDIA A100 chip need at least 2 GPUs to train models as big as 13B parameters.</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default, can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>fsdp_config</span><span class=p>:</span>
<span class=w>  </span><span class=nt>fsdp_auto_wrap_policy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class=w>  </span><span class=nt>fsdp_backward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class=w>  </span><span class=nt>fsdp_forward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_offload_params</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_sharding_strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_sharding_strategy}</span>
<span class=w>  </span><span class=nt>fsdp_state_dict_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_state_dict_type}</span>
<span class=w>  </span><span class=nt>fsdp_cpu_ram_efficient_loading</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_sync_module_states</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_transformer_layer_cls_to_wrap</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_fsdp_transformer_layer_cls_to_wrap}</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>For DDP runs we use this instead:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>debug</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">no</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>Commandline:</p> <div class=highlight><pre><span></span><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \
  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \
  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \
  --response_template &quot;\n### Response:&quot; --dataset_text_field output --log_level debug \
  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \
  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \
  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \
  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \
  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \
  --packing False --peft_method none --optim ${OPTIM} --bf16 ${BF16} \
  --gradient_checkpointing_kwargs=&#39;{&quot;use_reentrant&quot;: ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}&#39; \
  --fast_moe ${FAST_MOE}
</code></pre></div> <p><strong>Note</strong>: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:<ul> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul> </li> </ul> <h4 id=full-finetuning-requirements>Full Finetuning Requirements<a class=headerlink href=#full-finetuning-requirements title="Permanent link">&para;</a></h4> <ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the models:<ul> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> </ul> </li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic datasets of the SFTTrainer actuator</li> </ul> <h4 id=full-finetuning-entity-space>Full Finetuning Entity space<a class=headerlink href=#full-finetuning-entity-space title="Permanent link">&para;</a></h4> <p>Required:</p> <ul> <li>model_name: Supported models: <code>["granite-3b-1.5", "hf-tiny-model-private/tiny-random-BloomForCausalLM", "llama-7b", "granite-13b-v2", "llama-13b", "granite-20b-v2", "granite-7b-base", "granite-8b-japanese", "granite-8b-code-base", "granite-34b-code-base", "mistral-7b-v0.1", "llama3-8b", "llama3-70b", "mixtral-8x7b-instruct-v0.1", "llama2-70b", "llama3.1-8b", "llama3.1-70b", "llama3.1-405b", "granite-3b-code-base-128k", "granite-8b-code-base-128k", "allam-1-13b", "granite-3-8b", "granite-3.1-2b", "granite-3.1-8b-instruct", "mistral-123b-v2", "granite-3.1-3b-a800m-instruct", "granite-vision-3.2-2b", "smollm2-135m", "llava-v1.6-mistral-7b"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1, number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for example<ul> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> </li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options are:<ul> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127 (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127 (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127 (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least 16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2, llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096 entries. Each entry includes at least 16384 tokens when tokenized with <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image with dimensions 384×384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset also contains 4096 entries with a minimum of 16384 tokens per entry (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies of a single image sized 384×768.</li> </ul> </li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass </li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to accumulate before performing a backward/update pass. Only takes effect when gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code> (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>, <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding. number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list with strings of boolean values for <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>, <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>, <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>, <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>, <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>, <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>, <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>, <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>, <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>, <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards optimizer states, gradients and parameters), " [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform <code>requires_grad</code> during init, which means support for interspersed frozen and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code> requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None. List of transformer layer class names (case-sensitive) to wrap, e.g, <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>, <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures trl to skip preparing the dataset</li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p> </div> </details> <h3 id=finetune_full_stability-v100>finetune_full_stability-v1.0.0<a class=headerlink href=#finetune_full_stability-v100 title="Permanent link">&para;</a></h3> <p>Runs full fine-tuning five times and reports the proportion of tasks that fail due to GPU memory limits, unknown errors, or complete successfully. This experiment is useful for testing the model stability under different configurations.</p> <details class=note> <summary>Experiment documentation</summary> <p>An experiment instance:</p> <ul> <li>performs full fine-tuning 5 times and reports the fraction of tasks that ran out of GPU memory, exhibited some unknown error, or completed successfully</li> <li>You may notice that even large-memory GPUs like the 80GB variant of the NVIDIA A100 chip need at least 2 GPUs to train models as big as 13B parameters.</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code></li> <li>uses the <code>FSDP</code> distributed backend</li> <li>runs 5 optimization steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>We use the following <code>accelerate_config.yml</code> YAML file for all models:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>debug</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>fsdp_config</span><span class=p>:</span>
<span class=w>  </span><span class=nt>fsdp_auto_wrap_policy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class=w>  </span><span class=nt>fsdp_backward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class=w>  </span><span class=nt>fsdp_forward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_offload_params</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_sharding_strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=w>  </span><span class=nt>fsdp_state_dict_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FULL_STATE_DICT</span>
<span class=w>  </span><span class=nt>fsdp_cpu_ram_efficient_loading</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_sync_module_states</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>Commandline:</p> <div class=highlight><pre><span></span><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \
  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \
  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \
  --response_template &quot;\n### Response:&quot; --dataset_text_field output --log_level debug \
  --max_steps -1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \
  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \
  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \
  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \
  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \
  --packing False --peft_method none --optim ${OPTIM} --bf16 ${BF16} \
  --gradient_checkpointing_kwargs=&#39;{&quot;use_reentrant&quot;: ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}&#39; \
  --fast_moe ${FAST_MOE}
</code></pre></div> <p><strong>Note</strong>: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:<ul> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul> </li> </ul> <h4 id=full-finetuning-stability-requirements>Full Finetuning (Stability) Requirements<a class=headerlink href=#full-finetuning-stability-requirements title="Permanent link">&para;</a></h4> <ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the models:<ul> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> </ul> </li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic datasets of the SFTTrainer actuator</li> </ul> <h4 id=full-finetuning-stability-entity-space>Full Finetuning (Stability) Entity space<a class=headerlink href=#full-finetuning-stability-entity-space title="Permanent link">&para;</a></h4> <div class=highlight><pre><span></span><code>Required:
</code></pre></div> <ul> <li>model_name: Supported models: <code>["granite-3b-1.5", "hf-tiny-model-private/tiny-random-BloomForCausalLM", "llama-7b", "granite-13b-v2", "llama-13b", "granite-20b-v2", "granite-7b-base", "granite-8b-japanese", "granite-8b-code-base", "granite-34b-code-base", "mistral-7b-v0.1", "llama3-8b", "llama3-70b", "mixtral-8x7b-instruct-v0.1", "llama2-70b", "llama3.1-8b", "llama3.1-70b", "llama3.1-405b", "granite-3b-code-base-128k", "granite-8b-code-base-128k", "allam-1-13b", "granite-3-8b", "granite-3.1-2b", "granite-3.1-8b-instruct", "mistral-123b-v2", "granite-3.1-3b-a800m-instruct", "granite-vision-3.2-2b", "smollm2-135m", "llava-v1.6-mistral-7b"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1, number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for example<ul> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> </li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options are:<ul> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127 (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127 (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127 (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least 16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2, llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096 entries. Each entry includes at least 16384 tokens when tokenized with <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image with dimensions 384×384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset also contains 4096 entries with a minimum of 16384 tokens per entry (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies of a single image sized 384×768.</li> </ul> </li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass </li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to accumulate before performing a backward/update pass. Only takes effect when gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code> (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>, <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding. number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list with strings of boolean values for <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>, <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>, <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>, <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>, <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>, <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>, <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>, <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>, <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>, <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards optimizer states, gradients and parameters), " [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform <code>requires_grad</code> during init, which means support for interspersed frozen and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code> requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None. List of transformer layer class names (case-sensitive) to wrap, e.g, <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>, <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures trl to skip preparing the dataset</li> </ul> <h4 id=full-finetuning-stability-measured-properties>Full Finetuning (Stability) Measured properties<a class=headerlink href=#full-finetuning-stability-measured-properties title="Permanent link">&para;</a></h4> <ul> <li>f_gpu_oom: fraction of tasks that ran out of GPU memory </li> <li>f_other_error: fraction of tasks that ran into an unknown error</li> <li>f_no_error: fraction of tasks that completed successfully</li> <li>is_valid: whether this collection of tasks is a valid point to investigate</li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p> </div> </details> <h3 id=finetune_lora_benchmark-v100>finetune_lora_benchmark-v1.0.0<a class=headerlink href=#finetune_lora_benchmark-v100 title="Permanent link">&para;</a></h3> <p>Executes LoRA-based fine-tuning, a parameter-efficient method that adapts only a small subset of model weights. This benchmark is useful for scenarios where compute or memory resources are limited, while still enabling meaningful adaptation.</p> <details class=note> <summary>Experiment documentation</summary> <p>An experiment instance:</p> <ul> <li>performs LORA fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default, can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>fsdp_config</span><span class=p>:</span>
<span class=w>  </span><span class=nt>fsdp_auto_wrap_policy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class=w>  </span><span class=nt>fsdp_backward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class=w>  </span><span class=nt>fsdp_forward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_offload_params</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_sharding_strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_sharding_strategy}</span>
<span class=w>  </span><span class=nt>fsdp_state_dict_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_state_dict_type}</span>
<span class=w>  </span><span class=nt>fsdp_cpu_ram_efficient_loading</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_sync_module_states</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_transformer_layer_cls_to_wrap</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_fsdp_transformer_layer_cls_to_wrap}</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>For DDP runs we use this instead:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>debug</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">no</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>Commandline:</p> <div class=highlight><pre><span></span><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \
  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \
  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \
  --response_template &quot;\n### Response:&quot; --dataset_text_field output --log_level debug \
  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \
  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \
  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \
  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \
  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \
  --packing False --peft_method lora --target_modules ${SPACE SEPARATED LAYER NAMES} \
  --optim ${OPTIM} --bf16 ${BF16} \
  --gradient_checkpointing_kwargs=&#39;{&quot;use_reentrant&quot;: ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}&#39; \
  --fast_moe ${FAST_MOE}
</code></pre></div> <p><strong>Note</strong>: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:<ul> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul> </li> </ul> <h4 id=lora-requirements>LoRA Requirements<a class=headerlink href=#lora-requirements title="Permanent link">&para;</a></h4> <ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the models:<ul> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> </ul> </li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic datasets of the SFTTrainer actuator</li> </ul> <h4 id=lora-entity-space>LoRA Entity space<a class=headerlink href=#lora-entity-space title="Permanent link">&para;</a></h4> <p>Required:</p> <ul> <li>model_name: Supported models: <code>["granite-3b-1.5", "hf-tiny-model-private/tiny-random-BloomForCausalLM", "llama-7b", "granite-13b-v2", "llama-13b", "granite-20b-v2", "granite-7b-base", "granite-8b-japanese", "granite-8b-code-base", "granite-34b-code-base", "mistral-7b-v0.1", "llama3-8b", "llama3-70b", "mixtral-8x7b-instruct-v0.1", "llama2-70b", "llama3.1-8b", "llama3.1-70b", "llama3.1-405b", "granite-3b-code-base-128k", "granite-8b-code-base-128k", "allam-1-13b", "granite-3-8b", "granite-3.1-2b", "granite-3.1-8b-instruct", "mistral-123b-v2", "granite-3.1-3b-a800m-instruct", "granite-vision-3.2-2b", "smollm2-135m", "llava-v1.6-mistral-7b"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1, number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for example<ul> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> </li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127 (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127 (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127 (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least 16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2, llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096 entries. Each entry includes at least 16384 tokens when tokenized with <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image with dimensions 384×384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset also contains 4096 entries with a minimum of 16384 tokens per entry (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies of a single image sized 384×768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass </li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to accumulate before performing a backward/update pass. Only takes effect when gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code> (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>, <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding. number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list with strings of boolean values for <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>r: Default is <code>4</code>. The LORA rank</li> <li>lora_alpha: Default is <code>16</code>. Scales the learning weights.</li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>, <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>, <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>, <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>, <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>, <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>, <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>, <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>, <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>, <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards optimizer states, gradients and parameters), " [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform <code>requires_grad</code> during init, which means support for interspersed frozen and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code> requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None. List of transformer layer class names (case-sensitive) to wrap, e.g, <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>, <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures trl to skip preparing the dataset</li> </ul> <p>Hardcoded:</p> <p>Sets the <code>--target_modules</code> layer names based on the <code>model_name</code>:</p> <ul> <li><code>smollm2-135m</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-3.1-3b-a800m-instruct</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-vision-3.2-2b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-3b-code-base-128k</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-7b-base</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-8b-code-base-128k</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-8b-code-base</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-8b-japanese</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-13b-v2</code>: <code>["c_attn", "c_proj"]</code></li> <li><code>granite-20b-v2</code>: <code>["c_attn", "c_proj"]</code></li> <li><code>granite-34b-code-base</code>: <code>["c_attn", "c_proj"]</code></li> <li><code>llama-7b</code>: <code>["q_proj", "k_proj"]</code></li> <li><code>llama-13b</code>: <code>["q_proj", "k_proj"]</code></li> <li><code>llama2-70b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llama3-8b</code>: <code>["q_proj", "k_proj"]</code></li> <li><code>llama3-70b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llama3.1-8b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llama3.1-70b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llama3.1-405b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>allam-1-13b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>hf-tiny-model-private/tiny-random-BloomForCausalLM</code>: <code>["dense_h_to_4h", "dense_4h_to_4h"]</code></li> <li><code>mistral-7b-v0.1</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>mistral-123b-v2</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>mixtral-8x7b-instruct-v0.1</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-3-8b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-3.1-2b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-3.1-8b-instruct</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llava-v1.6-mistral-7b</code>: <code>["q_proj", "v_proj"]</code></li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p> </div> </details> <h3 id=finetune_pt_benchmark-v100>finetune_pt_benchmark-v1.0.0<a class=headerlink href=#finetune_pt_benchmark-v100 title="Permanent link">&para;</a></h3> <p>Runs prompt-tuning, a lightweight fine-tuning strategy that prepends trainable prompts to the input. Similar to LoRA, this benchmark is useful for compute or memory constrained environments.</p> <details class=note> <summary>Experiment documentation</summary> <p>An experiment instance:</p> <ul> <li>performs prompt-tuning fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>bfloat16</code> by default, can also be float16</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default, can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>fsdp_config</span><span class=p>:</span>
<span class=w>  </span><span class=nt>fsdp_auto_wrap_policy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class=w>  </span><span class=nt>fsdp_backward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class=w>  </span><span class=nt>fsdp_forward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_offload_params</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_sharding_strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_sharding_strategy}</span>
<span class=w>  </span><span class=nt>fsdp_state_dict_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_state_dict_type}</span>
<span class=w>  </span><span class=nt>fsdp_cpu_ram_efficient_loading</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_sync_module_states</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_transformer_layer_cls_to_wrap</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_fsdp_transformer_layer_cls_to_wrap}</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>For DDP runs we use this instead:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>debug</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">no</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>Commandline:</p> <div class=highlight><pre><span></span><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \
  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \
  --torch_dtype bfloat16 --use_flash_attn True --training_data_path ${DATASET_PATH} \
  --response_template &quot;\n### Response:&quot; --dataset_text_field output --log_level debug \
  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \
  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \
  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \
  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \
  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \
  --packing False --peft_method none \
  --fast_moe ${FAST_MOE}
</code></pre></div> <p><strong>Note</strong>: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:<ul> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul> </li> </ul> <h4 id=prompt-tuning-requirements>Prompt Tuning Requirements<a class=headerlink href=#prompt-tuning-requirements title="Permanent link">&para;</a></h4> <ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the models:<ul> <li>LLaMa/models/hf/13B/</li> <li>LLaMa/models/hf/7B/</li> <li>LLaMa/models/hf/llama2-70b/</li> <li>LLaMa/models/hf/llama3-70b/</li> <li>LLaMa/models/hf/llama3-8b/</li> <li>LLaMa/models/hf/llama3.1-405b/</li> <li>LLaMa/models/hf/llama3.1-70b/</li> <li>LLaMa/models/hf/llama3.1-8b/</li> <li>Mixtral-8x7B-Instruct-v0.1/</li> <li>allam-1-13b-instruct-20240607/</li> <li>granite-13b-base-v2/step_300000_ckpt/</li> <li>granite-20b-code-base-v2/step_280000_ckpt/</li> <li>granite-34b-code-base/</li> <li>granite-8b-code-base/</li> <li>granite-8b-japanese-base-v1-llama/</li> <li>mistralai-mistral-7b-v0.1/</li> <li>mistral-large/fp16_240620</li> </ul> </li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic datasets of the SFTTrainer actuator</li> </ul> <h4 id=prompt-tuning-entity-space>Prompt Tuning Entity space<a class=headerlink href=#prompt-tuning-entity-space title="Permanent link">&para;</a></h4> <p>Required:</p> <ul> <li>model_name: Supported models: <code>["granite-3b-1.5", "hf-tiny-model-private/tiny-random-BloomForCausalLM", "llama-7b", "granite-13b-v2", "llama-13b", "granite-20b-v2", "granite-7b-base", "granite-8b-japanese", "granite-8b-code-base", "granite-34b-code-base", "mistral-7b-v0.1", "llama3-8b", "llama3-70b", "mixtral-8x7b-instruct-v0.1", "llama2-70b", "llama3.1-8b", "llama3.1-70b", "llama3.1-405b", "granite-3b-code-base-128k", "granite-8b-code-base-128k", "allam-1-13b", "granite-3-8b", "granite-3.1-2b", "granite-3.1-8b-instruct", "mistral-123b-v2", "granite-3.1-3b-a800m-instruct", "granite-vision-3.2-2b", "smollm2-135m", "llava-v1.6-mistral-7b"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1, number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for example<ul> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> </li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127 (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127 (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127 (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least 16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2, llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096 entries. Each entry includes at least 16384 tokens when tokenized with <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image with dimensions 384×384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset also contains 4096 entries with a minimum of 16384 tokens per entry (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies of a single image sized 384×768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass </li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to accumulate before performing a backward/update pass. Only takes effect when gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>bfloat16</code>. One of <code>bfloat16</code>, <code>float32</code>, <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code> (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>, <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding. number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list with strings of boolean values for <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>, <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>, <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>, <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>, <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>, <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>, <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>, <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>, <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>, <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards optimizer states, gradients and parameters), " [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform <code>requires_grad</code> during init, which means support for interspersed frozen and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code> requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None. List of transformer layer class names (case-sensitive) to wrap, e.g, <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>, <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures trl to skip preparing the dataset</li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p> </div> </details> <h3 id=finetune_gtpq-lora_benchmark-v100>finetune_gtpq-lora_benchmark-v1.0.0<a class=headerlink href=#finetune_gtpq-lora_benchmark-v100 title="Permanent link">&para;</a></h3> <p>Combines LoRA with GPTQ quantization to enable fine-tuning on quantized models. This benchmark is tailored for scenarios where model size and inference efficiency are critical, and it leverages fused kernels and quantized weights for performance.</p> <details class=note> <summary>Experiment documentation</summary> <p>An experiment instance:</p> <ul> <li>performs LORA fine tuning</li> <li>the training data is artificial</li> <li><code>use_flash_attn</code> is set to True</li> <li><code>packing</code> is set to False</li> <li><code>torch_dtype</code> is set to <code>float16</code>, cannot be a different value</li> <li>uses the <code>FSDP</code> distributed backend for multi-gpu runs by default, can also be <code>DDP</code></li> <li>multi-gpu runs with FSDP and DDP backends use 1 process per GPU (via <code>accelerate</code>)</li> <li>runs 1 epoch by default, can also run a custom number of steps</li> <li>does not save checkpoint</li> <li>loads weights from a PVC</li> <li>request 2 CPU cores per GPU device (with a minimum of 2 cores)</li> <li>uses fms-acceleration plugins to perform GPTQ LoRA. Specifically:</li> <li><code>auto_gptq</code> is set to <code>triton_v2</code></li> <li><code>fast_kernels</code> is set to <code>True True True</code></li> <li><code>fused_lora</code> is set to <code>auto_gptq True</code></li> <li><code>torch_dtype</code> is set to <code>float16</code></li> <li>loads GPTQ compatible pre-quantized weights from a PVC</li> </ul> <p>For FSDP runs we use the following <code>accelerate_config.yml</code> YAML file:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">FSDP</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class=s>&#39;no&#39;</span>
<span class=nt>fsdp_config</span><span class=p>:</span>
<span class=w>  </span><span class=nt>fsdp_auto_wrap_policy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">TRANSFORMER_BASED_WRAP</span>
<span class=w>  </span><span class=nt>fsdp_backward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD_PRE</span>
<span class=w>  </span><span class=nt>fsdp_forward_prefetch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_offload_params</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>  </span><span class=nt>fsdp_sharding_strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_sharding_strategy}</span>
<span class=w>  </span><span class=nt>fsdp_state_dict_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${fsdp_state_dict_type}</span>
<span class=w>  </span><span class=nt>fsdp_cpu_ram_efficient_loading</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_sync_module_states</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>fsdp_transformer_layer_cls_to_wrap</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_fsdp_transformer_layer_cls_to_wrap}</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>For DDP runs we use this instead:</p> <div class=highlight><pre><span></span><code><span class=nt>compute_environment</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LOCAL_MACHINE</span>
<span class=nt>debug</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class=nt>downcast_bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">no</span>
<span class=nt>distributed_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MULTI_GPU</span>
<span class=nt>machine_rank</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$THE MACHINE RANK - always 0 for single-node runs</span><span class="p p-Indicator">}</span>
<span class=nt>main_training_function</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">main</span>
<span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${accelerate_config_mixed_precision}</span>
<span class=nt>num_machines</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=nt>rdzv_backend</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">static</span>
<span class=nt>same_network</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=nt>tpu_env</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">[]</span>
<span class=nt>tpu_use_cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>tpu_use_sudo</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>use_cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=nt>main_process_port</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$SOME_PORT</span><span class="p p-Indicator">}</span>
<span class=nt>num_processes</span><span class=p>:</span><span class=w> </span><span class="p p-Indicator">{</span><span class=nv>$NUM_GPUS</span><span class="p p-Indicator">}</span>
</code></pre></div> <p>Commandline:</p> <div class=highlight><pre><span></span><code>accelerate launch --config_file ${PATH_ACCELERATE_CONFIG} --num_processes ${NUMBER_GPUS} \
  ${PATH_TO_OUR_WRAPPER_OF_FMS_HF_TUNING_SFT_TRAINER} --model_name_or_path ${MODEL} \
  --torch_dtype float16 --use_flash_attn True --training_data_path ${DATASET_PATH} \
  --response_template &quot;\n### Response:&quot; --dataset_text_field output --log_level debug \
  --num_train_epochs 1 --per_device_train_batch_size ${BATCH_SIZE/NUM_GPUS} \
  --max_seq_length ${MODEL_MAX_LENGTH} --eval_strategy no --output_dir ${RANDOM_DIR} \
  --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} --save_strategy no \
  --learning_rate 1e-05 --weight_decay 0.0 --warmup_ratio 0.03 --lr_scheduler_type cosine \
  --logging_steps 1 --include_tokens_per_second True --gradient_checkpointing True \
  --packing False --peft_method lora --target_modules ${SPACE SEPARATED LAYER NAMES} \
  --fp16 true --fast_kernels true true true --fused_lora auto_gptq true --auto_gptq triton_v2 \
  --optim ${OPTIM} --bf16 ${BF16} \
  --gradient_checkpointing_kwargs=&#39;{&quot;use_reentrant&quot;: ${GRADIENT_CHECKPOINTING_USE_REENTRANT}}&#39; \
  --fast_moe ${FAST_MOE}
</code></pre></div> <p><strong>Note</strong>: <code>--fast_moe</code> is only supported for fms-hf-tuning v2.4.0+</p> <p>We use a thin wrapper of <code>sft_trainer.py</code> which injects a custom Callback that exports the metrics collected by AIM. You can repeat our experiments by just pointing the above command-line to <code>sft_trainer.py</code> from the <code>fms-hf-tuning</code> package.</p> <p>Versioning:</p> <ul> <li>Actuator version: <code>2.1.0</code></li> <li>fms-hf-tuning versions:<ul> <li>2.8.2</li> <li>2.7.1</li> <li>2.6.0</li> <li>2.5.0</li> <li>2.4.0</li> <li>2.3.1</li> <li>2.2.1</li> <li>2.1.2 (default)</li> <li>2.1.1</li> <li>2.1.0</li> <li>2.0.1</li> </ul> </li> </ul> <h4 id=gptq-lora-requirements>GPTQ LoRA Requirements<a class=headerlink href=#gptq-lora-requirements title="Permanent link">&para;</a></h4> <ul> <li>The PVC <code>hf-models-pvc</code> mounted under <code>/hf-models-pvc</code> - should contain the models:<ul> <li>LLaMa/models/hf/7B-gptq/</li> <li>LLaMa/models/hf/llama3-70b-gptq/</li> <li>LLaMa/models/hf/llama3.1-405b-gptq/</li> <li>granite-20b-code-base-v2/step_280000_ckpt-gptq/</li> <li>granite-34b-gptq/</li> <li>granite-7b-base-gtpq/</li> <li>granite-8b-code-instruct-gptq/</li> <li>mistral-7B-v0.3-gptq/</li> <li>mixtral_8x7b_instruct_v0.1_gptq/</li> </ul> </li> <li>The PVC <code>ray-disorch-storage</code> mounted under <code>/data</code> with the synthetic datasets of the SFTTrainer actuator</li> </ul> <h4 id=gptq-lora-entity-space>GPTQ LoRA Entity space<a class=headerlink href=#gptq-lora-entity-space title="Permanent link">&para;</a></h4> <p>Required:</p> <ul> <li>model_name: Supported models: <code>["llama-7b", "granite-20b-v2", "granite-7b-base", "granite-8b-code-instruct", "granite-34b-code-base", "mistral-7b-v0.1", "llama3-70b", "mixtral-8x7b-instruct-v0.1", "llama3.1-405b"]</code></li> <li>model_max_length: Maximum sequence length. Sequences will be right padded (and possibly truncated)</li> <li>number_gpus: The effective number of GPUs (to be evenly distributed to <code>number_nodes</code> machines)</li> <li>batch_size: the effective batch_size (will be evenly distributed to max(1, number_gpus) devices)</li> <li>gpu_model: The value of the kubernetes node label <code>nvidia.com/gpu.prod</code> for example<ul> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-H100-PCIe</code></li> </ul> </li> </ul> <p>Optional:</p> <ul> <li>dataset_id: Default is <code>news-tokens-16384plus-entries-4096</code>. Available options are:</li> <li><code>news-chars-512-entries-4096</code>: 4096 entries with samples of 512 + 127 (prompt) + 512 characters</li> <li><code>news-chars-1024-entries-4096</code>: 4096 entries with samples of 1024 + 127 (prompt) + 1024 characters</li> <li><code>news-chars-2048-entries-4096</code>: 4096 entries with samples of 2048 + 127 (prompt) + 2048 characters</li> <li><code>news-tokens-16384plus-entries-4096</code>: 4096 entries, each entry has least 16384 tokens when tokenized with any of the granite-13b-v2, llama-13b-v2, llama-7b, or granite-20b-v2 tokenizers</li> <li><code>vision-384x384-16384plus-entries-4096</code>: A vision dataset containing 4096 entries. Each entry includes at least 16384 tokens when tokenized with <code>granite-vision-3.2-2b</code>, and consists of repeated copies of a single image with dimensions 384×384.</li> <li><code>vision-384x768-16384plus-entries-4096</code>: Similar to the above, this dataset also contains 4096 entries with a minimum of 16384 tokens per entry (tokenized using <code>granite-vision-3.2-2b</code>). Each entry uses repeated copies of a single image sized 384×768.</li> <li>gradient_checkpointing: Default is <code>True</code>. If <code>True</code>, use gradient checkpointing to save memory (i.e. higher batchsizes) at the expense of slower backward pass </li> <li>gradient_accumulation_steps: Default is 4. Number of update steps to accumulate before performing a backward/update pass. Only takes effect when gradient_checkpointing is True</li> <li>torch_dtype: Default is <code>float16</code>. One of <code>float16</code></li> <li>max_steps: Default is <code>-1</code>. The number of optimization steps to perform. Set to -1 to respect num_train_epochs instead.</li> <li>num_train_epochs: Default is <code>1.0</code>. How many epochs to run. Ignored if max_steps is greater than 0.</li> <li>stop_after_seconds: Default is <code>-1.0</code>. If set, the optimizer will be asked to stop after the specified time elapses. The check is performed after the end of each training step.</li> <li>distributed_backend: Default is <code>FSDP</code> for multi-gpu measurements, <code>None</code> (i.e. Data Parallel (DP)) for single-gpu measurements. Which pytorch backend to use when training with multiple GPU devices.</li> <li>number_nodes: Default is <code>1</code>. If set, actuator distributes tasks on multiple nodes. Each Node will use number_gpus/number_nodes GPUs. Each Node will use 1 process for each GPU it uses</li> <li>fms_hf_tuning_version: Default is <code>2.1.2</code>. Which version of fms-hf-tuning to use. Available options are: <code>2.8.2</code>, <code>2.7.1</code>, <code>2.6.0</code>, <code>2.5.0</code>, <code>2.4.0</code>, <code>2.3.1</code>, <code>2.2.1</code>, <code>2.1.2</code>, <code>2.1.0</code>, <code>2.0.1</code></li> <li>enable_roce: Default is <code>False</code>. This setting is only in effect for multi-node runs. It controls whether RDMA over Converged Ethernet (RoCE) is switched on or not.</li> <li>fast_moe: Default is <code>0</code>. Configures the amount of expert parallel sharding. number_gpus must be divisible by it</li> <li>fast_kernels: Default is <code>None</code>. Switches on fast kernels, the value is a list with strings of boolean values for <code>[fast_loss, fast_rms_layernorm, fast_rope_embeddings]</code></li> <li>r: Default is <code>4</code>. The LORA rank</li> <li>lora_alpha: Default is <code>16</code>. Scales the learning weights.</li> <li>optim: Default is <code>adamw_torch</code>. The optimizer to use. Available options are <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_torch_xla</code>, <code>adamw_torch_npu_fused</code>, <code>adamw_apex_fused</code>, <code>adafactor</code>, <code>adamw_anyprecision</code>, <code>adamw_torch_4bit</code>, <code>ademamix</code>, <code>sgd</code>, <code>adagrad</code>, <code>adamw_bnb_8bit</code>, <code>adamw_8bit</code>, <code>ademamix_8bit</code>, <code>lion_8bit</code>, <code>lion_32bit</code>, <code>paged_adamw_32bit</code>, <code>paged_adamw_8bit</code>, <code>paged_ademamix_32bit</code>, <code>paged_ademamix_8bit</code>, <code>paged_lion_32bit</code>, <code>paged_lion_8bit</code>, <code>rmsprop</code>, <code>rmsprop_bnb</code>, <code>rmsprop_bnb_8bit</code>, <code>rmsprop_bnb_32bit</code>, <code>galore_adamw</code>, <code>galore_adamw_8bit</code>, <code>galore_adafactor</code>, <code>galore_adamw_layerwise</code>, <code>galore_adamw_8bit_layerwise</code>, <code>galore_adafactor_layerwise</code>, <code>lomo</code>, <code>adalomo</code>, <code>grokadamw</code>, <code>schedule_free_adamw</code>, <code>schedule_free_sgd</code></li> <li>bf16: Default is <code>False</code>. Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA add bf16 mixed precision support for NPU architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Can be <code>True</code>, <code>False</code>.</li> <li>gradient_checkpointing_use_reentrant: Default is <code>False</code> Specify whether to use the activation checkpoint variant that requires reentrant autograd. This parameter should be passed explicitly. Torch version 2.5 will raise an exception if use_reentrant is not passed. If use_reentrant=False, checkpoint will use an implementation that does not require reentrant autograd. This allows checkpoint to support additional functionality, such as working as expected with torch.autograd.grad and support for keyword arguments input into the checkpointed function. Can be <code>True</code>, <code>False</code>.</li> <li>fsdp_sharding_strategy: Default is <code>FULL_SHARD</code>. [1] FULL_SHARD (shards optimizer states, gradients and parameters), " [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards optimizer states and gradients within each node while each node has full copy). For more information, please refer the official PyTorch docs.</li> <li>fsdp_state_dict_type: Default is <code>FULL_STATE_DICT</code>. [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT</li> <li>fsdp_use_orig_params: Default is <code>True</code>. If True, allows non-uniform <code>requires_grad</code> during init, which means support for interspersed frozen and trainable parameters. (useful only when <code>use_fsdp</code> flag is passed).</li> <li>accelerate_config_mixed_precision: Default is <code>no</code>. Whether to use mixed precision training or not. Choose from <code>no</code>,<code>fp16</code>,<code>bf16</code> or <code>fp8</code>. <code>fp8</code> requires the installation of transformers-engine.</li> <li>accelerate_config_fsdp_transformer_layer_cls_to_wrap: Default is None. List of transformer layer class names (case-sensitive) to wrap, e.g, <code>GraniteDecoderLayer</code>, <code>LlamaDecoderLayer</code>, <code>MistralDecoderLayer</code>, <code>BertLayer</code>, <code>GPTJBlock</code>, <code>T5Block</code> ... (useful only when using FSDP)</li> <li>dataset_text_field: Default is None. Training dataset text field containing single sequence. Either the dataset_text_field or data_formatter_template need to be supplied. For running vision language model tuning pass the column name for text data.</li> <li>dataset_image_field: Default is None. For running vision language model tuning pass the column name of the image data in the dataset.</li> <li>remove_unused_columns: Default is True. Remove columns not required by the model when using an nlp.Dataset.</li> <li>dataset_kwargs_skip_prepare_dataset: Default is False. When True, configures trl to skip preparing the dataset</li> </ul> <p>Hardcoded:</p> <p>Sets the <code>--target_modules</code> layer names based on the <code>model_name</code>:</p> <ul> <li><code>granite-8b-code-instruct</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-7b-base</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>granite-20b-v2</code>: <code>["c_attn", "c_proj"]</code></li> <li><code>granite-34b-code-base</code>: <code>["c_attn", "c_proj"]</code></li> <li><code>llama-7b</code>: <code>["q_proj", "k_proj"]</code></li> <li><code>llama3-70b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>mistral-7b-v0.1</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>mixtral-8x7b-instruct-v0.1</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>llama3.1-405b</code>: <code>["q_proj", "v_proj"]</code></li> <li><code>allam-1-13b</code>: <code>["q_proj", "v_proj"]</code></li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>Because running <code>accelerate</code> with a single gpu is unsupported, when setting <code>number_gpus</code> to 1 this experiment actually runs the <code>tuning.sft_trainer</code> script directly (i.e. a DataParallel (DP) run).</p> </div> </details> <h3 id=actuator-parameters>Actuator Parameters<a class=headerlink href=#actuator-parameters title="Permanent link">&para;</a></h3> <p>This section describes the fields you may optionally configure in your <code>actuatorconfiguration</code> resource for the <code>SFTTrainer</code> actuator.</p> <h3 id=example-actuator-configuration-yaml>Example Actuator Configuration YAML<a class=headerlink href=#example-actuator-configuration-yaml title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=nt>actuatorIdentifier</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">SFTTrainer</span>
<span class=nt>parameters</span><span class=p>:</span>
<span class=w>  </span><span class=nt>match_exact_dependencies</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class=w>  </span><span class=nt>output_dir</span><span class=p>:</span><span class=w> </span><span class=s>&quot;output&quot;</span>
<span class=w>  </span><span class=nt>data_directory</span><span class=p>:</span><span class=w> </span><span class=s>&quot;/data/fms-hf-tuning/artificial-dataset/&quot;</span>
<span class=w>  </span><span class=nt>hf_home</span><span class=p>:</span><span class=w> </span><span class=s>&quot;/hf-models-pvc/huggingface_home&quot;</span>
<span class=w>  </span><span class=nt>model_map</span><span class=p>:</span>
<span class=w>    </span><span class=nt>granite-3.1-2b</span><span class=p>:</span>
<span class=w>      </span><span class=nt>Vanilla</span><span class=p>:</span><span class=w> </span><span class=s>&quot;ibm-granite/granite-3.1-2b-base&quot;</span>
<span class=w>  </span><span class=nt>num_tokens_cache_directory</span><span class=p>:</span><span class=w> </span><span class=s>&quot;cache&quot;</span>
</code></pre></div> <h3 id=configuration-fields>Configuration Fields<a class=headerlink href=#configuration-fields title="Permanent link">&para;</a></h3> <h4 id=match_exact_dependencies-bool-default-true><code>match_exact_dependencies</code> (bool, default: <code>true</code>)<a class=headerlink href=#match_exact_dependencies-bool-default-true title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: If <code>true</code>, the measurement runs in a virtual environment that exactly matches the Python packages of the selected <code>fms-hf-tuning</code> version. This enables all optional features like <code>fast_kernels</code>, <code>fast_moe</code>, and <code>flash_attn</code>.</li> <li><strong>Set to <code>false</code></strong> if running on devices with limited support (e.g., MacBooks or ARM CPUs), to avoid incompatible packages and features that depend on using NVIDIA GPUs.</li> </ul> <h4 id=output_dir-str-default-output><code>output_dir</code> (str, default: <code>"output"</code>)<a class=headerlink href=#output_dir-str-default-output title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Directory prefix where the fine-tuned model weights will be saved.</li> </ul> <h4 id=data_directory-str-default-datafms-hf-tuningartificial-dataset><code>data_directory</code> (str, default: <code>"/data/fms-hf-tuning/artificial-dataset/"</code>)<a class=headerlink href=#data_directory-str-default-datafms-hf-tuningartificial-dataset title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Path to the directory containing the dataset files used for fine-tuning.</li> </ul> <h4 id=aim_db-str-default-none><code>aim_db</code> (str, default: None)<a class=headerlink href=#aim_db-str-default-none title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Endpoint of the AIM server used to log training metrics. When set to None the measurement will use a temporary AIM repository that will be garbage collected after the termination of the measurement.</li> </ul> <h4 id=aim_dashboard_url-str-or-null-optional><code>aim_dashboard_url</code> (str or null, optional)<a class=headerlink href=#aim_dashboard_url-str-or-null-optional title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: URL of the AIM dashboard. If set, this will be included in the metadata of the measurement results.</li> <li><strong>Example</strong>: <code>"http://aim-dashboard.example.com"</code></li> </ul> <h4 id=hf_home-str-default-hf-models-pvchuggingface_home><code>hf_home</code> (str, default: <code>"/hf-models-pvc/huggingface_home"</code>)<a class=headerlink href=#hf_home-str-default-hf-models-pvchuggingface_home title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Directory where Hugging Face stores authentication tokens and model cache.</li> </ul> <h4 id=model_map-dict-optional><code>model_map</code> (dict, optional)<a class=headerlink href=#model_map-dict-optional title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Maps model identifiers to their corresponding Hugging Face model ids and absolute paths. The contents of this dictionary will override the defaults that ship with the Actuator.</li> <li><strong>Example</strong>: <div class=highlight><pre><span></span><code><span class=nt>model_map</span><span class=p>:</span>
<span class=w>  </span><span class=nt>granite-3.1-2b</span><span class=p>:</span>
<span class=w>    </span><span class=nt>Vanilla</span><span class=p>:</span><span class=w> </span><span class=s>&quot;ibm-granite/granite-3.1-2b-base&quot;</span>
</code></pre></div></li> </ul> <h4 id=num_tokens_cache_directory-str-or-null-default-cache><code>num_tokens_cache_directory</code> (str or null, default: <code>"cache"</code>)<a class=headerlink href=#num_tokens_cache_directory-str-or-null-default-cache title="Permanent link">&para;</a></h4> <ul> <li><strong>Description</strong>: Directory used to cache token counts for datasets. This avoids recomputing token counts, which can be time-consuming. Relative paths are resolved under <code>@data_directory</code>.</li> <li><strong>Set to <code>null</code></strong> to disable caching.</li> </ul> <h3 id=measured-properties>Measured Properties<a class=headerlink href=#measured-properties title="Permanent link">&para;</a></h3> <p>Each experiment collects detailed runtime and system-level metrics using <a href=https://github.com/aimhubio/aim>AIM</a>. The AIM metrics are aggregated into the following before being stored in ado's database:</p> <h5 id=gpu-metrics><strong>GPU Metrics</strong><a class=headerlink href=#gpu-metrics title="Permanent link">&para;</a></h5> <ul> <li><strong>Compute Utilization</strong>: <code>min</code>, <code>avg</code>, <code>max</code> (%)</li> <li><strong>Memory Utilization</strong>: <code>min</code>, <code>avg</code>, <code>max</code>, <code>peak</code> (%)</li> <li><strong>Power Usage</strong>: <code>min</code>, <code>avg</code>, <code>max</code> (Watts and %)</li> </ul> <h5 id=cpu-metrics><strong>CPU Metrics</strong><a class=headerlink href=#cpu-metrics title="Permanent link">&para;</a></h5> <ul> <li><strong>Compute Utilization</strong>: Average CPU usage per core (%)</li> <li><strong>Memory Utilization</strong>: Average memory usage of the training process (%)</li> </ul> <h5 id=training-performance><strong>Training Performance</strong><a class=headerlink href=#training-performance title="Permanent link">&para;</a></h5> <ul> <li><strong><code>train_runtime</code></strong>: Duration in seconds from the start of the first training step to the end of the last training step. </li> <li><strong><code>train_samples_per_second</code></strong>: May be inaccurate, as HuggingFace uses a heuristic to estimate this. </li> <li><strong><code>train_steps_per_second</code></strong>: May be inaccurate due to HuggingFace's heuristic-based measurement. </li> <li><strong><code>train_tokens_per_second</code></strong>: May be inaccurate, as it relies on HuggingFace's heuristic. </li> <li><strong><code>train_tokens_per_gpu_per_second</code></strong>: May be inaccurate for the same reason, HuggingFace uses a heuristic. </li> <li><strong><code>dataset_tokens_per_second</code></strong>: The actuator computes this in an accurate way.</li> <li><strong><code>dataset_tokens_per_second_per_gpu</code></strong>: The actuator computes this in an accurate way.</li> </ul> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>We report all <strong>system metrics</strong> as min/avg/max over the duration of the run. GPU metrics are collected per device; CPU metrics are collected for the training process. Token throughput accounts for padding and sequence truncation.</p> </div> <h4 id=validation><strong>Validation</strong><a class=headerlink href=#validation title="Permanent link">&para;</a></h4> <p>Each experiment includes a computed <code>is_valid</code> flag that indicates whether the run was structurally and functionally valid. A run is marked <strong>invalid</strong> if any of the following conditions are met:</p> <h5 id=configuration-errors><strong>Configuration Errors</strong><a class=headerlink href=#configuration-errors title="Permanent link">&para;</a></h5> <ul> <li><code>batch_size</code> is not evenly divisible by <code>number_gpus</code></li> <li><code>number_gpus</code> is not evenly divisible by <code>number_nodes</code></li> <li><code>number_nodes</code> is less than 1</li> <li><code>batch_size</code> is less than 1</li> <li><code>gpu_model</code> is missing or empty when <code>number_gpus &gt; 0</code></li> </ul> <h5 id=incompatible-mixture-of-experts-moe-settings><strong>Incompatible Mixture of Experts (MoE) Settings</strong><a class=headerlink href=#incompatible-mixture-of-experts-moe-settings title="Permanent link">&para;</a></h5> <ul> <li><code>fast_moe</code> is set but <code>number_gpus</code> is not divisible by it</li> <li><code>fast_moe</code> is set but the model’s <code>num_local_experts</code> is not divisible by <code>fast_moe</code></li> </ul> <h5 id=runtime-failures><strong>Runtime Failures</strong><a class=headerlink href=#runtime-failures title="Permanent link">&para;</a></h5> <ul> <li>The run raises a <code>torch.cuda.OutOfMemoryError</code> (considered invalid due to GPU memory exhaustion)</li> <li>The run raises a <code>RuntimeError: CUDA error: an illegal memory access was encountered</code> exception (considered invalid due to GPU memory exhaustion)</li> <li> <p>The run raises other exceptions (e.g., <code>RuntimeError</code> with <code>NCCL Error</code>) - these are marked as <strong>failed</strong> and do not record any metrics</p> <blockquote> <p><strong>Note</strong>: Failed runs are not persisted into ado's database. Restarting an operation will cause ado to retry them.</p> </blockquote> </li> </ul> <p>This validation logic ensures that only meaningful and resource-compatible runs are included in the information we store in ado's database.</p> <h2 id=configure-your-raycluster>Configure your RayCluster<a class=headerlink href=#configure-your-raycluster title="Permanent link">&para;</a></h2> <p>Running SFTTrainer experiments requires:</p> <ul> <li><a href=#annotating-gpu-workers-with-custom-resources>GPU workers with custom resources indicating the available GPU devices</a></li> <li><a href=#creating-the-datasets>A dataset</a></li> <li><a href=#model-weights>The model weights</a></li> </ul> <p>Use the information below to <a href=../../getting-started/installing-backend-services/#deploying-kuberay-and-creating-a-raycluster>deploy your RayCluster</a>.</p> <h3 id=annotating-gpu-workers-with-custom-resources>Annotating GPU workers with custom resources<a class=headerlink href=#annotating-gpu-workers-with-custom-resources title="Permanent link">&para;</a></h3> <p>The <code>SFTTrainer</code> actuator leverages Ray's custom resource scheduling to efficiently allocate GPU-powered tasks to workers equipped with the appropriate hardware. It uses the following custom resources:</p> <h4 id=custom-resource-types>Custom Resource Types<a class=headerlink href=#custom-resource-types title="Permanent link">&para;</a></h4> <ul> <li> <p><strong><code>full-worker</code></strong><br> Some Ray tasks require exclusive access to an entire node. These tasks request the <code>full-worker</code> resource. GPU workers that occupy a full node should have exactly one <code>full-worker</code> custom resource.</p> </li> <li> <p><strong><code>${GPU_MODEL}</code></strong><br> This custom resource key corresponds to the specific GPU model available on the node, with the value indicating the number of devices. Supported GPU models include:</p> <ul> <li><code>NVIDIA-A100-SXM4-80GB</code></li> <li><code>NVIDIA-A100-80GB-PCIe</code></li> <li><code>NVIDIA-H100-80GB-HBM3</code></li> <li><code>NVIDIA-H100-PCIe</code></li> <li><code>Tesla-V100-PCIE-16GB</code></li> <li><code>Tesla-T4</code></li> <li><code>L40S</code></li> </ul> </li> <li> <p><strong><code>RoCE</code></strong><br> Tasks that utilize RDMA over Converged Ethernet (RoCE) request the <code>RoCE</code> resource. For guidance on configuring RoCE in your RayCluster, refer to the instructions linked at the bottom of this page.</p> </li> </ul> <h2 id=creating-the-datasets>Creating the datasets<a class=headerlink href=#creating-the-datasets title="Permanent link">&para;</a></h2> <p>The <strong>SFTTrainer</strong> actuator supports both <strong>text-to-text</strong> and <strong>image-to-text</strong> tuning experiments. Installing the actuator provides access to 2 command-line utilities for generating synthetic datasets.</p> <p>By default, the actuator expects the dataset files under <code>/data/fms-hf-tuning/artificial-dataset/</code></p> <p>You can override this path by setting the <code>data_directory</code> parameter via an <strong>ActuatorConfiguration</strong> resource and referencing it in the <strong>Operations</strong> you create. We include a link to the relevant documentation at the bottom of this page.</p> <h3 id=dataset-for-text-to-text-tasks>Dataset for text-to-text tasks<a class=headerlink href=#dataset-for-text-to-text-tasks title="Permanent link">&para;</a></h3> <p>For text-to-text tasks, create a dataset file with the name <code>news-tokens-16384plus-entries-4096.jsonl</code>. </p> <p>Use the following command:</p> <div class=highlight><pre><span></span><code>sfttrainer_generate_dataset_text -o /data/fms-hf-tuning/artificial-dataset/news-tokens-16384plus-entries-4096.jsonl
</code></pre></div> <p>If you are working with a remote RayCluster, run this as a <strong>remote Ray job</strong> using a Ray runtime environment that contains the python package for the SFTTrainer actuator. At the bottom of this page you will find a link to our documentation on submitting remote Ray jobs that use the code of Actuators.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>If your RayCluster Worker nodes already have the SFTTrainer wheel installed, you can skip building the wheel and using a ray runtime environment. Go directly to the <code>ray job submit</code> step. Just change the commandline so that it does not use the <code>ray_runtime.yaml</code> file.</p> </div> <p>For example, build the wheel file for SFTTrainer and create the following <code>ray_runtime_env.yaml</code>:</p> <div class=highlight><pre><span></span><code><span class=nt>pip</span><span class=p>:</span>
<span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/sfttrainer-0.9.4.dev84+g1ab8f43d-py3-none-any.whl</span>
<span class=nt>env_vars</span><span class=p>:</span>
<span class=w>  </span><span class=nt>PYTHONUNBUFFERED</span><span class=p>:</span><span class=w> </span><span class=s>&quot;x&quot;</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Your wheel file will have a different name so update the <code>ray_runtime_env.yaml</code> file accordingly. Make sure you keep the <code>${RAY_RUNTIME_ENV_CREATE_WORKING_DIR}/</code> prefix.</p> </div> <p>Then start a Ray job that executes <code>sfttrainer_generate_dataset_text</code> and pointing it to your remote RayCluster and references your <code>ray_runtime_env.yaml</code> file. For example, if your RayCluster is listening on <code>http://localhost:8265</code> run the following command in the same directory as your <code>ray_runtime_env.yaml</code> file:</p> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>If you are using a remote RayCluster on Kubernetes remember to <a href=../../getting-started/remote_run/#specifying-the-remote-ray-cluster-to-submit-to-address>start a port-forward to the RayCluster head node</a>.</p> </div> <div class=highlight><pre><span></span><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \
  sfttrainer_generate_dataset_text \
  -o /data/fms-hf-tuning/artificial-dataset/news-tokens-16384plus-entries-4096.jsonl
</code></pre></div> <h3 id=dataset-for-image-to-text-tasks>Dataset for image-to-text tasks<a class=headerlink href=#dataset-for-image-to-text-tasks title="Permanent link">&para;</a></h3> <p>SFTTrainer supports 2 datasets for text-to-image tasks:</p> <ul> <li><code>vision-384x384-16384plus-entries-4096.parquet</code></li> <li><code>vision-384x768-16384plus-entries-4096.parquet</code></li> </ul> <p>To create the dataset files use the same <code>ray_runtime_env.yaml</code> file as above but this time start 2 Ray Jobs:</p> <div class=highlight><pre><span></span><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \
  sfttrainer_generate_dataset_vision --image-width 384  --image-height 384 \
  -o /data/fms-hf-tuning/artificial-dataset/vision-384x384-16384plus-entries-4096.parquet


ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \
  sfttrainer_generate_dataset_vision --image-width 384  --image-height 768 \
  -o /data/fms-hf-tuning/artificial-dataset/vision-384x768-16384plus-entries-4096.parquet
</code></pre></div> <h2 id=model-weights>Model Weights<a class=headerlink href=#model-weights title="Permanent link">&para;</a></h2> <p>The actuator supports model weights from both the <strong>HuggingFace repository</strong> and <strong>local directories</strong>. You can find the full list of supported models in the <a href=https://github.com/ibm/ado/blob/main/plugins/actuators/sfttrainer/ado_actuators/sfttrainer/config/models.yaml>models.yaml</a> file.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>The actuator attempts to cache Hugging Face model weights the first time it runs an operation that references them. To avoid race conditions when running multiple experiments with the same weights, we recommend <strong>pre-fetching</strong> the weights in advance.</p> </div> <p>Identify the models you want to cache and then create a <code>models.yaml</code> file structured as a double-nested dictionary.</p> <ul> <li>The <strong>outer dictionary</strong> keys are the names of the models.</li> <li>Each <strong>inner dictionary</strong> maps model weight types to their corresponding Hugging Face identifiers.</li> </ul> <p>Supported model weight types include:</p> <ul> <li><code>Vanilla</code></li> <li><code>QPTQ-Quantized</code></li> </ul> <p>Here’s a simple example that caches the <code>HuggingFaceTB/SmolLM2-135M</code> model weights from HuggingFace`:</p> <div class=highlight><pre><span></span><code><span class=nt>smollm2-135m</span><span class=p>:</span>
<span class=w>  </span><span class=nt>Vanilla</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">HuggingFaceTB/SmolLM2-135M</span>
</code></pre></div> <p>Next, choose a directory to use as your HuggingFace home. By default, SFTTrainer uses <code>/hf-models-pvc/huggingface_home</code>. To override this, set the <code>hf_home</code> parameter in your <strong>ActuatorConfiguration</strong> resource just like you did for overriding the location of dataset files.</p> <p>For example, to cache the model weights under <code>/my/hf_home/</code> use the following command:</p> <div class=highlight><pre><span></span><code>sfttrainer_download_hf_weights -i models.yaml -o /my/hf_home
</code></pre></div> <p>If you are working with a remote RayCluster then submit a Ray job similar to the above section for generating datasets:</p> <div class="admonition info end"> <p class=admonition-title>Info</p> <p>If you are using a remote RayCluster on Kubernetes remember to <a href=../../getting-started/remote_run/#specifying-the-remote-ray-cluster-to-submit-to-address>start a port-forward to the RayCluster head node</a>.</p> </div> <div class=highlight><pre><span></span><code>ray job submit --address http://localhost:8265 --runtime-env ray_runtime_env.yaml --working-dir $PWD -v -- \
  sfttrainer_download_hf_weights -i models.yaml -o /my/hf_home
</code></pre></div> <h2 id=configure-your-raycluster-for-rdma-over-converged-ethernet-roce>Configure your RayCluster for RDMA over Converged Ethernet (RoCE)<a class=headerlink href=#configure-your-raycluster-for-rdma-over-converged-ethernet-roce title="Permanent link">&para;</a></h2> <p><a href=https://www.roceinitiative.org/roce-introduction/ ><strong>RoCE</strong></a> enables high-throughput, low-latency communication between GPU nodes distributed on multiple nodes by bypassing the kernel and reducing CPU overhead. This is especially beneficial for multi-node AI workloads that rely on fast inter-GPU communication, such as distributed training with <a href=https://developer.nvidia.com/nccl>NVIDIA NCCL</a>.</p> <p>To enable RoCE in a RayCluster on Kubernetes, you need to:</p> <ol type=1> <li><strong>Build a GPU worker image</strong> with the necessary OFED and NCCL libraries.</li> <li><strong>Configure the RayCluster custom resource</strong> to:<ul> <li>Set environment variables for NCCL that switch on the RoCE feature.</li> <li>Mount the NCCL topology file to ensure optimal GPU-to-GPU communication paths are used during collective operations.</li> </ul> </li> <li><strong>Ensure the Kubernetes nodes and network</strong> are RoCE-capable and properly configured.</li> </ol> <p>Here’s the revised and improved list of <strong>prerequisites</strong> for enabling RoCE in GPU workers of a RayCluster on Kubernetes, incorporating clarity, completeness, and technical accuracy:</p> <h3 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h3> <ul> <li>The <strong>system administrator</strong> has configured the GPU nodes and network infrastructure to support RoCE, including BIOS, firmware, switch settings, and lossless Ethernet features.</li> <li>The <strong>NCCL topology file</strong> is provided by the system administrator to optimize GPU communication paths.</li> <li>The <strong>Kubernetes administrator</strong> has granted the RayCluster service account appropriate RBAC permissions and PodSecurity settings necessary for RoCE. In this example we will:<ul> <li>Run containers as <code>root</code>.</li> <li>Use the <code>IPC_LOCK</code> capability to lock memory.</li> </ul> </li> <li>The <strong>device plugin</strong> for RoCE-capable NICs (e.g., NVIDIA Network Operator or custom RDMA plugin) is installed and configured on the cluster.</li> <li>The <strong>GPU worker</strong> has the required drivers and libraries. In this example, we will deploy Ray on a Kubernetes cluster. Thus, our image will contain:<ul> <li>OFED modules</li> <li>the NVIDIA and NCCL runtime binaries</li> </ul> </li> <li>The <strong>system administrator</strong> has shared the number of GPUs and RoCE-capable NICs available per node to guide resource requests and topology mapping.</li> <li>The <strong>Kubernetes administrator</strong> has explained how to:<ul> <li>Request RoCE devices (e.g., <code>nvidia.com/roce_gdr: 2</code>)</li> <li>Enable pods to access the RDMA-enabled network zones</li> <li>Schedule GPU workers on the correct nodes e.g via labels, taints, affinity rules, etc</li> </ul> </li> </ul> <h3 id=install-the-required-libraries-and-drivers>Install the required libraries and drivers<a class=headerlink href=#install-the-required-libraries-and-drivers title="Permanent link">&para;</a></h3> <p>This example walks you through deploying a RayCluster on Kubernetes, including building a custom image for the GPU worker nodes. We’ll use the <code>mirror.gcr.io/rayproject/ray:latest-py310-cu121</code> base image, which includes both Ray and the necessary NVIDIA libraries.</p> <div class=highlight><pre><span></span><code><span class=k>ARG</span><span class=w> </span><span class=nv>base_image</span><span class=o>=</span>mirror.gcr.io/rayproject/ray:latest-py310-cu121
<span class=k>FROM</span><span class=w> </span><span class=s>$base_image</span>

<span class=k>USER</span><span class=w> </span><span class=s>0</span>

<span class=k>ENV</span><span class=w> </span><span class=nv>MOFED_VER</span><span class=o>=</span><span class=m>24</span>.10-1.1.4.0
<span class=k>ENV</span><span class=w> </span><span class=nv>OS_VER</span><span class=o>=</span>ubuntu22.04
<span class=k>ENV</span><span class=w> </span><span class=nv>PLATFORM</span><span class=o>=</span>x86_64

<span class=k>RUN</span><span class=w> </span>mkdir<span class=w> </span>app<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span><span class=nb>cd</span><span class=w> </span>app<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>wget<span class=w> </span>-q<span class=w> </span>http://content.mellanox.com/ofed/MLNX_OFED-<span class=si>${</span><span class=nv>MOFED_VER</span><span class=si>}</span>/MLNX_OFED_LINUX-<span class=si>${</span><span class=nv>MOFED_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>OS_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>PLATFORM</span><span class=si>}</span>.tgz<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>tar<span class=w> </span>-xvzf<span class=w> </span>MLNX_OFED_LINUX-<span class=si>${</span><span class=nv>MOFED_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>OS_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>PLATFORM</span><span class=si>}</span>.tgz<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span><span class=nb>cd</span><span class=w> </span>MLNX_OFED_LINUX-<span class=si>${</span><span class=nv>MOFED_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>OS_VER</span><span class=si>}</span>-<span class=si>${</span><span class=nv>PLATFORM</span><span class=si>}</span><span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>./mlnxofedinstall<span class=w> </span>--user-space-only<span class=w> </span>--without-fw-update<span class=w> </span>--without-ucx-cuda<span class=w> </span>--all<span class=w> </span>--force<span class=w> </span>--distro<span class=w> </span><span class=nv>$OS_VER</span><span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span><span class=nb>cd</span><span class=w> </span>..<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>rm<span class=w> </span>-rf<span class=w> </span>MLNX*<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>apt-get<span class=w> </span>-y<span class=w> </span>clean<span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=se>\</span>
<span class=w>    </span>rm<span class=w> </span>-rf<span class=w> </span>/var/lib/apt/lists/*
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Mellanox OFED is now in long-term support and will reach end-of-life in Q4 2027. NVIDIA has replaced it with DOCA-OFED, which will receive all future updates and features. This example currently uses MLNX_OFED, but we’ll update it with DOCA-OFED installation steps in a future revision.</p> </div> <h3 id=collect-all-necessary-information>Collect all necessary information<a class=headerlink href=#collect-all-necessary-information title="Permanent link">&para;</a></h3> <h4 id=identify-roce-capable-network-devices>Identify RoCE-Capable Network Devices<a class=headerlink href=#identify-roce-capable-network-devices title="Permanent link">&para;</a></h4> <p>To determine which network interfaces support RoCE v2, run the <code>show_gids</code> command on a GPU node. Look for entries where the <strong><code>VER</code></strong> column is <code>v2</code>, which indicates RoCE v2 support.</p> <p>For example, given the following output:</p> <div class=highlight><pre><span></span><code>DEV    PORT INDEX GID                                   IPv4         VER DEV
---    ---- ----- ---                                   ------------ --- ---
mlx5_0 1       0 fe80:0000:0000:0000:0000:60ff:fe68:d096              v1 net1-0
mlx5_0 1       1 fe80:0000:0000:0000:0000:60ff:fe68:d096              v2 net1-0
...
mlx5_3 1       1 fe80:0000:0000:0000:0000:5fff:fe68:d09a              v2 net1-1
</code></pre></div> <p>You should select the devices with <code>v2</code> under the <code>VER</code> column. In this case, the RoCE-capable devices are:</p> <ul> <li><code>mlx5_0_1</code></li> <li><code>mlx5_3_1</code></li> </ul> <p>You will use these device names to set the <code>NCCL_IB_HCA</code> environment variable in your Ray GPU worker pods. For the above example you will set <code>NCCL_IB_HCA="=mlx5_0,mlx5_3"</code></p> <p>You also need to configure <code>NCCL_IB_GID_INDEX</code>. Select the GID index such that it maps to a v2 entry across all nodes to ensure consistent behavior. For the above example you will set <code>NCCL_IB_GID_INDEX=1</code></p> <h3 id=putting-it-all-together>Putting it all together<a class=headerlink href=#putting-it-all-together title="Permanent link">&para;</a></h3> <p>In this section we will use the information we gathered above to define a Ray GPU worker with support for RoCE.</p> <h4 id=summary-of-steps>Summary of Steps<a class=headerlink href=#summary-of-steps title="Permanent link">&para;</a></h4> <ol type=1> <li><strong>Enable memory locking in containers</strong><ul> <li>Request the <code>IPC_LOCK</code> capability in your container’s security context.</li> <li>Use a <code>ServiceAccount</code> (e.g. <code>gdr</code>) that grants permission to request <code>IPC_LOCK</code>.</li> <li>To allow unlimited memory locking:</li> <li><strong>Option A:</strong> Run the container as root (in the example we assume that the <code>roce</code> service account has adequate RBAC to request this).</li> <li><strong>Option B:</strong> Configure the node with <code>ulimit -l unlimited</code> (not available on Vela).</li> </ul> </li> <li><strong>Attach and request RoCE-capable NICs</strong><ul> <li>On our cluster:<ul> <li>We add the annotation: <code>k8s.v1.cni.cncf.io/networks: multi-nic-network</code></li> <li>Request RoCE devices: <code>nvidia.com/roce_gdr: 2</code></li> </ul> </li> </ul> </li> <li><strong>Set NCCL environment variables</strong><ul> <li>Configure variables like <code>NCCL_IB_HCA</code>, <code>NCCL_IB_GID_INDEX</code>, and others to enable RoCE and optimize performance.</li> </ul> </li> <li><strong>Mount the NCCL topology file</strong><ul> <li>Mount the <code>topology-roce</code> ConfigMap at <code>/var/run/nvidia-topologyd</code>.</li> </ul> </li> </ol> <div class=highlight><pre><span></span><code><span class=c1># ... trimmed ...</span>
<span class=w>  </span><span class=nt>workerGroupSpecs</span><span class=p>:</span>
<span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>rayStartParams</span><span class=p>:</span>
<span class=w>    </span><span class=nt>block</span><span class=p>:</span><span class=w> </span><span class=s>&quot;true&quot;</span>
<span class=w>    </span><span class=nt>num-gpus</span><span class=p>:</span><span class=w> </span><span class=s>&quot;8&quot;</span>
<span class=w>    </span><span class=c1># VV: We&#39;ll use the RoCE custom resource to ensure the jobs land on a properly configured node for RoCE</span>
<span class=w>    </span><span class=c1># we support running up to 1 RoCE measurement. Similarly, we have a custom resource called </span>
<span class=w>    </span><span class=c1># &quot;full-worker&quot; for reserving the entire GPU worker if necessary.</span>
<span class=w>    </span><span class=nt>resources</span><span class=p>:</span><span class=w> </span><span class=s>&quot;\&quot;{\\\&quot;NVIDIA-A100-SXM4-80GB\\\&quot;:</span><span class=nv> </span><span class=s>8,</span><span class=nv> </span><span class=s>\\\&quot;full-worker\\\&quot;:</span><span class=nv> </span><span class=s>1,</span><span class=nv> </span><span class=s>\\\&quot;RoCE\\\&quot;:</span><span class=nv> </span><span class=s>1}\&quot;&quot;</span>
<span class=w>  </span><span class=c1># Here, we configure an eightou GPU worker with A100 that can have up to 4 replicas</span>
<span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class=w>  </span><span class=nt>minReplicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class=w>  </span><span class=nt>maxReplicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class=w>  </span><span class=nt>numOfHosts</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class=w>  </span><span class=nt>groupName</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">eight-A100-80G-gpu-WG</span>
<span class=w>  </span><span class=nt>template</span><span class=p>:</span>
<span class=w>    </span><span class=nt>metadata</span><span class=p>:</span>
<span class=w>      </span><span class=nt>annotations</span><span class=p>:</span>
<span class=w>        </span><span class=c1># We use this annotation on our cluster to get access to the appropriate network zone</span>
<span class=w>        </span><span class=nt>k8s.v1.cni.cncf.io/networks</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">multi-nic-network</span>
<span class=w>      </span><span class=nt>labels</span><span class=p>:</span>
<span class=w>        </span><span class=nt>helm.sh/chart</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ray-cluster-1.1.0</span>
<span class=w>        </span><span class=nt>app.kubernetes.io/instance</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ray-disorch</span>
<span class=w>    </span><span class=c1># ServiceAccount gives your pod adequate RBAC to request the IPC_LOCK capability and run as root</span>
<span class=w>    </span><span class=nt>serviceAccountName</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">roce</span>
<span class=w>    </span><span class=c1># RoCE requires root privileges.</span>
<span class=w>    </span><span class=c1># An alternative to using a root account is to request the capability CAP_SYS_RESOURCE and </span>
<span class=w>    </span><span class=c1># run `ulimit -l unlimited` before starting up the Ray worker</span>
<span class=w>    </span><span class=nt>securityContext</span><span class=p>:</span>
<span class=w>      </span><span class=nt>fsGroup</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class=w>      </span><span class=nt>runAsGroup</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class=w>      </span><span class=nt>runAsNonRoot</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>      </span><span class=nt>runAsUser</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class=w>    </span><span class=nt>volumes</span><span class=p>:</span>
<span class=w>      </span><span class=nt>volumes</span><span class=p>:</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topology-volume</span>
<span class=w>        </span><span class=nt>configMap</span><span class=p>:</span>
<span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topology-roce</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dshm</span>
<span class=w>        </span><span class=nt>emptyDir</span><span class=p>:</span>
<span class=w>          </span><span class=nt>medium</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Memory</span>
<span class=w>      </span><span class=c1># Add your remaining PVCs here e.g. a GPFS volume for storing the </span>
<span class=w>      </span><span class=c1># HF_HOME path that you will use with &quot;accelerate launch&quot; etc</span>
<span class=w>    </span><span class=nt>containers</span><span class=p>:</span>
<span class=w>    </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">pytorch</span>
<span class=w>      </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">$YOUR_ROCE_ENABLED_IMAGE_HERE</span>
<span class=w>      </span><span class=nt>imagePullPolicy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Always</span><span class=w>                      </span>
<span class=w>      </span><span class=nt>securityContext</span><span class=p>:</span>
<span class=w>        </span><span class=nt>allowPrivilegeEscalation</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class=w>        </span><span class=nt>capabilities</span><span class=p>:</span>
<span class=w>          </span><span class=nt>add</span><span class=p>:</span>
<span class=w>          </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">IPC_LOCK</span><span class=w>  </span><span class=c1># for RoCE to work</span>
<span class=w>      </span><span class=nt>env</span><span class=p>:</span>
<span class=w>      </span><span class=c1># To enable RoCE</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_HCA</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">=mlx5_0,mlx5_3</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_GID_INDEX</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;1&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_DISABLE</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;0&quot;</span><span class=w> </span><span class=c1># Set this to &quot;1&quot; to disable RoCE</span>
<span class=w>      </span><span class=c1># To visually verify that RoCE is On based on the logs that NCCL prints</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_DEBUG</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">INFO</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_DEBUG_SUBSYS</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;INIT,BOOTSTRAP,ENV&quot;</span>
<span class=w>      </span><span class=c1># Remaining NCCL environment variables we use on our cluster</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_QPS_PER_CONNECTION</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;8&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_SPLIT_DATA_ON_QPS</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;0&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IB_PCI_RELAXED_ORDERING</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;1&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_ALGO</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Ring</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_IGNORE_CPU_AFFINITY</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;1&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_SOCKET_NTHREADS</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;2&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NCCL_CROSS_NIC</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;0&quot;</span>
<span class=w>      </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">OMP_NUM_THREADS</span>
<span class=w>        </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s>&quot;16&quot;</span><span class=w>                  </span>
<span class=w>    </span><span class=nt>volumeMounts</span><span class=p>:</span>
<span class=w>    </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topology-volume</span>
<span class=w>      </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/var/run/nvidia-topologyd</span>
<span class=w>    </span><span class=c1># Your other volumemounts here</span>
<span class=w>    </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dshm</span>
<span class=w>      </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=s>&quot;/dev/shm&quot;</span><span class=w>                      </span>
<span class=w>    </span><span class=nt>resources</span><span class=p>:</span>
<span class=w>      </span><span class=c1># Here we are requesting an entire node</span>
<span class=w>      </span><span class=nt>requests</span><span class=p>:</span>
<span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class=w>        </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">720Gi</span>
<span class=w>        </span><span class=nt>nvidia.com/roce_gdr</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class=w>      </span><span class=nt>limits</span><span class=p>:</span>
<span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class=w>        </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">720Gi</span>
<span class=w>        </span><span class=nt>nvidia.com/roce_gdr</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w> </span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>We recommend enabling RoCE only for GPU workers that occupy an entire Kubernetes node. This ensures that multi-node jobs are using separate Kubernetes nodes, allowing RoCE to be effectively utilized.</p> </div> <h4 id=verify-youre-using-roce>Verify you're using RoCE:<a class=headerlink href=#verify-youre-using-roce title="Permanent link">&para;</a></h4> <p>Remember, RoCE only applies to multi-node jobs. To verify that it’s working, run a multi-node NCCL job and inspect the logs. If your GPU workers are properly configured for RoCE, you should see output similar to the snippet below. We’ve annotated the important lines with <code>&lt;--</code> and added comments to highlight what to look for.</p> <div class=highlight><pre><span></span><code>[0] NCCL INFO NCCL_IB_DISABLE set by environment to 0. &lt;-- double check that this is set to 0
[4] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_3 &lt;-- This does not confirm that you are using RoCE
[3] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_3:1/RoCE [RO]; OOB net1-0:1.2.3.21&lt;0&gt; &lt;-- Name of the NICs 
                                                                                                and /RoCE
[3] NCCL INFO Using non-device net plugin version 0
[3] NCCL INFO Using network IB &lt;-- Uses the IB network
</code></pre></div> <p>NCCL falls back to "Socket" network when RoCE is unavailable. In this scenario your log output will be similar to the snippet below.</p> <div class=highlight><pre><span></span><code>[1] NCCL INFO NCCL_IB_DISABLE set by environment to 0. &lt;-- if this is set to 1, you will NOT use RoCE even
                                                           if it is properly configured
[1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to net1-0,net1-1
[1] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_3
[1] NCCL INFO NET/IB : No device found. # &lt;-- No network Infiniband network found
[1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to net1-0,net1-1
[1] NCCL INFO NET/Socket : Using [0]net1-0:1.2.3.30&lt;0&gt; [1]net1-1:1.2.4.30&lt;0&gt; # &lt;-- No mention of /RoCE 
                                                                                   or the NICs
[1] NCCL INFO Using non-device net plugin version 0
[2] NCCL INFO Using network Socket # &lt;-- Switches to TCP
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>You might see warnings indicating that NCCL failed to load certain .so files. These messages are harmless and unrelated to RoCE configuration. You can ignore them safely.</p> </div> <h2 id=next-steps>Next steps:<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <div class="grid cards"> <ul> <li> <p>⚙️ <strong>Customize Actuators using ActuatorConfiguration resources</strong></p> <hr> <p>Learn how to use <strong>ActuatorConfiguration</strong> resources to customize the SFTTrainer <strong>Operations</strong></p> <p><a href=../../resources/actuatorconfig/ >ActuatorConfiguration documentation</a></p> </li> <li> <p>🖥️ <strong>Ready to try it out?</strong></p> <hr> <p>The SFTTrainer actuator can run experiments locally as well. Just follow the example below to get started:</p> <p><img alt=🔗 class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f517.svg title=:link:> <a href=../../examples/finetune-locally/ >Run a local fine-tuning experiment</a></p> </li> <li> <p><span class="twemoji lg middle"><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.322.75h1.176a1.75 1.75 0 0 1 1.75 1.749v1.177a10.75 10.75 0 0 1-2.925 7.374l-1.228 1.304a24 24 0 0 1-1.596 1.542v5.038c0 .615-.323 1.184-.85 1.5l-4.514 2.709a.75.75 0 0 1-1.12-.488l-.963-4.572a1.3 1.3 0 0 1-.14-.129L8.04 15.96l-1.994-1.873a1.3 1.3 0 0 1-.129-.14l-4.571-.963a.75.75 0 0 1-.49-1.12l2.71-4.514c.316-.527.885-.85 1.5-.85h5.037a24 24 0 0 1 1.542-1.594l1.304-1.23A10.75 10.75 0 0 1 20.321.75Zm-6.344 4.018v-.001l-1.304 1.23a22.3 22.3 0 0 0-3.255 3.851l-2.193 3.29 1.859 1.744.034.034 1.743 1.858 3.288-2.192a22.3 22.3 0 0 0 3.854-3.257l1.228-1.303a9.25 9.25 0 0 0 2.517-6.346V2.5a.25.25 0 0 0-.25-.25h-1.177a9.25 9.25 0 0 0-6.344 2.518M6.5 21c-1.209 1.209-3.901 1.445-4.743 1.49a.24.24 0 0 1-.18-.067.24.24 0 0 1-.067-.18c.045-.842.281-3.534 1.49-4.743.9-.9 2.6-.9 3.5 0s.9 2.6 0 3.5m-.592-8.588L8.17 9.017q.346-.519.717-1.017H5.066a.25.25 0 0 0-.214.121l-2.167 3.612ZM16 15.112q-.5.372-1.018.718l-3.393 2.262.678 3.223 3.612-2.167a.25.25 0 0 0 .121-.214ZM17.5 8a1.5 1.5 0 1 1-3.001-.001A1.5 1.5 0 0 1 17.5 8"/></svg></span> <strong>Take it to the next level</strong></p> <hr> <p>Do you have a RayCluster with GPUs in it?</p> <p><img alt=🔗 class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f517.svg title=:link:> <a href=../../examples/finetune-remotely/ >Run a fine-tuning experiment on a remote RayCluster</a></p> </li> </ul> </div> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../replay/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Using externally obtained data"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Using externally obtained data </div> </div> </a> <a href=../../operators/working-with-operators/ class="md-footer__link md-footer__link--next" aria-label="Next: Operators overview"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Operators overview </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright IBM Research. </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/ibm/ado target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.ibm.com/ target=_blank rel=noopener title=www.ibm.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill=currentColor d="M351.9 280H161c2.9 64.5 17.2 123.9 37.5 167.4 11.4 24.5 23.7 41.8 35.1 52.4 11.2 10.5 18.9 12.2 22.9 12.2s11.7-1.7 22.9-12.2c11.4-10.6 23.7-28 35.1-52.4 20.3-43.5 34.6-102.9 37.5-167.4zm-191-48h190.9c-2.8-64.5-17.1-123.9-37.4-167.4-11.4-24.4-23.7-41.8-35.1-52.4C268.1 1.7 260.4 0 256.4 0s-11.7 1.7-22.9 12.2c-11.4 10.6-23.7 28-35.1 52.4-20.3 43.5-34.6 102.9-37.5 167.4m-48 0c3.5-85.6 25.6-165.1 57.9-217.3C78.7 47.3 10.9 131.2 1.5 232zM1.5 280c9.4 100.8 77.2 184.7 169.3 217.3-32.3-52.2-54.4-131.7-57.9-217.3zm398.4 0c-3.5 85.6-25.6 165.1-57.9 217.3 92.1-32.7 159.9-116.5 169.3-217.3zm111.4-48C501.9 131.2 434.1 47.3 342 14.7c32.3 52.2 54.4 131.7 57.9 217.3z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.feedback", "content.code.annotate", "content.code.copy", "content.tabs.link", "navigation.footer", "navigation.expand", "navigation.indexes", "navigation.instant", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.share", "search.suggest", "toc.integrate"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.92b07e13.min.js></script> </body> </html>